module: autograd	 feature request  a rank revealing svd for better stability in backward 
module: autograd	 libtorch can not save a  vector int  to autogradcontex  saved data 
module: autograd	 question  how to extract expose the complete pytorch computation graph  forward and backward  
module: autograd	add flag for functional jacobian to return output as well
module: autograd	add more explanation on multithreaded graph building of autograd
module: autograd	addmv  mv will trigger internal assert failed when input requiring grad
module: autograd	arctan  fp   error when optimising
module: autograd	autocast   torch no grad inference cause backward graph nodes to be lost
module: autograd	backward and grad behave inconsistently w r t  set  on leaf variable
module: autograd	backward implicit conversion from tuple to torch tensor results in an indexing error message
module: autograd	backward inputs  does not need to execute grad fn of the inputs
module: autograd	conv d padding same gradgradcheck fails on cuda
module: autograd	copy deepcopy does not copy gradients of nn parameter
module: autograd	ctc loss will backward crash
module: autograd	disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda 
module: autograd	disabled test grad nn functional conv d cuda float      main   testoperatorscuda 
module: autograd	forward program terminated from   cxa pure virtual
module: autograd	improved error checking for custom function when saving intermediates
module: autograd	incorrect gradient calculation for upsample nearest on cuda
module: autograd	jacobian mismatch for nn functional ctc loss
module: autograd	leakyrelu and elu use more vram than needed
module: autograd	matmul  mm triggers internal assert failed when input requires grad
module: autograd	nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: autograd	oom during backward   leads to memory leaks
module: autograd	pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: autograd	pytorch  torch autograd grad returns nonetype
module: autograd	registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: autograd	sparse updates to logits in distributions categorical
module: autograd	svd triggers internal assert failed when computing jacobian in forward mode
module: autograd	torch clamp does not distribute gradients as element wisemin max do
module: autograd	torch compile incorrect when imperative autograd apis are used
module: autograd	torch func jacrev fails if model contains full backward hook
module: autograd	torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: autograd	torch ldexp generated tests fail on call to torch mul
module: autograd	torch linalg cond has different results for tensor requiring autograd
module: autograd	torch nn functional l  loss fails gradgradcheck for complex inputs
module: autograd	tsan issue in autograd  set next edges 
module: autograd	undocumented error on torch autograd function jvp for non tensor forward returns
module: autograd	wrong return type from operation on custom tensor inside registered hook
module: cpp	      cu    package missing proper libnvrtc builtins so     
module: cpp	 cpp  allow binding config structs into the python front end
module: cpp	 cpp op  torch library schema doesn t respect keyword only
module: cpp	 cppdocs  torch  load function istringstream example typo
module: cpp	a few functions in fbgemm utils cpp are defined in global namespace
module: cpp	batchnormfuncoptions object cant be printed in c  
module: cpp	c   api  nn  sequential  has inconsistent behavior with python conterpart
module: cpp	c   api for torch autograd functional jacobian
module: cpp	compiling libtorch from source on mac beyond v      
module: cpp	consider not checking in autogenerated core  tensor h tensormethods h 
module: cpp	custom c   extension build process doesn t preserve color from compiler
module: cpp	document how to generate pybind bindings for c   autograd
module: cpp	error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: cpp	generator c   api should match python api
module: cpp	get errors after compiling and running pytorch minimal example for c   mac m  with make
module: cpp	gradient checkpointing support in c   api
module: cpp	how libtorch can work with  tensor data as same as  pytorch
module: cpp	is there doc that explains how to call an extension op in another extension implementation 
module: cpp	libtorch consumes too much memory as      
module: cpp	libtorch for windows  mnist example does no work 
module: cpp	libtorch forward memory leak
module: cpp	libtorch gpu set id bug
module: cpp	libtorch memory leak
module: cpp	libtorch opencv mat result error  different from the python ones
module: cpp	libtorch segfault in packed gru evaluation with cuda batch sizes
module: cpp	libtorch so file size is very large
module: cpp	make         no rule to make target  libtorch lib libc   so 
module: cpp	make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: cpp	newoperatorregistrationtest testimplnodefgetscaught failed 
module: cpp	output values not same and much slower than python api
module: cpp	path tp torch torch h        fatal error c      internal compiler error 
module: cpp	properly design manual cpp binding  make it less error prone 
module: cpp	pytorch      can not using namespace torch  indexing
module: cpp	pytorch and c   inference disagree
module: cpp	pytorch c   api cannot call operator   on torch  nn  sequential
module: cpp	request to revise the pytorch tutorial 
module: cpp	requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: cpp	runtime errors with convolution backward out when handling optional bias gradient
module: cpp	scalar construction in c    pytorch tensor        c  at  tensor   
module: cpp	simple c   custom autograd function code throws error  cuda error  driver shutting down 
module: cpp	tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: cpp	the python module installs cmake files and headers under the python s site packages directory that can t be used
module: cpp	torch  tensor scalar  behaves differently from at  tensor scalar 
module: cpp	undefined symbol error when compiling and loading c   extension
module: cpp	unexpected poor performance of c   extension   wish for a fast operator  
module: cuda	 bug  big difference between the output of conv float precision and double precision
module: cuda	 cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: cuda	 tracker  inconsistencies between cpu and gpu computation
module: cuda	adam  fused true  issues
module: cuda	can only import torch after tensorflow accessed its gpu device
module: cuda	can t reproduce non deterministic results with cuda
module: cuda	convtranspose fails on cpu but returns an empty tensor on cuda
module: cuda	coo   coo tries to allocate way too much memory on cuda
module: cuda	cublas status not supported
module: cuda	cuda      cudnn       run conv d error
module: cuda	cuda extension error message doesn t look correct
module: cuda	cuda is available   error
module: cuda	cuda unknown error after suspend during debugging
module: cuda	cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: cuda	dataparallel scatter method split tensor wrong
module: cuda	disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda 
module: cuda	enable postlocalsgdoptimizer on cuda tensors
module: cuda	error with monai swinunetr and checkpointing
module: cuda	flashattentionv  will oom when building on ci cd with default settings
module: cuda	groupnorm   instancenorm does not handle channels last correctly
module: cuda	implement a torch cuda visible device indexes function 
module: cuda	inconsistent behaviour of torch all  
module: cuda	inconsistent performance degradation of  x  convolution  torch       cu    
module: cuda	increasing batch size makes network forward      times slower
module: cuda	internal assert failed
module: cuda	log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat  
module: cuda	max pool d with indices backward out cuda  remove useless  code gradinput zero    
module: cuda	nn functional embedding bag trigger out of bound read under compute sanitizer
module: cuda	out of memory with pytorch version after      
module: cuda	overflow     on cuda tensor after matrix multiplication
module: cuda	perf drop  factor    ubuntu vs windows on same pc  dual boot 
module: cuda	pytorch could not build from source with cudnn      
module: cuda	runtimeerror in use torch       cuda     
module: cuda	sm    support
module: cuda	support cuda     
module: cuda	tan tanh discrepancies with complex due to jiterator
module: cuda	tensor copied over to multiple gpus on its own
module: cuda	test foreach failing cuda memory leak check
module: cuda	the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: cuda	the speed of matrix inversion is relatively slow for many small matrices
module: cuda	torch     import hangs forever
module: cuda	torch addbmm throws different exception differences on cpu and gpu 
module: cuda	torch cuda device count cached return value does not reflect environment changes 
module: cuda	torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu 
module: cuda	unexpected none value for stream with dynamo
module: docs	 docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: docs	 docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch    
module: docs	 docs  document dtype conversions dtype to complex   dtype to real  
module: docs	 docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: docs	adaptivemaxpool documentation is not detailed
module: docs	add a diagram showing the code structure to contributing md
module: docs	add docs for   tensor flatten       tensor unflatten  
module: docs	ambiguous docstring on register module forward hook
module: docs	autocast documentation examples would break
module: docs	can t build pytorch     from source by gcc      on m  macos
module: docs	citation request for  probabilities for each class  in the doc s description on cross entropy
module: docs	coverage test is only checking packages and not all submodules
module: docs	documentation and typing hints for rprop
module: docs	documentation for torch cuda event blocking true  is wrong
module: docs	errors in contributing md
module: docs	extra information messages for mac in setup py would help 
module: docs	fix docstring errors in  torch docs py  serialization py  overrides py   utils py        
module: docs	fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: docs	fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: docs	guide for diagnosing excess graph breaks
module: docs	improving clarity in the docs of different losses
module: docs	incorrect version in the instructions on official website
module: docs	is there doc that explains how to call an extension op in another extension implementation 
module: docs	masked scatter  is very lacking
module: docs	mps backend doc  model   yourfavoritenet   not defined
module: docs	multiple invalid summaries in torch nn documentation page
module: docs	no document for parameter load debug files in torch  jit  load in c   api
module: docs	no documentation to show how to implement aten  view for custom backend
module: docs	odd hand wavy mathematical notation for conv d
module: docs	patch        
module: docs	permute
module: docs	profiler documentation
module: docs	sdpa tutorial   fails for cpu runs on google colab
module: docs	semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: docs	some c   library docstrings incorrectly linked repeated
module: docs	specify version
module: docs	the formula for kl divloss is wrong in the document
module: docs	torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: docs	torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: docs	torch tensor and torch as tensor keyword argument device documentation wrong
module: docs	torch tensor layout is not documented
module: docs	undocumented error on torch autograd function jvp for non tensor forward returns
module: docs	wrong example of sliced computation in doc page numerical accuracy
module: docs	wrong functionalization of as strided leads to wrong results
module: nn	 bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: nn	 rfc  scaled dot product attention  api changes
module: nn	 torchdistx  future of the large model initialization
module: nn	adaptiveavgpool d failed in the lower version
module: nn	add a deterministic version of reflection pad d backward cuda
module: nn	add additional  sigmoid  approximation to gelu activation 
module: nn	add head mask for transformers
module: nn	backward hook execution order changes when input requires grad is false
module: nn	conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: nn	could be clearer that cross entropy takes logits as input
module: nn	depthwise conv d slower than normal conv d
module: nn	disabled test decoder padding and src mask bool cpu    main   testtransformerscpu 
module: nn	disabled test grad nn functional conv d cuda float      main   testoperatorscuda 
module: nn	disabled test grad nn groupnorm cuda float      main   testmodulecuda 
module: nn	disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda 
module: nn	disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda 
module: nn	dynamo can t parse torch rrelu or torch nn functional rrelu
module: nn	easy way to  freeze  batchnorm running mean running var
module: nn	expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: nn	exported model with dropout incorrectly applies dropout during eval
module: nn	groupnorm   instancenorm does not handle channels last correctly
module: nn	hidden rule in nn fractionalmaxpool d
module: nn	inconsistent behavior of convtranspose d on cpu and cuda
module: nn	instancenorm does not catch dim mismatch
module: nn	investigate lazy   norm   d modules no batch dim support
module: nn	log softmax could be        to         times more accurate on small outputs
module: nn	make dropout take a dim     argument
module: nn	many padding module fail memory format tests
module: nn	max pool d can succeed when padding is negative for tensor requiring grad
module: nn	max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: nn	max unpool d is not deterministic
module: nn	max unpool gives wrong gradient when indices has duplicate
module: nn	need  valid  and  same  padding mode for convtranspose d
module: nn	negative values still produced by torch nn functional kl div
module: nn	parameter   deepcopy   doesn t preserve view relationships
module: nn	position embedding aware global circular convolution
module: nn	potential issue with custom transformer masks when using fast path and batch first true
module: nn	regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch              
module: nn	runtimeerror when calling conv transpose d with groups
module: nn	should  native batch norm legit functional be in native functions yaml 
module: nn	support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: nn	support for learnable p values in lppool like pool
module: nn	torch nn conv d conv d s padding mode circular cannot accept   dim input
module: nn	torch nn crossentropyloss  class weighting changes label smoothing
module: nn	torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu 
module: nn	torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: nn	torch nn instancenorm       d doesn t verify the value type of parameter num features
module: nn	torch nn pairwisedistance the results vary widely from version to version
module: nn	torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: nn	transformer with convolutional position wise feed forward network
module: nn	use isinstance instead of type when checking for torch nn parameter
module: numpy	 bug  numpy is no longer a required dependency
module: numpy	 complex  torch  exp   does not match numpy
module: numpy	 doc  view appears to mean different things  view reshape vs transpose permute 
module: numpy	 feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: numpy	 numpy  add iscomplexobj and isrealobj
module: numpy	 numpy  add torch newdim torch newaxis
module: numpy	add a numpy like pad function
module: numpy	add docs on pytorch   numpy interaction
module: numpy	add support for reading the whole file in from file
module: numpy	allow try except check for numpy bfloat   representation
module: numpy	better argument names for torch atan  and other math functions
module: numpy	better support for operators that return  named  tuples of tensors
module: numpy	broadcasting for torch cross
module: numpy	comparison ops for complex tensors
module: numpy	creating torch tensor as a function of index value
module: numpy	disabled test dunder round edgecases val            ndigits       main   testnonarrayargs 
module: numpy	endpoint false for torch linspace and torch logspace
module: numpy	enhance supported types of functional pad
module: numpy	feature request  tests for int should be tests for numbers integral
module: numpy	function request  scipy interpolate griddata
module: numpy	function request  scipy interpolate interpolatedunivariatespline
module: numpy	implement missing torch nan  operators
module: numpy	implementing packbits
module: numpy	importing numpy interacts with tensor sum perf
module: numpy	inconsistent complex results with numpy when computing non positive power of  
module: numpy	indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: numpy	interpolation tracking issue
module: numpy	list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: numpy	make torch cross dim parameter work intuitively
module: numpy	mixing numpy s arrays and pytorch tensors
module: numpy	multiplication of torch tensor with np array does the operation with numpy 
module: numpy	new feature   a very fast algorithm for computing matrix rank
module: numpy	numpy compatibility tracking issue
module: numpy	out  resizing  and restriding  behavior is confusing
module: numpy	please add  tensor astype dtype string   syntax for numpy interoperability
module: numpy	precision consistency issue in linspace
module: numpy	pytorch s flip returns a new tensor  but numpy s flip returns a view
module: numpy	reciprocals of complex tensors with infinities are different from numpy 
module: numpy	reducing over empty dimensions for reductions without identity
module: numpy	set dtype if tensor converted to numpy
module: numpy	support alternate casting rules
module: numpy	support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype 
module: numpy	support divmod for tensors
module: numpy	support keep stride for neg with requires grad false
module: numpy	test float to int conversion finite cpu int   is failing on macos
module: numpy	tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: numpy	torch allclose does not allow different types for comparison
module: numpy	torch ceil  torch floor should accept a dtype argument
module: numpy	torch equal can still run successfully when the parameter types are different 
module: numpy	torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: numpy	torch polygamma inconsistent with scipy special polygamma for n     
module: numpy	torch searchsorted issues
module: numpy	torch tensor relies on implicit conversion being deprecated in python     
module: numpy	unbuffered operation
module: onnx	 dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: onnx	 onnx  assertion in models is not supported by fx exporter
module: onnx	 onnx  dynamo  failed to export cumsum with dtype float  
module: onnx	 onnx  export failed for module with keyword only inputs
module: onnx	 onnx  fx exporter  replace aten  copy  with out place version
module: onnx	 onnx  fx exporter  test models onnxruntime py  tracker
module: onnx	 onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: onnx	 onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: onnx	 onnx  remove the deprecated function  export
module: onnx	 onnx  result from export onnx in pytorch returns different result from pytorch
module: onnx	 onnx  stft export fails with dynamo export
module: onnx	 onnx  test op consistency py doesn t support constant inputs
module: onnx	add onnx backend to torch export api
module: onnx	aten  int repr not supported in torch onnx export
module: onnx	couldn t export yolov  quantized model to onnx
module: onnx	crash on converting circular padding  to onnx
module: onnx	documentation error of torch onnx
module: onnx	error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: onnx	exporting the operator  aten   convolution mode  to onnx opset version    is not supported 
module: onnx	exporting the operator  aten  linalg inv  to onnx opset version    is not supported 
module: onnx	exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: onnx	facing error while using onnx from scatterelements
module: onnx	got expand nodes with static shape input when exporting onnx model with dynamic shape
module: onnx	graph  export onnx   incorrect data types in the binary string representation
module: onnx	huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: onnx	onnx export of batch norm for unknown channel size issue 
module: onnx	onnx exporter for circular padding mode in convolution ops
module: onnx	please verify        onnx release candidate on testpypi
module: onnx	revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence 
module: onnx	runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by    
module: onnx	support onnx opset    to export gelu to one single op
module: onnx	test view dynamic zero dim no longer testing zero input
module: onnx	torch compile fails with  internal assert failed  when compiling gpt  
module: onnx	torch export does not support torchaudio transforms spectrogram
module: onnx	torch onnx dynamo export stuck at reshape
module: onnx	torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported 
module: onnx	unsupported  onnx export of operator group norm  unknown input rank 
module: onnx	use system onnx  undefined references
module: onnx	when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: optimizer	 c   api parity  incorrect documentation for optim initialization in serialization docs
module: optimizer	 discussion  relax optimizer constructor constraints for simplicity
module: optimizer	 docs  torch optim lr scheduler
module: optimizer	 dynamo  slow compile times for optimizers due to for loops
module: optimizer	 feature pitch  full batch optimization toolkit
module: optimizer	 feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it 
module: optimizer	 testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: optimizer	adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr 
module: optimizer	add testing regarding sparseadam state dicts
module: optimizer	adding novel  adafamily  optimizer
module: optimizer	all keys matched successfully missing when loading state dict on optimizers
module: optimizer	allow lrscheduler to take in param groups directly without an optimizer
module: optimizer	c   optimizer check for duplicate parameters
module: optimizer	consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: optimizer	cosineannealingwarmrestarts lr scheduler fails when lash epoch      
module: optimizer	dynamo d optimizer does not handle closure correctly
module: optimizer	dynamo does not correctly handle future iterations if a specific iteration is frozen
module: optimizer	dynamo ing sgd w  momentum errors for cpu params with empty grads
module: optimizer	ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: optimizer	exponentiallr unexpectedly calls step   when init argument last epoch is larger than   
module: optimizer	flattening nn parameters while maintaining gradients from neural network forward pass
module: optimizer	implement l  and l  gradient as hooks with the option of changing the weight decay value 
module: optimizer	implementation of cg  and bicgstab methods
module: optimizer	improve  group tensors by device and dtype
module: optimizer	including adabound in the list of optimizers 
module: optimizer	is this a typo in optimizer pyi   it says statue instead of state
module: optimizer	keyerror  xxxxxxxxxx when calling optimizer state dict  
module: optimizer	lbfgs accuracy difference between cpu and gpu
module: optimizer	learning rate scheduler list index out of range
module: optimizer	optim adadelta  local variable  lr  referenced before assignment
module: optimizer	potential memory leak in adam optimizer in amd chips  cpu 
module: optimizer	problematic asgd optimizer
module: optimizer	pytorch        adam optimizer malfunction   
module: optimizer	pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: optimizer	sgd optimizer with deprecation warning
module: optimizer	small learning rate with capturable true causes adam optimizer to blow up model parameters 
module: optimizer	support for arbitrary schedulers in sequentiallr
module: optimizer	suppport fused adamw on cpu
module: optimizer	weight decay in torch adam
module: tests	 docker  test corrcoef cpu complex   fails on cpu build
module: tests	 feature request  quant  support fakequant qconfigs in test module init
module: tests	 multi device  tests get skipped in standard ci
module: tests	 opinfo  improvements for sparse ops tests
module: tests	 tests  cumprod opinfo tests take long time to run  around  min 
module: tests	add bc test for load state dict on optimizerinfos and moduleinfos
module: tests	add torch nn conv d correctness test
module: tests	backcompat tests in test nn py are slow
module: tests	better handling of opinfo sample inputs
module: tests	cuda gradcheck tests can occasionally leak memory in hud ci
module: tests	disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda 
module: tests	disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda 
module: tests	fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: tests	functorch slow tests not being run in slow ci
module: tests	get test jit py below   k lines
module: tests	improve memory format testing
module: tests	improve primtorch testing for view consistency 
module: tests	improve visibility in test suite timings
module: tests	instability in test input weight equalization activation values test for random test values 
module: tests	is the current behavior with addcmul and integer dtypes intended 
module: tests	kwonly arguments without defaults don t work with test overrides py
module: tests	linalg pinv singular tests are slow
module: tests	many inplace operators are not being tested for variant consistency  test variant consistency eager 
module: tests	opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: tests	opinfo mechanism to test for nondeterminism
module: tests	opinfo tests should compare gpu to cpu implementations
module: tests	opinfo tests to validate that all operators are being tested with strided tensors
module: tests	opinfos disabled for batched forward grad computation
module: tests	pytorch        test optim fails on nvidia a   
module: tests	pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: tests	some inplace ops don t raise on incompatible shapes and meta device
module: tests	test lazy spuriously fails if lapack is not installed
module: tests	test nccl barrier timeout new group non member fails intermittently
module: tests	test public bindings in ci gives weird output on error
module: tests	test reductions ignoring some tests
module: tests	test replicationpad d  test nn testnn  takes too long to run
module: tests	test variant consistency eager addbmm fails on both cpu and cuda
module: tests	test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: tests	testcase assertequal does not distinguish python builtin types and single element tensor
module: tests	testdataloader test proper exit takes    min to finish
module: tests	torch  dynamo config suppress errors may not be properly reset
module: tests	tracker  moduleinfo based testing
module: tests	when one distributed test fails in ci  the next one can fail spuriously
module: tests	where opinfo doesn t handle cases where one of the inputs is a scalar