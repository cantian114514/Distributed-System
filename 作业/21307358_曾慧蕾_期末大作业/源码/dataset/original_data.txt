module: tests	Add testing regarding SparseAdam state_dicts
module: tests	Add BC test for load_state_dict on OptimizerInfos and ModuleInfos
module: tests	test_can_load_older_state_dict_NAdam_cuda_float32 errors with PYTORCH_TEST_WITH_DYNAMO
module: tests	Dynamo'd test_mixed_device_dtype needs higher tolerances for SGD and RMSProp
module: tests	With dynamo, peak memory usage is higher for the Adam family
module: tests	With dynamo, test_foreach_matches_forloop recompiles too many times
module: tests	Auto skip the entire test when @parametrize takes an empty list
module: tests	torch._dynamo.config.suppress_errors may not be properly reset
module: tests	[dynamo + optim] complex, sparse are not on tracing testing path
module: tests	test_pytorch_onnx_onnxruntime_cuda.py is not run in CI
module: tests	DISABLED test_nondeterministic_alert_median_cuda_float64 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_kthvalue_cuda_float64 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_histc_cuda (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float32 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA)
module: tests	inductor/test_max_autotune having timeout issues
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool2d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool2d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float32 (__main__.TestTorchDeviceTypeCUDA)
module: tests	DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA)
module: tests	[testing] dynamo testing: we should call dynamo.reset before running each test with dynamo.
module: tests	FFT Samples Inputs with More than Three Dimensions
module: tests	Command to reproduce error is incorrect
module: tests	Dynamo test pipeline failed on MaxPool2d test when changed to use f-string
module: tests	Nondeterministic segfault in test_content_store.py under Dynamo config
module: tests	Add missing OpInfos for prims ops
module: tests	Look into test coverage for UntypedStorage
module: tests	Test failure: TestCommonCPU.test_python_ref__refs_abs_cpu_complex32
module: tests	Split getitem OpInfo into dynamic and non-dynamic inputs
module: tests	test_fx_passes generate bad test names
module: tests	"multi device" tests get skipped in standard CI
module: tests	Extend test_proxy_tensor tests to support ops test non floating point types
module: tests	Unit test with --subprocess command doesn't respect the -k filter flag and runs all available sub tests
module: tests	Feature request: Deterministic test input generation
module: tests	NVFuser batch norm with prims: internal assert failure from test suite
module: tests	OpInfo Tests To Validate that All Operators Are Being Tested With Strided Tensors
module: tests	CI fails for test_compare_cpu_nn_functional_embedding_cuda_float32 which is not reproducible locally
module: tests	make_traced() doesn't respect setting the seed
module: tests	Update use_deterministic_algorithms documentation and tests to include nn.functional counterparts for all nn modules
module: tests	Add unit tests for test decorators
module: tests	OpInfo tests should compare gpu to cpu implementations
module: tests	fill_ OpInfo code not used, also, doesn't test the case where the second argument is a Tensor
module: tests	test_lazy spuriously fails if LAPACK is not installed
module: tests	test_profiler_experimental_tree_cuda_detailed is too unstable, and as its CUDA only difficult to regen
module: tests	Test public bindings in CI gives weird output on error
module: tests	functorch slow tests not being run in slow CI
module: tests	Move functorch tests to under test/
module: tests	OpInfos for torch.ops.aten operations
module: tests	Improve PrimTorch testing for view consistency.
module: tests	[distributed_test.py] Improve test_barrier
module: tests	[Better Engineering] Make OpInfo-based test failures easy to reproduce
module: tests	linalg.pinv_singular tests are slow
module: tests	RuntimeError: Event device type CUDA does not match blocking stream’s device type CPU
module: tests	toleranceOverride should override atol and rtol even when explicitly specified in a test
module: tests	How to handle __module__  attribute for Public API bindings
module: tests	microbenchmark-style tests
module: tests	test_python_reference_meta_functions takes too long to run
module: tests	OpInfo incorrectly advertises lu_solve support on CUDA even when compiled without magma
module: tests	OpInfo CUDA bfloat16 support detection is buggy
module: tests	AttributeError: '_thread._local' object has no attribute 'rel_tol' (cannot use TestCase.assertEqual from other threads)
module: tests	Expand pow and float_pow sampling function for more coverage
module: tests	test_wishart_log_prob fails locally for me
module: tests	test_license_for_wheel always fails on my local dev copy
module: tests	run_test.py option to write out failed tests
module: tests	backcompat tests in test_nn.py are slow
module: tests	Addressing skips in OpInfo nn.functional.binary_cross_entropy_with_logits
module: tests	torch.overrides testing is not catching people adding new kwargs and not passing on to handle_torch_function
module: tests	Require PyTorch test suite to be warnings clean
module: tests	Include finfo(dtype).[min, max, eps, tiny] in the extremal test case
module: tests	__rpow__(self, other) OpInfo should not test the case where other is a Tensor
module: tests	OpInfo request for nn.functional and unbind
module: tests	[CTA] Let's Stamp Out Flaky Tests!
module: tests	Many inplace operators are not being tested for variant consistency (test_variant_consistency_eager)
module: tests	[docker] test_corrcoef_cpu_complex64 fails on CPU build
module: tests	assertEqual gives confusing error when comparing tuple with Tensor with Tensor
module: tests	addcdiv is failing the ASAN test for zero divisors
module: tests	TestCommonCUDA.test_noncontiguous_samples_pca_lowrank_cuda_float32 fails on A100 due to TF32 operation in svd_lowrank
module: tests	index_fill_ accepts wrong dtype for meta tensors
module: tests	Is the current behavior with addcmul and integer dtypes intended?
module: tests	nn.functional No-batch-dim support should have OpInfo examples
module: tests	Some inplace ops don't raise on incompatible shapes and meta device
module: tests	TestCase.assertEqual has equal_nan default to True
module: tests	Better Engineering: test_..._mem_overlap in test_torch.py should be ported to ErrorInputs
module: tests	Better Engineering: Create test_dlpack
module: tests	Rewrite tests in test_nn to not depend on LAPACK
module: tests	NewOperatorRegistrationTest.testImplNoDefGetsCaught failed.
module: tests	c2r fft input generation
module: tests	The SequentialLR scheduler uses a deprecated pattern
module: tests	Tracker: ModuleInfo-based testing
module: tests	Instability in test_input_weight_equalization_activation_values test for random test values.
module: tests	test_jit_fuser_legacy and test_jit_fuser_te are failing with SIGIOT
module: tests	XLA not being tested in TestAutogradDeviceType
module: tests	CUDA gradcheck tests can occasionally leak memory in HUD CI
module: tests	pytorch_linux_xenial_py3_6_gcc5_4_test may timeout during test_multiprocessing_spawn
module: tests	TestForeachCUDA.test_binary_op_tensorlists_fastpath__foreach_add_cuda_bool and TestForeachCUDA.test_pointwise_op_fastpath__foreach_addcmul_cuda_uint8 fail intermittently
module: tests	kwonly arguments without defaults don't work with test_overrides.py
module: tests	test_nccl_barrier_timeout_new_group_non_member fails intermittently
module: tests	Migrate C++ tests to Python runner
module: tests	Bazel target all_tests improperly reports failures on CPU-only (non-CUDA) build
module: tests	Normalize handling of scalar arguments
module: tests	[opinfo] Confusing interface for ops decorator
module: tests	OpInfos disabled for batched forward grad computation
module: tests	[feature request][quant] Support FakeQuant qconfigs in test_module_init
module: tests	Data Loader tests hang when run in ASAN test job
module: tests	Finishing OpInfos: test_autograd.py
module: tests	TestMultiprocessing.test_fs_sharing is flaky
module: tests	pytorch framework tests using make_tensor hangs with pytest's boxed exec option "--forked"
module: tests	DISABLED test_profiler (__main__.TestJit)
module: tests	Improve meta tensor testing
module: tests	TensorPipeDistAutogradTest is frequently failing
module: tests	FR: Record results of OpInfo reference tests and detect when numerics of an operator change
module: tests	Uniformly test that defaulted Tensor arguments appropriately handle __torch_function__
module: tests	Cleanup tests in rpc_test.py
module: tests	modulefinder_determinator is incompatible with imports into the test/ directory
module: tests	Be able to use pytest to run a "core" set of tests
module: tests	torch.scatter doesn't fail correctly on CUDA (memory overlap)
module: tests	[rfc] Hardcoded Target Determination
module: tests	Property based testing like AFL
module: tests	PyTorch 1.9.0, test_optim fails on Nvidia A10.
module: tests	[Testing] memory_format decorator
module: tests	Review replacing test/test_namedtuple_return_api.py with an OpInfo-based test
module: tests	Some way to specify expected failures for OpInfo-based tests
module: tests	Refactor serialization tests to use device parametrization
module: tests	Improved OpInfo dtype testing
module: tests	Scipy 1.7.0 may cause some test failures
module: tests	Where OpInfo doesn't handle cases where one of the inputs is a scalar
module: tests	Record file/line number when creating test data, and then report it in backtraces associated with this data
module: tests	[Mkldnn] has_bf16 check only works on Linux for tests
module: tests	[testing] SkipInfo should error if cls_name and test_name combination is not valid
module: tests	Improvement to CUDA mem leak check
module: tests	pytorch test failed
module: tests	Add supports_nnc metadata to OpInfos
module: tests	test_bottleneck_cuda fails without error message
module: tests	OpInfo JIT tests do not work with Tensor kwarg arguments
module: tests	test_grid_sample (from TestNN) fails on POWER
module: tests	Avoid code repeat in create sample inputs for sort/msort
module: tests	Tests in CI are run from the test/ directory
module: tests	test_variant_consistency_jit fails for torch.tensordot with dtype float32 with error INTERNAL ASSERT FAILED
module: tests	cumprod gradgradcheck fails in fast_mode=True
module: tests	test_cholesky_solve gradgradcheck fails sometimes
module: tests	Tolerance for non-determinism operators in gradcheck
module: tests	[Opinfo] Better ErrorMsg for test_out with wrong shape
module: tests	test_stream_event_nogil: Is the test making a wrong assumption?
module: tests	test_variant_consistency_eager_addbmm fails on both cpu and cuda
module: tests	Add torch.nn.Conv2D correctness test
module: tests	Improve test runtime of distributed tests.
module: tests	Isolate CPU tests from GPU tests
module: tests	Better syntax for OpInfo
module: tests	Better handling of OpInfo.sample_inputs
module: tests	[tests] cumprod OpInfo tests take long time to run (around 1min)
module: tests	[testing] test_reference_numerics_extremal_clamp_cpu_bfloat16 fails on ci build with GCC5.4 and Python 3.6
module: tests	Test test_large_cumprod_cuda_float16 gets killed due to (probably) OOM
module: tests	"Skipped!" tests should have more descriptive skipped reason
module: tests	Add OpInfo metadata for "is_torch_functional" and a skip for these ops in TestOperatorSignatures.test_get_torch_func_signature_exhaustive
module: tests	Some Numba tests are failing
module: tests	Missing tests for gradcheck
module: tests	test_randperm is failing on CPU-only build
module: tests	[Static Runtime] StaticRuntime.EmbeddingBag test case broken
module: tests	Many advanced indexing operations have untested large tensor branches
module: tests	enable parallel test execution for GPU tests
module: tests	test_reductions ignoring some tests
module: tests	test_metal.py must skip when not compiled with metal support
module: tests	Improve memory format testing
module: tests	torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures
module: tests	OpInfo mechanism to test for nondeterminism
module: tests	index_copy_  test fails on PyTorch/XLA
module: tests	Tracker: pytest-related test improvements
module: tests	[OpInfo] Improvements for sparse ops tests
module: tests	Add autograd tests to verify correctness for R -> C cases
module: tests	Detect OpenMP Loop and this application may hang warnings
module: tests	Discourage slow gradchecks
module: tests	test_fn_grad_fft_fftn_cpu_complex128 and test_fn_grad_fft_rfftn_cpu_float64 are failing under TSAN
module: tests	Test functionality to detect extra cross device synchronizations
module: tests	Turn deprecation warnings into errors in CI
module: tests	Automatically rerun tests with CUDA_LAUNCH_BLOCKING=1 when they fail with CUDA errors in CI
module: tests	test_fs_pool fails
module: tests	Add distributed examples into PyTorch CI tests
module: tests	test_distributed_* does not show error details from the subprocess
module: tests	test_distributed_* does not work with run_test.py -i option
module: tests	"distributed" NCCL tests fail when having more than 3 GPUs
module: tests	Use better tempfile creation mechanism to avoid skip windows test
module: tests	test_cat_cuda (__main__.TestTensorExprFuser) fails
module: tests	TestXNNPACKConv1dTransformPass.test_conv1d_with_relu_fc takes 2+ min to finsh
module: tests	TestDataLoader.test_proper_exit takes 2.5min to finish
module: tests	[Feature Request] Add an option to run GPU tests only, and skip all CPU tests
module: tests	Some largeCUDATensorTest fails with OOM when running with the entire test suit, but not when running standalone
module: tests	test_doc_template is not working correctly
module: tests	Get test_jit.py below 10k lines
module: tests	When one distributed test fails in CI, the next one can fail spuriously
module: tests	pytorch tests failed
module: tests	Proper testing of nn.Module loading backward compatibility
module: tests	test_bottleneck_cuda fails on Power
module: tests	Helping test example code blocks in the docs
module: tests	test_upsampling_not_recompute_scale_factor fails with Eigen/OpenBLAS
module: tests	test_autograd failures on Power
module: tests	jit's default dtype is different in sandcastle and test_jit.py
module: tests	TestListwiseL2rOps::test_lambda_rank_loss fails
module: tests	TestTorchDeviceTypeCPU.test_float_to_int_conversion_finite_cpu_uint8 is broken on PowerPC
module: tests	test_nn_module_tests should run less tests
module: tests	pytest suppresses stderr from Python startup by default
module: tests	Some @slowtests are never run in CI
module: tests	test_float_to_int_conversion_finite_cpu_int16 is failing on MacOS
module: tests	TestCase.assertEqual does not distinguish Python builtin types and single-element Tensor
module: tests	JitTest.testAutogradProfiler is broken in test_misc.cpp
module: tests	SIGXCPU at test_cholesky_solve with AMD EPYC 7742 64-Core Processor
module: tests	test_ReplicationPad3d (test_nn.TestNN) takes too long to run
module: tests	test_LocalResponseNorm_3d_custom_params (test_nn.TestNN) takes too long to run
module: tests	test_interpolate_nearest_scale_3d in test_nn takes too long to run
module: tests	test_cpp_warnings_have_python_context_cpu fails under some build configurations
module: tests	Improve visibility in test suite timings
module: tests	Add a CI configuration to test USE_DISTRIBUTED=0
module: tests	Caffe2 utility_ops_gpu_test fails on Windows
module: tests	Caffe2 ReshapeOpGPUTest crashes on Windows
module: tests	Caffe2 generate_proposals_op_gpu_test crashes on Windows
module: tests	caffe2 DEPTHWISE3x3.Conv test is broken
module: tests	Core dumps being created when running test_c10d.py and test_multiprocessing_spawn.py
module: tests	test_baddbmm_cpu_float32 fails locally for me when built with DEBUG=1
module: tests	Slighty out of tolerance for test_mv and test_cholesky_solve_batched_cuda_float64
module: tests	Refactor/consolidate code for generating test tensors
module: tests	Provide a mechanism to set global state per test in thread-safe manner
module: tests	CPU and CUDA error messages are divergent in type promotion
module: tests	CUDAPytorchToCaffe2.MutualResizes is flaky
module: tests	TestAutograd.test_deep_reentrant fails with SIGBUS on macOS
module: docs	Profiler Documentation
module: docs	Document required semantics of allocation functions in CUDAPluggableAllocator
module: docs	Hidden rule in nn.FractionalMaxPool2d
module: docs	[cppdocs] torch::load function istringstream example typo
module: docs	Improve usability of CUDA package by adding description of CUDA
module: docs	Incorrect line in description of torch.frombuffer() method
module: docs	Typo in example of torch.linalg.solve_triangular
module: docs	[docs] Add reference/decomp impl snippets to functions in online docs for hackability and educational purposes / compensate for some unclear language in existing docs
module: docs	document functional_collectives
module: docs	SDPA Tutorial - fails for CPU runs on Google Colab
module: docs	patch 1130843
module: docs	Fix docstring errors in stream.py, pipe.py, blockpartition.py, microbatch.py, namespace.py, profile.py, tracker.py, portal.py, layout.py, __init__.py
module: docs	Fix docstring errors in checkpoint_example.py, basic_strategy.py, op_schema.py, contract.py, redistribute.py, __init__.py, api.py, device_mesh.py, _utils.py, sharding_prop.py, random.py, checkpoint_activation.py, placement_types.py, fully_shard.py, parallel_mode.py, replicate.py
module: docs	Fix docstring errors in _state_dict_utils.py, _runtime_utils.py, _shard_utils.py
module: docs	Fix docstring errors in _common_utils.py, _optim_utils.py, _wrap_utils.py, _unshard_param_utils.py, _fsdp_extensions.py, api.py, _debug_utils.py, _utils.py, wrap.py, sharded_grad_scaler.py
module: docs	Fix docstring errors in embedding.py, _limiter_utils.py, _dynamo_utils.py, embedding_bag.py, tensor_ops.py, api.py, _internals.py, _common.py, init.py, _exec_order_utils.py, _traversal_utils.py, chunk_sharding_spec.py, _trace_utils.py
module: docs	Fix docstring errors in shard.py, op_registry_utils.py, local_timer.py, file_based_local_timer.py, api.py, distributed.py, cycling_iterator.py, __init__.py, _utils.py, reshard.py, common_op_utils.py, elastic_distributed_sampler.py, utils.py, log_level.py, store.py, logging.py, sharder.py, metadata.py
module: docs	Request to add system requirements to doc
module: docs	Add docs for __tensor_flatten__ / __tensor_unflatten__
module: docs	Fix docstring errors in __init__.py, _tensor_docs.py, _meta_registrations.py, _tensor.py
module: docs	docs: fix docstrings in functional.py and others
module: docs	Can't build PyTorch 2.1 from source by GCC 13.2 on M1 MacOS
module: docs	Fix to bug #112588
module: docs	Fixing the docstring
module: docs	Fix docstring errors in _torch_docs.py, serialization.py, overrides.py, _utils.py #112588
module: docs	Wrong with code_coverage/readme.md
module: docs	Fix docstring errors in stream.py, pipe.py, blockpartition.py, microbatch.py, namespace.py, profile.py, tracker.py, portal.py, layout.py, __init__.py
module: docs	Fix docstring errors in default_hooks.py, post_localSGD_hook.py, debugging_hooks.py, utils.py, hierarchical_model_averager.py, optimizer_overlap_hooks.py, mixed_precision_hooks.py, quantization_hooks.py, ddp_zero_hook.py, __init__.py, powerSGD_hook.py, averagers.py
module: docs	Fix docstring errors in post_localSGD_optimizer.py, functional_sgd.py, _functional_collectives.py, optimizer.py, utils.py, api.py, server_process_global_profiler.py, functions.py, backend_registry.py, __init__.py, options.py, internal.py, functional_adam.py
module: docs	Fix docstring errors in spectral_ops_fuzz_test.py, simple_timeit.py, timer_interface.py, op_benchmark.py, _stubs.py, fuzzer.py, compare.py, compile.py, interp.py, hipify_python.py, common.py, end_to_end.py, timer.py, __init__.py, sparse_fuzzer.py, blas_compare_setup.py
module: docs	Fix docstring errors in loss.py
module: docs	Fix docstring errors in nadam.py, radam.py, sgd.py, anomaly_mode.py, rprop.py, __init__.py, swa_utils.py, rmsprop.py, optimizer.py, lr_scheduler.py
module: docs	Fix docstring errors in _torch_docs.py, serialization.py, overrides.py, _utils.py
module: docs	Fix docstring errors in __init__.py, _tensor_docs.py, _meta_registrations.py, _tensor.py
module: docs	Fix docstring errors in _guards.py, _ops.py, _jit_internal.py, functional.py, _tensor_str.py, library.py
module: docs	Fix docstring errors in _VF.py, _appdirs.py, hub.py, _classes.py, _storage_docs.py, _linalg_utils.py, torch_version.py, quasirandom.py, random.py, __future__.py, _lowrank.py, _vmap_internals.py, _sources.py, __config__.py, _lobpcg.py, _namedtensor_internals.py
module: docs	Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors
module: docs	~ Docathon H2 2023 ~
module: docs	maximum Python version supported is not indicated
module: docs	Incorrect docstring / documentation for torch.nn.functional.scaled_dot_product_attention in 2.1
module: docs	cuda/tf32 docs are outdated
module: docs	scatter_add: Mixing 0-dim and 1-dim tensors
module: docs	pytorch XLA document error
module: docs	[Docs][Distributed] Add migration notes for --local-rank option style change for torchrun in PyTorch 2.0
module: docs	RNN Documentation is Confusing / Wrong
module: docs	FSDP vs. MiCS
module: docs	Parameters of cuda module zero out when used in multiprocessing
module: docs	nn.Transformer has dropout layers that BERT / GPT-2 do not have
module: docs	[docs] F.interpolate(uint8_input, mode = 'bicubic', ...) overshoot behavior: adjust the note in docs to explain that for uint8 saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint8 inputs
module: docs	AdaptiveMaxPool documentation is not detailed
module: docs	Hardtanh docs are inaccurate/incomplete, since hardtanh behaves like clamp
module: docs	[docs] Document dtype conversions dtype.to_complex() dtype.to_real()
module: docs	[Minor Bug] Should consume_prefix_in_state_dict_if_present change ordering of keys?
module: docs	[docs] Idea collection of examples of custom ops / inline torch extensions
module: docs	[docs] URL and link format proposal to make function page URLs more concise
module: docs	https://pytorch.org/docs/stable/backends.html does not describe torch.backends.cpu
module: docs	torch.unique() messes around with order even if sorted=False
module: docs	Specify version
module: docs	Top level Glossary for users (not contributers)
module: docs	[docs] torch.sigmoid to make clear equivalence relations to other sigmoid functions
module: docs	Repro str could be displayed with slightly wrong env vars
module: docs	extra information messages for mac in setup.py would help.
module: docs	Add a diagram showing the code structure to CONTRIBUTING.md
module: docs	errors in CONTRIBUTING.md
module: docs	Wrong functionalization of as_strided leads to wrong results
module: docs	No document for parameter load_debug_files in torch::jit::load in C++ API
module: docs	Numbers bigger than the range should be inf while the implementation just keeps the original.
module: docs	Documentation building fails due to torchgen
module: docs	Reproducibility documentation should be updated
module: docs	CODEOWNERS file has errors due to non existent people being referred to
module: docs	Some parameters are missing type descriptions
module: docs	The document style is inconsistent with other documents, and the parameter type is not clearly highlight
module: docs	Missing examples in some API docs
module: docs	[question] [docs] Short/mid/long-term status of TorchScript / JIT / torch.jit.trace / FX / symbolic tracing and its replacement by Dynamo
module: docs	Disclose C++ ATen ops type promotion rules under OpOverload in Python
module: docs	The document does not emphasize hidden range in nn.Embedding
module: docs	The document does not emphasize hidden range in nn.MaxPool2d
module: docs	pytorch java api documentation is not clear and does not cover example
module: docs	discuss.pytorch.org signup issue
module: docs	[Dynamo]Outdated logging setting
module: docs	Documentation Error of torch.onnx
module: docs	Parameter gradient is not moved parameter is moved across devices
module: docs	Theme update
module: docs	Revise glossary
module: docs	[doc] torch.scalar_tensor doc is missing
module: docs	undocumented error on torch.autograd.Function.jvp for non-Tensor forward returns
module: docs	Backward hook execution order changes when input.requires_grad is False
module: docs	some of the enteries in the previous version of pytorch section are invalid
module: docs	nn.MultiheadAttention doesn't use efficient scaled_dot_product_attention
module: docs	Question about GRU(RNN/LSTM) outputs shape
module: docs	torch.linalg.lstsq doc arguments error
module: docs	Torch func Documentation for trees
module: docs	No documentation to show how to implement aten::view for custom backend
module: docs	torch.cuda.is_available() return False
module: docs	Document the user-facing API for the component-level logging system
module: docs	Discrepancy of supported Python versions between Get Started page and index of pre-built binaries for PIP installation
module: docs	Wrong illustration in README.md
module: docs	F.interpolate and F.grid_sample - documentation error and bug
module: docs	torch.nn.utils.rnn.unpad_sequence modifies arguments in-place
module: docs	README could use link to governance
module: docs	torch.Tensor.layout is not documented
module: docs	torch.randn signature is missing generator
module: docs	Some c++ library docstrings incorrectly linked/repeated
module: docs	Insufficient MPS Documentation
module: docs	nn.Conv function to compute conv formula
module: docs	torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines
module: docs	How to get list of all valid devices?
module: docs	MPS Backend Doc, model = YourFavoriteNet() not defined
module: docs	Pytorch 2.0 installation tutorial does not work under Macbook
module: docs	The torch.sparse document's typo error
module: docs	Pytorch Home Page does not specify which version of python it requires
module: docs	Performance bugs exists in multiple convolution operations(e.g., Convtranspose2d) when useing the groups argument
module: docs	torch.jit.load documentation doesn't specify if it is safe to load untrusted models or not
module: docs	Extend docs - Fixing out of memory with python garbage collection
module: docs	Missing FX documents for some modules
module: docs	Add plots of LRSchedulers to doc to make it easier to read
module: docs	Adding a page for subfolder/subfile overview/descriptions in the developer wiki
module: docs	There is no developer documentation about getting started with MPS native debugging
module: docs	Security policy impractical / lacks contact information?
module: docs	Odd/hand-wavy mathematical notation for Conv2D
module: docs	Inference Mode docs
module: docs	torch.empty produces incorrect tensors with layout=sparse_csr|sparse_csc on the CPU
module: docs	torch.min document not up to date
module: docs	Tensor indexing and slicing documentation should explicitly state that indexing follows numpy semantics and link to the numpy indexing documentation.
module: docs	documentation need to be as per python version
module: docs	The current example for torch.mode is IMHO confusing and has room for improvement.
module: docs	Documentation: torch.nn.functional.embedding docs could more clearly state the requirement that weight be a 2D tensor
module: docs	torch.addcdiv: input, tensor1, and tensor2 parameters should be of the same type
module: docs	Incorrect version in the instructions on official website
module: docs	view_as_real and split_with_sizes links in Tensor Views docs are broken
module: docs	MSE documentation is weak
module: docs	[docs] torch.is_neg/torch.Tensor.is_neg not documented
module: docs	Permute
module: docs	Missing docstring for resize_as
module: docs	Semantics of sparse operations clarification - Sparsity of the gradient with respect to a sparse tensor input
module: docs	Document dist.new_subgroups
module: docs	register_package has no further documentation
module: docs	Missing docker directory in tools/
module: docs	Autograd doc does not mention torch.autograd.set_grad_enabled
module: docs	Documentation and typing hints for RProp
module: docs	[functorch] colab links on functorch 0.2.0 website should be linked to a permalinked version of the colabs
module: docs	Functorch memory_efficient_fusion gives wrong output batch size
module: docs	torch.mm produces wrong result on cpu when using in-place computation
module: docs	torch.Tensor.transpose().contiguous() on dimension of size 1 gives  wrong stride
module: docs	Update use_deterministic_algorithms documentation and tests to include nn.functional counterparts for all nn modules
module: docs	Add documentation about backward graph gc behavior
module: docs	Minimal example for torch.optim.SparseAdam
module: docs	Please include virtual/physical batch sizes in the tutorials
module: docs	torch.pinverse produces wrong output!
module: docs	torch.nn.Upsample's error message is inconsistent with the documentation
module: docs	torch.nn.ReplicationPad{1|2}d supports more input dimension than are written on documentation
module: docs	torch.nn.functional.avg_pool{1|2|3}d error message does not match what is described in the documentation
module: docs	torch.empty_strided argument 'size'and 'stride' documentation wrong
module: docs	torch.bitwise_xor argument 'other' documentation wrong
module: docs	torch.tensor and torch.as_tensor keyword argument device documentation wrong
module: docs	[ROCm] build instruction is haphazard missing information unclear, build does not work
module: docs	RReLU doc doesn't specify the eval mode behaving just like LeakyReLU
module: docs	Is there Doc that explains how to call an extension op in another extension implementation?
module: docs	Guide for diagnosing excess graph breaks
module: docs	Minor inconsistency in description of attn_output_weights in MultiheadAttention docs
module: docs	masked_scatter_ is very lacking
module: docs	torch.sum promotes integral tensors to int64.
module: docs	Could be clearer that Cross Entropy takes logits as input
module: docs	Autocast documentation examples would break
module: docs	Documentation for torch.cuda.Event(blocking=True) is wrong
module: docs	[Misleading] The doc started using Tensorflow terminology in the document to explain how to use the Pytorch code.
module: docs	Tensor.backward type hints clarification
module: docs	torch.searchsorted error message and documentation is unclear
module: docs	Wrong example of sliced computation in doc page Numerical Accuracy
module: docs	Compatibility List
module: docs	Anaconda is not a package manager
module: docs	Distributed Store get doesn't work well with add
module: docs	Comprehensive documentation for Tensor indexing?
module: docs	Display a "reference" link for ops that points to primTorch implementations
module: docs	Improving clarity in the docs of different losses
module: docs	Expose docs from the yaml for each torch.Tag in Python
module: docs	Add a gallery of examples with sphinx-gallery
module: docs	More clarity in doc for torch.cuda.Event.record?
module: docs	Incorrect documentation in gumble_softmax function.
module: docs	Remove all docstrings when python is running in optimization mode
module: docs	Tensor.logit's signature in doc misses eps argument
module: docs	Tensor.register_hook() Source Link Broken
module: docs	torch.bucketize doc typo on the left boundary when 'right=True'
module: docs	Coverage test is only checking packages and not all submodules
module: docs	A bug in instructions for building PyTorch with ASAN
module: docs	torch.device missing doctring
module: docs	Misleading documentation for cholesky_inverse
module: docs	[doc] view appears to mean different things, view/reshape vs transpose/permute.
module: docs	[torch.distributed] Document bfloat16 support
module: docs	[docs] RandomSampler has unrendered back-ticks
module: docs	torch.nn.ConvTranspose2d's example in docstring is invalid
module: docs	package.PackageExporter does not actually appear to have a file_structure method
module: docs	Ambiguous docstring on register_module_forward_hook
module: docs	Inconsistent implementation on SWA
module: docs	torch.cuda.amp: Remove SPMD DDP doc portion
module: docs	torch.distributions.multinomial.Multinomial (an example mistake of docs)?
module: docs	How to get tolerance override in OpInfo-based test?
module: docs	Feature: support better rendering for ..deprecated Sphinx directive
module: docs	Docs bug: type annotations for linspace (and logspace) start and end arguments is wrong
module: docs	Split up torch.distributions docs into multiple pages
module: docs	Transformer Initialization
module: docs	recurrent neural network module
module: docs	Clarify the behavior of DataLoader sampler and batch_sampler parameters
module: docs	Negative Exponents of Int tensors result in output of zero
module: docs	[docs] Tensor.uniform_ docs are not clear about whether from/to boundary values are included in sampling or not
module: docs	Error in torch.trapz documentation
module: docs	nn.Batchnorm1d input shape notation inconsistency
module: docs	[docs] nn.Sequential docs should list member functions
module: docs	Error in torch.Tensor.logit documentation
module: docs	argmin/argmax incorrect doc for the first form
module: docs	Cannot compile C++ documentation: Sphynx assertion
module: docs	Multiple invalid summaries in torch.nn documentation page
module: docs	Building docs locally fails
module: docs	LazyModules cls_to_become field exposes implementation detail
module: docs	Shape parameter inconsistency in torch.Tensor.view, torch.reshape, torch.Tensor.reshape
module: docs	Error in torch.cdist documentation
module: docs	Docs for torch.nn.MSELoss are confusing
module: docs	torch.cuda.set_per_process_memory_fraction() does not perform VRAM isolation
module: docs	The formula for KL-divloss is wrong in the document
module: docs	torch.hstack should raise an error when tensor is 0 dimensional
module: docs	Citation request for "probabilities for each class" in the doc's description on cross-entropy
module: docs	Lazy Tensor Core Documentation Out-of-Date
module: docs	Clarify variables of BatchNorm*d functions
module: docs	[docs] Clarify DDP activation checkpointing support
module: docs	Incorrect documentation for BCEWithLogitsLoss weight?
module: numpy	torch.cartesian_prod inconsitent return shape for one input tensor
module: numpy	[dynamo] Fix np.issubdtype
module: numpy	[dynamo] Tests using kstest are being killed in CI
module: numpy	DISABLED test_dunder_round_edgecases_val_2147483647_ndigits_-1 (__main__.TestNonarrayArgs)
module: numpy	Unexpected Results in PyTorch Tensor Operations with Python Scalars
module: numpy	[torch._numpy] implement ndarray.tobytes
module: numpy	DISABLED test_numpy_non_writeable_cpu (__main__.TestNumPyInteropCPU)
module: numpy	WIP: fix compiling np.array(list_of_arrays)
module: numpy	Tracker for torch._numpy errors under dynamo
module: numpy	[discussion] Have PyTorch functions support python scalars (like NumPy) + introduce convenience constants like torch.pi and torch.e and maybe analogue of scipy.constants namespace
module: numpy	ValueError issued instead of TypeError when tensor is cast to a scalar
module: numpy	On the correctness of torch.signal.windows.cosine
module: numpy	TypeError: Got unsupported ScalarType BFloat16
module: numpy	Allow try except check for numpy bfloat16 representation
module: numpy	bytes(...) support of torch tensor does not match numpy + it would be nice to support tensor.tobytes() as alias
module: numpy	NumPy 2.0 Support
module: numpy	torch.polygamma inconsistent with scipy.special.polygamma for n >= 1
module: numpy	Turn indexing with a scalar tensor into an copy into a view and avoid a D2H synchronization.
module: numpy	Multiple dimensions support for torch.max
module: numpy	Implement to_numpy method to speed up matplotlib with PyTorch arrays
module: numpy	Add support for bfloat16 in torch.from_numpy()
module: numpy	torch.lobpcg producing different largest eigenvalue than scipy and np.linalg.eig
module: numpy	[Indexing] Incoherent Tensor indexing for nested lists
module: numpy	Test Failure: TestUnaryUfuncsCPU.test_reference_numerics_normal_cos_cpu_float32 on s390x
module: numpy	Reversing along a dimension, similarly to numpy
module: numpy	[numpy] mean & nanmean should support int dtypes
module: numpy	Better Numpy API (interoperability between ML frameworks)
module: numpy	Process get killed when running torch.normal
module: numpy	Cannot cast float64 to float32
module: numpy	Degenerate ranges are allowed in NumPy, but not in PyTorch.
module: numpy	Support divmod for tensors
module: numpy	Bfloat16 tensor .numpy() support
module: numpy	Tensor indexing and slicing documentation should explicitly state that indexing follows numpy semantics and link to the numpy indexing documentation.
module: numpy	Different behavior for complex numbers operations with numpy
module: numpy	Importing numpy makes Tensor min max crash
module: numpy	torch.equal can still run successfully when the parameter types are different.
module: numpy	Edge case: CPU bool abs is not supported
module: numpy	[PT][1.13] torch .numpy() fn broke for some scenario
module: numpy	torch.clamp does not clamp out of -0 from 0 when ran on the CPU
module: numpy	Drop deprecated behavior from NumPy-style T
module: numpy	Feature request: Tests for int should be tests for numbers.Integral
module: numpy	Inductor stable baselines assertion errors
module: numpy	Creating NumPy array with dtype=object of PyTorch tensors fails
module: numpy	Can we rewrite numpy operators to pytorch operators?
module: numpy	Set dtype if tensor converted to numpy
module: numpy	list of tensors can't be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: numpy	torch.angle differs from np.angle for -0.
module: numpy	Operating on boolean torch tensor and numpy array casts to unit8
module: numpy	[numpy] Missing Tensor-Scalar support for multiple binary ops
module: numpy	[doc] view appears to mean different things, view/reshape vs transpose/permute.
module: numpy	torch.histogram has wrong output dtype and doesn't support integer inputs
module: numpy	Possible issue with memory allocation.
module: numpy	Inconsistent numpy indexing
module: numpy	Fancy indexing bug when combining masks with indexes
module: numpy	[numpy compat] torch.stack and torch.tensor doesn't support nested list+tensors (NumPy does support) - at least document the difference in the error message
module: numpy	torch.diag unexpectedly fails
module: numpy	[feature request] Exponential moving average (EMA) of a tensor across a dimension
module: numpy	boolean mask + ellipsis lead to incorrect indexing
module: numpy	[feature request] Support like= argument in tensor factory methods
module: numpy	[RFC] Support MemoryView for Tensors
module: numpy	torch.tensor relies on implicit conversion being deprecated in Python 3.10
module: numpy	RFC: “What’s in a (NumPY) name?” — Changing PyTorch Operator Names
module: numpy	[feature request] torch.clamp on BoolTensors
module: numpy	Importing numpy interacts with tensor.sum perf
module: numpy	Missing doc for torch.segment_reduce
module: numpy	The uniform operator param names in the C++ impl use python keywords
module: numpy	torch.mul is not consistent with torch.multiply
module: numpy	[numpy] Add torch.newdim/torch.newaxis
module: numpy	[numpy] Add iscomplexobj and isrealobj
module: numpy	Indexing a tensor with a NumPy array sometimes works and sometimes doesn't
module: numpy	Quantile is limited to 16 million elements and have poor performance.
module: numpy	[numpy] torch.nonzero is similar to np.argwhere not np.nonzero
module: numpy	[feature request] numpy.append / numpy.insrt / numpy.delete equivalents and implement dynamic arrays (reallocate storage with a surplus)
module: numpy	as_tensor and negative strided np arrays
module: numpy	torch.unique acting up for a binary tensor
module: numpy	lstm's input h0 and c0 bug
module: numpy	Serialising torch.bool generates a warning about np.bool being deprecated.
module: numpy	[feature request] Type promotions for Boolean tensors with sub operation + Numpy compatability
module: numpy	Make axes selection keyword arguments in torch.diagonal and torch.transpose consistent
module: numpy	Runtime error when passing dim as None in torch.squeeze (and Tensor.squeeze)
module: numpy	Wrong implementation of method log_prob in torch.distributions.negative_binomial
module: numpy	Should we be upcasting integral types to int64 in torch.sum and torch.prod?
module: numpy	Reducing over empty dimensions for reductions without identity
module: numpy	Better argument names for torch.atan2 and other math functions
module: numpy	Pytorch model load failure in Gunicorn with Gevent workers
module: numpy	Many reduction operators do not support reducing over multiple dimensions
module: numpy	Function Request: scipy.ndimage.map_coordinates
module: numpy	Deprecate torch.(min|max|median|mode) to only return values and not indices
module: numpy	Implement missing torch.nan* operators
module: numpy	Reduce with any(), all(), median() over multiple dimensions
module: numpy	Reductions tracking issue
module: numpy	torch.linspace tensor support
module: numpy	Add a NumPy-like pad function
module: numpy	[Bug] numpy is no longer a required dependency
module: numpy	Documentation for torch.finfo doesn't match implementation
module: numpy	Support complex numbers in at::nan_to_num.
module: numpy	Multiplication of torch.tensor with np.array does the operation with numpy.
module: numpy	precision/consistency issue in linspace
module: numpy	[feature request] torch.as_tensor to support any object that NumPy's asarray or array can consume (consume __array_interface__)
module: numpy	einsum "jk,ijkl->il" is ~16x slower than numpy
module: numpy	Attempting to concatenate scalar tensors throws a runtime error
module: numpy	scatter does not accept scalar src=
module: numpy	Support expand_dims
module: numpy	{h / v / d}split methods are missing
module: numpy	Add compute_residuals flag for torch.linalg.lstsq
module: numpy	torch.allclose does not allow different types for comparison
module: numpy	Feature Request: Add a rounding mode to round
module: numpy	endpoint=False for torch.linspace and torch.logspace
module: numpy	Vertices=torch.matmul(vertices.unsqueeze(0), rotations_init), RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched in CentOS
module: numpy	RuntimeError: "log2" "_vml_cpu" not implemented for 'Half'
module: numpy	torch.sqrt for negative values should either return complex tensors or clearly throw a domain error/warning
module: numpy	[Feature request] Add batched matrix support for torch.diag
module: numpy	torch.pow returns incorrect value for 0^0j
module: numpy	torch.pow(tensor, tensor) throws RuntimeError for dtype bool
module: numpy	Make searchsorted and bucketize API consistent
module: numpy	Support Array Interface (__array_interface__ attribute)
module: numpy	torch.put is divergent from np.put
module: numpy	Have the possibility to reduce a tensor with median on more than one specified dimension
module: numpy	Add complex support for torch.unique
module: numpy	Find a good namespace home for torch._assert_async
module: numpy	Dimension argument names in torch.diag_embed/diagonal vs. transpose/transpose_
module: numpy	Reset mask for torch.cumsum?
module: numpy	'Tensor' object has no attribute 'astype'
module: numpy	torch.searchsorted issues
module: numpy	Creating torch tensor as a function of index value
module: numpy	Cumulative integration?
module: numpy	Normal_like operator
module: numpy	Reciprocals of complex tensors with infinities are different from NumPy.
module: numpy	Implementation of many complex functions is fast but inaccurate in libc++
module: numpy	tolist called on torch scalar does not return a list => proposal to support new arg force = True to provide a work around
module: numpy	Allow F.pad(mode = 'reflect') when shape == pad
module: numpy	.numpy() array failes to keep original storage around
module: numpy	[numpy] round and trunc not supported for Integral Type while Python and NumPy supports them
module: numpy	result_type doesn't take dtypes and doesn't match numpy
module: numpy	[ux] Proposal to have t() === transpose(-1, -2), since batches are very frequent
module: numpy	[numpy] torch.ceil and torch.floor don't support integer inputs while Numpy does
module: numpy	[Feature Request] Support tensor creation from objects that implement the __array__ interface
module: numpy	torch.unique(x, dim=1, return_inverse=True) returns inverse for only the last sub-tensor along dim
module: numpy	[feature request] torch.scan (also port lax.fori_loop / lax.while_loop / lax.associative_scan and hopefully parallelized associative scans)
module: numpy	error: conversion from ‘std::vector<at::Tensor>’ to non-scalar type ‘at::Tensor’ requested
module: numpy	torch.tril_indices is incompatible with np.tril_indices
module: numpy	torch.special tracking issue
module: numpy	NumPy Compatibility tracking issue
module: numpy	RFC: identify analogous NumPy operators when documenting PyTorch operators
module: numpy	torch.any and torch.all map uint8 -> uint8 but should map uint8 -> bool
module: numpy	Interpolation tracking issue
module: numpy	Function request: scipy.interpolate.InterpolatedUnivariateSpline
module: numpy	Function request: scipy.interpolate.griddata
module: numpy	Function request: scipy.interpolate.RegularGridInterpolator
module: numpy	Function request: scipy.interpolate.RectBivariateSpline
module: numpy	Function Request: scipy.ndimage.zoom
module: numpy	Function Request: scipy.interpolate.interp1d
module: numpy	Function Request: np.interp
module: numpy	Function Request: scipy.stats.pearsonr
module: numpy	torch.Tensor.random_ is divergent from NumPy's np.random.random
module: numpy	torch.meshgrid is divergent from np.meshgrid
module: numpy	torch.transpose is divergent from np.transpose
module: numpy	torch.equal is divergent from np.equal
module: numpy	torch.cross is divergent from np.cross
module: numpy	Multinomial without replacement produces samples that have zero probability
module: numpy	torch.Tensor.repeat is divergent from np.repeat
module: numpy	torch.split is divergent from np.split
module: numpy	torch.var and torch.std are not compatible with np.var and np.std
module: numpy	Apparent Memory Leak with torch.as_tensor
module: numpy	Looking for a more convenient way to getter value of the upper/lower triangular matrix into 1D
module: numpy	Please add "dim" feature for function "torch.masked_select"
module: numpy	Add docs on PyTorch - NumPy interaction
module: numpy	[complex] torch.abs: does not match numpy
module: numpy	[FR] bool tensor should support basic arithmetics
module: numpy	torch.float128 datatype
module: numpy	[complex] torch.{exp}: does not match numpy
module: numpy	Inconsistent complex results with NumPy when computing non-positive power of 0
module: numpy	Factorial & Binomial Coefficient
module: numpy	Mixing Numpy's arrays and PyTorch tensors
module: numpy	PyTorch NaN behavior and API design
module: numpy	Advanced indexing: allow combining Boolean & integer index
module: numpy	Support "symmetric" reflection padding
module: numpy	torch.mode when input has nans
module: numpy	Better support for operators that return (named) tuples of tensors
module: numpy	Squared 2-norm pdist (as available in SciPy / Faiss)
module: numpy	Add support for reading the whole file in from_file
module: numpy	Support torch.mean for BoolTensors and other integer tensor inputs (without manual upcasting and hopefully without hidden upcasting)
module: numpy	torch.nonzero(t, as_tuple=...) does not work with the JIT because the as_tuple signatures are not exposed properly
module: numpy	Boolean indexing of an ndarray with a torch.tensor mask breaks for size=1
module: numpy	Custom Datatypes in Tensors
module: numpy	Tensordot does not support Bool
module: numpy	[feature request] dtype argument for torch.sign
module: numpy	Support keep stride for neg with requires_grad=False
module: numpy	Adding entropy function analogous to SciPy
module: numpy	Accessing elements of tensor with multi-dimensional index results IndexError
module: numpy	torch.dot throws an error for input tensors of different dtypes
module: numpy	Unbuffered operation
module: numpy	[discussion] Support __round__ magic
module: numpy	Implementing packbits
module: numpy	Memory usage of torch.nn.functional.interpolate increased with v1.5.0 when run on numpy input
module: numpy	Function request: np.copy (alias of clone?)
module: numpy	out-variant for tensor.bitwise_and (exists for torch.bitwise_and) + bitwise_friends
module: numpy	New Feature : A very fast algorithm for computing matrix rank
module: numpy	torch.sign is divergent from numpy.sign on NaN
module: numpy	out= resizing (and restriding) behavior is confusing
module: numpy	Enhance supported types of functional.pad
module: numpy	Inconsistent behavior between numpy.exp and torch.exp on CPU for complex numbers
module: numpy	Converting NumPy dtype to Torch dtype when using as_tensor
module: numpy	please add 'tensor.astype(dtype_string)' syntax for numpy interoperability
module: numpy	Broadcasting for torch.cross
module: numpy	Make torch.cross dim parameter work intuitively
module: numpy	test_float_to_int_conversion_finite_cpu_int16 is failing on MacOS
module: numpy	torch.as_tensor(np_array) is sometimes much faster than torch.tensor(np_array)
module: numpy	Provide an issubdtype API
module: numpy	Support alternate casting rules
module: numpy	PyTorch's rot90 returns a new tensor, inconsistent with NumPy's returning a view
module: numpy	PyTorch's flip returns a new tensor, but NumPy's flip returns a view
module: numpy	Add Numpy-like "order" argument to reshape
module: numpy	equal_nan keyword not implemented for complex torch.isclose
module: numpy	return_index option for torch.unique
module: numpy	Allow __array__ to automatically detach and move to CPU
module: numpy	Wrong results for multiplication of non-finite complex numbers with real numbers
module: numpy	Comparison ops for Complex Tensors
module: numpy	torch.ceil, torch.floor should accept a dtype argument
module: numpy	Half type promotion with Numpy arrays is incorrect
module: numpy	Support tensor.cumsum() for 1-dim tensors
module: numpy	Segfault when using misaligned data pointer (from joblib)
module: numpy	Support creating a CPU tensor from ctypes pointer in Python / from_blob(ptr, shape, strides, dtype)
module: numpy	[docs] Unclear arg spec for torch.full
module: numpy	bytearray(tensor) behaves very differently from bytearray(tensor.numpy())
module: cpp	The Python module installs cmake files and headers under the Python's site-packages directory that can't be used
module: cpp	DISABLED test_cpp_frontend_module_has_same_output_as_python (__main__.TestCppExtensionJIT)
module: cpp	Runtime Errors with convolution_backward_out When Handling Optional Bias Gradient
module: cpp	[cppdocs] torch::load function istringstream example typo
module: cpp	libtorch can't switch between with and without layers by is_training()
module: cpp	Unexpected poor performance of C++ extension / wish for a fast operator[]
module: cpp	In version 2.1, libtorch needs to be woken up every time it is called after the model is initialized, which means that every time the model is called, it is very slow to predict the first picture.
module: cpp	Parameters between models don't copy in the C++ Pytroch Frontend under windows
module: cpp	Tensors Can't be  Overwritten in Visual Studio Windows
module: cpp	Request to add system requirements to doc
module: cpp	libtorch exports miniz symbols
module: cpp	Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors
module: cpp	Add more flexibility on print / output console
module: cpp	Static Linking C++, Op not available at runtime
module: cpp	M2 Failing to build example-app in c++
module: cpp	[C++ Frontend] Simple Changes for Cleaner Options
module: cpp	libtorch: runtime error when iterating batch of dataloader
module: cpp	[CPP API] Add Adadelta, Adamax, ASGD, NAdam, RAdam and Rprop
module: cpp	C++ API torch::nn::MultiheadAttention Crashes by division by zero
module: cpp	CUDA device support does not register allocator to c10::GetAllocator(...)
module: cpp	vmap, jacrev, jacfwd, hessian, etc., in libTorch
module: cpp	Libtorch report C10 error when compiling on my own project
module: cpp	Get errors after compiling and running PyTorch MINIMAL EXAMPLE for c++ Mac M1 with make
module: cpp	LibTorch 2.0.1 scripting in Debug mode on Windows
module: cpp	type conflict
module: cpp	libtorch > 1.9.1 produces segfault on Qt5 gui application exit
module: cpp	"Y.getIntrusivePtr()->set_storage(X.getIntrusivePtr()->storage()); " in C++ is not supported
module: cpp	File Missing When i build with C++
module: cpp	[Pytorch 2.0] torch::nn::Dropout output is incorrect on Windows
module: cpp	after add /path_to_libtorch/libtorch/lib to LD_LIBRARY_PATH, I can't import torch_scatter.
module: cpp	GLCM implementation in pytorch C++ api and cuda
module: cpp	Extending compatibility of LibTorch
module: cpp	2.0.0+cu118 package missing proper libnvrtc-builtins.so.11.8
module: cpp	Libtorch consumes too much memory as 16225
module: cpp	Move template code to header
module: cpp	TORCH_LIBRARIES variable leads to undefined reference function error in compiling while using libtorch in c++
module: cpp	Some c++ library docstrings incorrectly linked/repeated
module: cpp	Linking error with Libtorch
module: cpp	Compiling libtorch from Source on Mac Beyond v1.11.0
module: cpp	pytorch prune in libtorch
module: cpp	A few functions in fbgemm_utils.cpp are defined in global namespace
module: cpp	build: cmake: need to uniformize installation of libraries in CMAKE_INSTALL_LIBDIR (not lib)
module: cpp	kind_.is_prim() INTERNAL ASSERT FAILED at "../torch/csrc/jit/ir/ir.cpp":1098
module: cpp	caffe2_interface_library CMake macro prevents linking to LibTorch as a transitive dependency
module: cpp	1.12.1 incompatible with c++ built for 1.12.0 and vice versa
module: cpp	link error happen when intergrate libtorch to other tool
module: cpp	C++ Adagrad optimizer doesn't initialize parameter state
module: cpp	libtorch create a tensor is very slow, who can tell me why
module: cpp	Install LibTorch by Conan or other C++ package manager
module: cpp	would you like  upload to the cpp libtorch to  vcpkg  package repo?
module: cpp	scripted fasterRCNN model cannot be loaded with libtorch c++ API
module: cpp	libtorch malloc cause coredump
module: cpp	Libtorch C++ torch::stack error
module: cpp	Is there Doc that explains how to call an extension op in another extension implementation?
module: cpp	When using libtorch v1.10.2, calling at::slow_conv_dilated3d directly returns wrong results on cpu backend
module: cpp	[bug] the output shape from torch::mean and torch::var is different  in libtorch
module: cpp	forward program terminated from __cxa_pure_virtual
module: cpp	torch.ops.aten.find inconsistent with str.find
module: cpp	[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer
module: cpp	PyTorch leaks a macro definition called "CHECK" in the C++ version
module: cpp	Doc on index of CPU Device seems wrong
module: cpp	fatal_signal_asan_no_sig_test in current master hang.
module: cpp	Remove const from function return type if returning const value
module: cpp	libtorch1.8 torch::sigmoid is wrong
module: cpp	Undefined symbol error when compiling and loading C++ extension
module: cpp	SymInt shouldn't be in dynamic_type.h
module: cpp	clang-tidy "error: do not use const_cast" cppcoreguidelines-pro-type-const-cast is counterproductive
module: cpp	Store SourceDataset in MapDataset using pointer
module: cpp	C++ Context::setDeterministicAlgorithms default 2nd arg not defined in header
module: cpp	Segfault on unloading a model
module: cpp	libtorch need operator= in torch::Device
module: cpp	Unable to build and use libtorch function via pybind11: undefined symbol error upon import
module: cpp	Followup requires for MKL link issue / cannot find -lmkl_core
module: cpp	[libtorch]can not save a  vector<int> to AutogradContex->saved_data.
module: cpp	Calling .backward() inside of an LBFGS closure function throws an exception in Libtorch v1.6.0+
module: cpp	Leaky cmake cuda compile options
module: cpp	path\tp\torch\torch.h(14,1): fatal error C1001: Internal compiler error.
module: cpp	C++ torch::nn::Sequential clone() method overwrites child module names
module: cpp	libtorch cuda use too much system memory
module: cpp	NewOperatorRegistrationTest.testImplNoDefGetsCaught failed.
module: cpp	Include Declarations.yaml in Libtorch distributions
module: cpp	C++ at::Tensor's pinned_memory status is not printing out correctly.
module: cpp	Request to revise the Pytorch tutorial.
module: cpp	Domain Transformation APIs for LibTorch and LibTorch-Lite
module: cpp	libtorch: collate_fn equivalent
module: cpp	Add support for c++ Profiler APIs
module: cpp	Document how to generate Pybind bindings for C++ Autograd
module: cpp	Scalar construction in C++: PYtorch.tensor(7) !~= C++at::tensor(7)
module: cpp	Cannot include extension.h under Windows - linker error  THPVariable_Wrap
module: cpp	c10::optional<T> operators should delegate to corresponding operators on T
module: cpp	libtorch on Apple m1
module: cpp	inerror: reference to ‘DeviceType’ is ambiguous
module: cpp	C++ version 1.9.0 libtorch dynamic load fails -- GCC only
module: cpp	m.fallback(torch::CppFunction::makeFromBoxedFunction<&my_fallback>) gives bad error message
module: cpp	what():  result type Float can't be cast to the desired output type Long
module: cpp	Libtorch segfault in packed GRU evaluation with cuda batch_sizes
module: cpp	c++ use pybind11 to import torch  free(): invalid pointer
module: cpp	Named Tensors in C++ Is Undocumented
module: cpp	[cpp op] TORCH_LIBRARY schema doesn't respect keyword only
module: cpp	Get a thread safe copy of torch::nn::Sequential object
module: cpp	TensorExpr LoopNest.get_loops_for misbehaved after loop distribution transformation
module: cpp	Provide half-away-from-zero rounding mode on Tensor::round
module: cpp	c++ convert from std::vector<Tensor> to c10::List<optional<Tensor>>
module: cpp	hope to support something like  "torch::manual_seed_for_mulit_thread"
module: cpp	terminate called after throwing an instance of 'c10::Error'   what():  isTuple() INTERNAL ASSERT FAILED at "/home/wenda/libtorch/include/ATen/core/ivalue_inl.h":927, please report a bug to PyTorch. Expected Tuple but got GenericList
module: cpp	Properly design manual_cpp_binding (make it less error prone)
module: cpp	How to delete Module from GPU? (libtorch C++)
module: cpp	doxygen and pytorch documentation
module: cpp	profile pure C++ process
module: cpp	Gradient checkpointing support in C++ API
module: cpp	code linking to libtorch cannot use thrust/cub functions
module: cpp	Error: ‘str’ is not a member of ‘c10’; did you mean ‘c10::aten::str’? while using libtorch
module: cpp	Status of pip wheels with _GLIBCXX_USE_CXX11_ABI=1
module: cpp	libtorch gpu set id bug
module: cpp	out variant of many loss functions are not consistent with non-out variant when reduction is not none
module: cpp	Provide a set of C++ foreach APIs that will take tensor pointers as an input
module: cpp	Inconsistent Variable Naming in FindTorch.cmake
module: cpp	from_blob segfaults when given CUDA pointer
module: cpp	how could I print the log in source code
module: cpp	at::size documentation conflict
module: cpp	PyTorch1.3.1 Can not using namespace torch::indexing
module: cpp	The documentation for c10::Dict is completely empty.
module: cpp	Do we have plan to offer C++ binding for prune related features.
module: cpp	Functional interface for optimizers
module: cpp	Mention accessor/data_ptr for raw memory access in Libtorch index API document and discuss performance implications
module: cpp	“doxygenfunction: Unable to resolve multiple matches for function ...” in C++ documentation
module: cpp	Implement a set_printoptions method in libtorch
module: cpp	Do not call nullptr deleter in at::fromDLPack (dlpack)
module: cpp	Add support for user defined types in serialization in libtorch
module: cpp	Optimizer support via Libtorch C++ on Android
module: cpp	can not use nn::Functional(torch::softmax(-1)) in Sequential
module: cpp	[C++] adding type checking or type casting to torch::PackedTensorAccessor indexing
module: cpp	c++ indexing vs python
module: cpp	libTorch cpp docs missing for Tensor::item()
module: cpp	The new version of the libtorch become slow
module: cpp	Will the model run slower when deployed using libtorch ?
module: cpp	Custom c++ extension build process doesn't preserve color from compiler
module: cpp	Suppress scientific notation in libtorch
module: cpp	Libtorch C++ multiple GPU performance slower than single GPU
module: cpp	C++ API for torch.autograd.functional.jacobian
module: cpp	Updating learning rate with Libtorch 1.5 and optimiser options
module: cpp	libtorch 1.5 macos crash when loading on some mac
module: cpp	Valgrind leak checking flags losses in libtorch
module: cpp	Add cpack support to CMakeLists.txt
module: cpp	LibTorch 1.5.0 not supporting GLIBC < 2.23
module: cpp	Compiling errors when trying to cross-compile the C++ API for RTOS (QNX)
module: cpp	Failed to link torch_library using cmake
module: cpp	Pybind11 cpp extensions broken with pytorch v1.5.0
module: cpp	Problem with c10/utils/variant.h
module: cpp	pytorch and c++ inference disagree
module: cpp	Add load_state_dict and state_dict() in C++
module: cpp	Issue when linking C++ code with libtorch_cpu: cuda not detected
module: cpp	In AutogradContext, get_saved_variables() should be renamed to get_saved_tensors()
module: cpp	C++ tensor print doesn't show requires_grad and grad_fn like Python tensor print
module: cpp	torch.autograd.set_detect_anomaly(True) does not exist in C++?
module: cpp	Backward function causes device error in C++ when changing module's device repeatly.
module: cpp	BatchNormFuncOptions object cant be printed in C++
module: cpp	Simple C++ custom autograd function code throws error "CUDA error: driver shutting down"
module: cpp	LibTorch API on Mobile
module: cpp	libtorch for Windows. MNIST example does no work.
module: cpp	How do you change Adam learning rate since the latest commits ?
module: cpp	torch::normal only supports (double, double), but at::normal supports (double, double) / (double, Tensor) / (Tensor, double) / (Tensor, Tensor)
module: cpp	Generator C++ API should match Python API
module: cpp	libtorch memory leak
module: cpp	[C++ API Parity] Incorrect documentation for optim initialization in serialization docs
module: cpp	libtorch.so file size is very large
module: cpp	Model loaded in C++ runtime is not thread safe
module: cpp	Models saved in C++ LibTorch with torch::save, cannot be loaded in python using torch.load
module: cpp	Make setter non-optional, e.g., TensorOptions::device(optional<Device>) -> device(Device), and add a device_opt setter
module: cpp	torch::var_out and dimnames
module: cpp	Don't take TensorOptions by reference
module: cpp	[v1.5] Python/C++ API parity master tracking task
module: cpp	Logical AND and OR for Tensors in C++ API.
module: cpp	Segmentation fault in C++ API torch::from_blob(...).clone()
module: cpp	DistributedStreamSampler: support stream sampler in distributed setting
module: cpp	PyTorch C++ API docs only tracks master branch
module: cpp	C++ randint returns float32, python returns int64
module: cpp	Compile libtorch by source code failed.
module: cpp	Parallelization: more balanced work distribution among workers
module: cpp	torch::nn::functional::interpolate crash
module: cpp	LibTorch, Error in 'xxx': free(): invalid pointer
module: cpp	Add support for multidimensional input to at::tensor
module: cpp	torch::tensor(scalar) behaves differently from at::tensor(scalar)
module: cpp	Provide rpc, remote and dist autograd C++ APIs and register them as Prim::ops
module: cpp	Python/C++ API Parity: torch.optim optimizers
module: cpp	how to use libtorch library in cuda file with nvcc compiler(c++)?
module: cpp	Deployment training model at C + + end
module: cpp	I can't set gpu is 1 it always use gpu 0
module: cpp	torch::NoGradGuard no_grad get wrong  when I use batchsize!=1
module: cpp	Missing bin and include when building with torchvision on CentOS
module: cpp	AnyValueTest.CorrectlyAccessesIntWhenCorrectType UBSAN failure: owncast of address 0x60300105d750 which does not point to an object of type 'Holder<const int>' Sep 27 00:01:03 0x60300105d750: note: object is of type 'torch::nn::AnyModule::Value::Holder<int>'
module: cpp	torch::nn::Sequential not compatible with torch::nn::RNN
module: cpp	[libtorch]Same model in CUDA and CPU got different result?
module: cpp	[Feature Request] Trace / Script C++ models
module: cpp	[C++] Module::pretty_print is broken
module: cpp	Forward/backward hooks for C++ torch::nn modules
module: cpp	Python/C++ API Parity: torch.nn modules and functional
module: cpp	libtorch forward memory leak
module: cpp	[C++] Support negative index in torch::TensorAccessor::size()
module: cpp	pytorch c++ api cannot call operator() on torch::nn::Sequential
module: cpp	Build link not right
module: cpp	Consider not checking in autogenerated core/{Tensor.h,TensorMethods.h}
module: cpp	CUDA: THTensor code complains about devices not matching when creating tensor from blob
module: cpp	Error while using Libtorch + OpenCV + Qt Creator
module: cpp	Libtorch with deeplabv3_resnet101 will not forward.
module: cpp	[c++] torch::conv2d() expected output_padding to be a single integer value or a list of 3 values
module: cpp	make[2]: *** No rule to make target 'libtorch/lib/libc10.so'
module: cpp	libtorch new op
module: cpp	nn.modules.functional.h does not support optional arguments
module: cpp	contradictory output values
module: cpp	Crash when using tensor.set_data() function in libtorch on windows
module: cpp	torch::zeros is slow for small tensors (C++)
module: cpp	how libtorch can work with  tensor data as same as  pytorch
module: cpp	output values not same and much slower than Python API
module: cpp	Segmentation fault when use torch::from_blob
module: cpp	c++ torch::nn::Sequential increments count on name errors
module: cpp	libtorch+opencv Mat result error: different from the python ones
module: cpp	C++ API 'nn::Sequential' has inconsistent behavior with python conterpart
module: cpp	How to load PyTorch model with LSTM using C++ api
module: cpp	Multi-gpu via torch::nn::parallel::data_parallel
module: cpp	Performance issue with torch.jit.trace(), slow prediction in C++ (CPU)
module: cpp	C++ custom module not thread safe
module: cpp	documentation for C++ / libtorch autograd profiler
module: cpp	[CPP] Allow binding config structs into the Python front end
module: cpp	Doubly freed pointer in torch::cat error handling when called via pybind11.
module: cuda	Add out_dtype support for sparse semi-structured CUTLASS back-end
module: cuda	[discussion] Route pointwise Conv1d/Conv2d to matmul?
module: cuda	RuntimeError: CUDA error: an illegal memory access was encountered using vmap and model ensembling call for cuda system
module: cuda	GPU tests can fail with invalid memory access due to compiler generating invalid code
module: cuda	import torch results in cuInit call
module: cuda	Building project using libtorch results in "Failed to find nvToolsExt"
module: cuda	Potential cublas handle leaking
module: cuda	[CUDA] Raise softmax_forward_64bit_indexing GPU memory requirement
module: cuda	[FR] Add a way to reserve memory that survives torch.cuda.empty_cache()
module: cuda	Document required semantics of allocation functions in CUDAPluggableAllocator
module: cuda	Latest PyTorch is not buildable against CUDA-11.2
module: cuda	Pytorch with GPU support compile error on Jetson Xavier RX
module: cuda	RuntimeError: handle_0 INTERNAL ASSERT FAILED at "../c10/cuda/driver_api.cpp":15
module: cuda	Optimize cudnn_convolution_out to Reduce Unnecessary Memory Allocation and Copy
module: cuda	[Bug] Big difference between the output of Conv float precision and double precision
module: cuda	Batchnorm NAN in amp autocast mode.
module: cuda	max_pool2d_with_indices_backward_out_cuda: remove useless  code gradInput.zero_();
module: cuda	RuntimeError: CUDA unknown error even after sudo modprobe -r nvidia_uvm && sudo modprobe nvidia_uvm
module: cuda	DISABLED test_noncontiguous_samples_nn_functional_conv3d_cuda_complex64 (__main__.TestCommonCUDA)
module: cuda	Speed up triu_tril_kernel
module: cuda	CUDAExtension no longer works with ccache
module: cuda	Nvidia P100, where to disable upcasting? Plus kernel image missing.
module: cuda	torch.einsum may choose a strategy for which there is not enough memory
module: cuda	single-batch torch.bmm is significantly slower with cuBLAS>12.1.0
module: cuda	inconsistency between nan cast to int32 on CPU and GPU
module: cuda	inconsistency on torch.clamp
module: cuda	PYTORCH_NO_CUDA_MEMORY_CACHING=1 with torch.multiprocessing shared tensors seems to perform use-after-free
module: cuda	Tensor copied over to multiple GPUs on its own
module: cuda	Align on the minimum supported Linux version (CentOS 7 is EOL in july 2024)
module: cuda	Unable to build on CUDA 11.8 due to cutlass incompatibility
module: cuda	[Tracker] Inconsistencies between CPU and GPU computation
module: cuda	Unexpected None value for stream with dynamo
module: cuda	torch.matrix_exp(x) get inf and nan
module: cuda	IndexError: map::at with MPI CUDA collectives
module: cuda	iSTFT gives wrong results for some batched input
module: cuda	Unknown CUDA Architecture Name 9.0a in CUDA_SELECT_NVCC_ARCH_FLAGS (compiling from source)
module: cuda	RuntimeError in use torch 2.1.0 cuda 11.8
module: cuda	[cusparseLt] CUDA error: internal error when calling cusparseLtStructuredDescriptorInit
module: cuda	The cuda batched GEMM has a poor performance for bigger batch size with smaller matrix size
module: cuda	[cuDNN][cuDNN V8 API] cuDNN Flash-Attention Upstreaming RFC/tracking issue
module: cuda	interpolate::trilinear returns wrong gradients on CUDA
module: cuda	t.contiguous() ~10 slower in eager mode compared to torch.compile
module: cuda	CUDA extension error message doesn't look correct
module: cuda	Enhanced RNG State Management with Index-Based Control for Graph-Safe Tensor Parallelism
module: cuda	cuDNN error: CUDNN_STATUS_MAPPING_ERROR on gtx_1080/A10 when conv1d is called
module: cuda	Early testing stop logic for CUDA error looks wrong for instantiated_test with pytest
module: cuda	torch._C._cuda_getDeviceCount inflates system memory usage
module: cuda	[CUDA-12.2] cuSPARSE deprecated support for sparse BSR
module: cuda	Training a network SUPER slow with Pytorch 2.1
module: cuda	High dimensional grid sample
module: cuda	log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16
module: cuda	RuntimeError: "grid_sampler_2d_cuda" not implemented for 'BFloat16'
module: cuda	RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at "/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp":1154, please report a bug to PyTorch
module: cuda	pyTorch 2.1 3x slower than 2.0
module: cuda	GroupNorm & InstanceNorm does not handle channels_last correctly
module: cuda	pytorch support for cuda 12.2 ?
module: cuda	Tensor .cuda() very slow with specific array sizes
module: cuda	yolov5_train
module: cuda	Simulating lower memory on GPU does not indicate simulated memory in error message
module: cuda	CUDA version 12.2 has differential accuracy when executing CPU and GPU
module: cuda	pytorch index_select is too slow
module: cuda	Regression on CUDA 12.1 for vanilla transformer layer
module: cuda	ncu python conv2d.py runs indefinitely after activating cudnn.benchmark
module: cuda	Error with monai SwinUNETR and checkpointing
module: cuda	Add mixed dtypes MM implementation based on CUTLASS upstream
module: cuda	test_pytorch_onnx_onnxruntime_cuda.py is not run in CI
module: cuda	Perf-Drop (factor=2) Ubuntu-vs-Windows on same PC (dual-boot)
module: cuda	cuda/tf32 docs are outdated
module: cuda	cudaMallocAsync cause too much fragmentation.
module: cuda	Skip cuda kernel launch with torch.sum when dimension length is 0
module: cuda	tan/tanh discrepancies with complex due to jiterator
module: cuda	RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED
module: cuda	Implmenet kthvalue for bfloat16 on CUDA
module: cuda	[BUG?] Why Allocator use stream to manage Block?
module: cuda	Performance degradation on AMD + A800 when computation is small
module: cuda	cuda rng state for 2.0.1 cannot be used for 2.1.0
module: cuda	F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
module: cuda	Parameters of cuda module zero out when used in multiprocessing
module: cuda	Allow reductions to write into pinned memory
module: cuda	Unnecessary cuda synchronizations that we should remove in PyTorch
module: cuda	uninformative OOM error
module: cuda	Breaking incompatibility with Cuda 12.2, pytorch stable, torchvision
module: cuda	backend-friendly distributions
module: cuda	torch.einsum() computes different results on cpu and cuda on A100 GPU.
module: cuda	Transformer performance drop due to slow PyTorch GEMMs
module: cuda	FlashAttentionV2 will OOM when building on ci/cd with default settings
module: cuda	C10_HOST_DEVICE for std::isnan(c10::complex<T>)?
module: cuda	[feature request] [discussion] Include basic ctypes bindings for cudart/cublasLt/cublas/nvrtc/cudnn with stock PyTorch
module: cuda	Inconsistent results when running torch.arctanh
module: cuda	[regression] Not getting CUDA error: device-side assert triggered on main for CUDA_KERNEL_ASSERT2
module: cuda	DataParallel scatter method split tensor wrong
module: cuda	Regression in text encoding
module: cuda	CUBLAS_STATUS_NOT_SUPPORTED
module: cuda	The difference between channels last backward and channels first backward of AvgPool2d on CUDA is too large
module: cuda	More Performant CachingHostAllocator for Pinned Memory Allocation
module: cuda	CUDA device support does not register allocator to c10::GetAllocator(...)
module: cuda	Distributed torch.linalg.eigh (and other functions) on cuda using cuSOLVERMG
module: cuda	Increasing batch size makes network forward 1000 times slower
module: cuda	Extreme slowdown of torch.mm for certain sizes and strides with bfloat16
module: cuda	Pytorch nighlty and openAI/triton cuda
module: cuda	PyTorch 2.0.x CUDA error: operation not supported when Tensor.to a different device
module: cuda	There is a big precision error between A100 and 3090 when using torch.matmul with fp16 precision
module: cuda	Repro str could be displayed with slightly wrong env vars
module: cuda	NotImplementedError: Could not run 'aten::_spdiags' with arguments from the 'CUDA' backend.
module: cuda	Unnecessary record_stream call for backend:cudaMallocAsync
module: cuda	rfftn and irfftn operations in pt2 return different results compared to v1.12.1
module: cuda	System memory leak when using different input size of torch.nn.Conv3d
module: cuda	DISABLED test_cuda_memory_leak_detection (__main__.TestCudaMultiGPU)
module: cuda	version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time reference
module: cuda	Support CUDA 12.2
module: cuda	Illegal Memory Access on H100 TestSparseCompressedTritonKernelsCUDA.test_triton_sampled_addmm_block_size_16_cuda_bfloat16
module: cuda	ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory
module: cuda	DISABLED test_mem_get_info (__main__.TestCudaMultiGPU)
module: cuda	Error 101: invalid device ordinal (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
module: cuda	Tensor to_sparse fails on large matrices
module: cuda	Regressions with torch.compile + amp + ddp with recent nightly builds
module: cuda	Upgrading SpGEMM algorithm to resolve Cusparse SpGEMM insufficient resources problem
module: cuda	interpolate with antialias=True on CUDA doesn't work if the difference of spatial size is large
module: cuda	torch.cuda.mem_get_info to return 0 if CUDA context isn't initialized
module: cuda	ImportError: undefined symbol: cublasSetWorkspace_v2, version libcublas.so.11
module: cuda	DISABLED test_mem_get_info (__main__.TestCuda)
module: cuda	torch.cuda.memory_reserved always returns 0 bytes
module: cuda	torch.svd fails on large matrices
module: cuda	DISABLED test_Conv2d_dilated_cuda_tf32 (__main__.TestNN)
module: cuda	[cuda] Switching CI to CUDA 12.1 timing out linux-bionic-cuda12.1-py3.10-gcc7 / test (distributed, 3, 3, linux.8xlarge.nvidia.gpu)
module: cuda	Can't reproduce/non-deterministic results with CUDA
module: cuda	DISABLED test_compare_cpu__refs_empty_strided_cuda_float32 (__main__.TestCommonCUDA)
module: cuda	Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118)
module: cuda	DISABLED test_noncontiguous_samples_matmul_cuda_float32 (__main__.TestCommonCUDA)
module: cuda	Pytorch 2.1.0.dev20230512 cuda not available
module: cuda	Speed when installing from source is very low with CUDA 11
module: cuda	Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi
module: cuda	RuntimeError: nonzero is not supported for tensors with more than INT_MAX  elements, file a support request
module: cuda	[BUG] Poor torch.bmm performance on H100
module: cuda	Accuracy issues with Jitterated complex kernels for acos, acosh, asin, asinh, tan and tanh
module: cuda	[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs
module: cuda	Higher GPU consumption for Lenet-5 and LSTM models when compared to other frameworks
module: cuda	This flag not work : torch.backends.cudnn.allow_tf32 = False
module: cuda	torch.cuda.is_available() crashes python in systems with disabled gpu
module: cuda	Can the CUDA device LUID be exposed as part of _CudaDeviceProperties?
module: cuda	cuda.is_available() error
module: cuda	cuda 12.0 support request for building pytorch from source code
module: cuda	pca_lowrank and svd_lowrank broken under automatic mixed precision.
module: cuda	CUPTI Initialization error
module: cuda	torch.nn.functional.multilabel_margin_loss cuda lacks checking of "out of bound"
module: cuda	PyTorch 2.0.0 encountered CUDA error: an illegal memory access was encountered
module: cuda	torch.cuda.is_available() return False
module: cuda	Request for deterministic support for reflection_pad2d_backward_cuda
module: cuda	Problem with instalation torch2 on a100+cu12.1
module: cuda	Cannot use AT_CUDA_DRIVER_CHECK from user code
module: cuda	Bring CudaPluggableAllocator to feature parity with the Native Allocator
module: cuda	RuntimeError: CUDA error: an illegal memory access was encountered, torch/cuda/streams.py", line 94, in synchronize
module: cuda	[interoperability] zero-size cuda arrays do not look supported
module: cuda	Relax version dependencies on CUDA pip wheels?
module: cuda	CUDA 10.2 cudnn 8.2.4 run Conv2d error
module: cuda	Torch 2.0 import hangs forever
module: cuda	[H100] test_ops.py::TestFakeTensorCUDA.test_fake_crossref_backward_amp_nn_functional_scaled_dot_product_attention_cuda_float32 failed
module: cuda	No GPU found, using CPU during preprocessing Error processing dataset with NsfHifiGAN
module: cuda	Why doesn't PyTorch install the REAL nvidia cuDNN pip package?
module: cuda	training hangs at line torch.cuda.synchronize()
module: cuda	Inconsistent behaviour of torch.all()
module: cuda	Can only import torch after Tensorflow accessed its gpu device
module: cuda	torch needs to SHOW that it support sm_89 even if functionally the same as sm_86
module: cuda	test_foreach failing cuda memory leak check
module: cuda	COO @ COO tries to allocate way too much memory on CUDA
module: cuda	cuda 12 support request.
module: cuda	Implement a torch.cuda.visible_device_indexes function.
module: cuda	torch.cuda.device_count cached return value does not reflect environment changes.
module: cuda	Upsampling ResBlock GPU memory spike
module: cuda	[libtorh]Consistency problem of gpu computing
module: cuda	CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz
module: cuda	aten::cudnn_convolution chooses different conv implementation given the same inputs.
module: cuda	CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv
module: cuda	USE_CUDNN=1 doesn't force cmake to fail if cudnn is not found
module: cuda	Failed to Open libnvrtc-builtins.so.11.7
module: cuda	DISABLED test_inplace_grad_index_put_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: cuda	Dynamo graph break due to context manager do not resume inside/outside the context manager
module: cuda	torch.cuda.is_available() returns True even if the CUDA hardware can't run pytorch
module: cuda	CUDA error CUBLAS_STATUS_NOT_INITIALIZED
module: cuda	CUDA error: initialization error
module: cuda	Stochastic Illegal Memory Access error mid-epoch on AWS p4d instances
module: cuda	Softmax function slows down for data with large range
module: cuda	The speed of matrix inversion is relatively slow for many small matrices
module: cuda	[bazel] error: use of undeclared identifier 'cudaGraphDebugDotPrint'
module: cuda	Internal Assert failed
module: cuda	[RFC] quantile should work for float16/half on the GPU
module: cuda	Pytorch 1.13 conda package with cuda requires too many unneccessary packages
module: cuda	Unexpected behavior when running torch.max in cuda
module: cuda	overflow (?) on cuda tensor after matrix multiplication
module: cuda	nn.CrossEntropyLoss error out when the sample size is large
module: cuda	Adam (fused=True) issues
module: cuda	[A ERROR in Docker] RuntimeError: CUDA error: no kernel image is available for execution on the device
module: cuda	Why torch.mode return different value between CPU and GPU
module: cuda	addcmul on CUDA does not have the correct FMA behavior
module: cuda	torch.addbmm throws different exception differences on CPU and GPU.
module: cuda	Feature Request: deterministic CUDA cumsum
module: cuda	torch.nn.functional.embedding_bag throws an exception when it runs on a CPU, but it runs successfully on a GPU.
module: cuda	AdaptiveAvgPool1d throws different exceptions when using the gpu
module: cuda	torch.mm: Exceptions thrown on the CPU and GPU are inconsistent
module: cuda	sm_80 support
module: cuda	torch.nn.CTCLoss Trigger out-of-bound Read under Compute Sanitizer
module: cuda	Got many TestDTensorOpsCUDA.test_dtensor_op_db_X test failures
module: cuda	Can't import torch --> OSError related to libcublasLt.so.11
module: cuda	ImportError: libcupti.so.11.2: cannot open shared object file: No such file or directory
module: cuda	MultiMarginLoss doesn't check the value of target on CUDA
module: cuda	ConvTranspose fails on CPU but returns an empty tensor on CUDA
module: cuda	CUDA unknown error after suspend during debugging
module: cuda	nn.functional.embedding_bag Trigger out-of-bound Read under Compute Sanitizer
module: cuda	benchmark cache persist
module: cuda	cuDNN error (CUDNN_STATUS_NOT_SUPPORTED) for torch.nn.functional.grid_sample()
module: cuda	torch.nn.RReLU not reporting lower > upper on CUDA
module: cuda	Moving tensor to GPU by .cuda() gets stucked when AMD Secure Encripted Virtualization (SEV) is activated
module: cuda	Placing LSTM model on bfloat16 on GPU causes error
module: cuda	out of memory with pytorch version after 1.8.1
module: cuda	getting error error: namespace "cub" has no member "Debug" when try to build v1.8.2 with CUDA 11.6
module: cuda	torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed)
module: cuda	max_unpool3d will trigger an assertion fail under compute sanitizer
module: cuda	Enable PostLocalSGDOptimizer on CUDA tensors
module: cuda	RAM leak when copying tensor from cpu to cuda
module: cuda	pytorch could not build from source with cudnn 8.0.5
module: cuda	Installing PyTorch with BUILD_SPLIT_CUDA=ON and CUDNN fails on linker error
module: cuda	RuntimeError: Tensors of type TensorImpl do not have numel
module: cuda	Reproducible "CUDA error: an illegal memory access was encountered"
module: cuda	DISABLED test_variant_consistency_jit_linalg_lu_cuda_complex64 (__main__.TestJitCUDA)
module: cuda	TF32 conv_transpose2d with groups has bad precision compared to fp32
module: autograd	backward implicit conversion from tuple to torch.Tensor results in an indexing error message
module: autograd	Legitimize/harden differentiable optimizers in torch.optim
module: autograd	RuntimeError: CUDA error: an illegal memory access was encountered using vmap and model ensembling call for cuda system
module: autograd	Warning in autograd init regarding to aten::reshape
module: autograd	arctan2 fp16 error when optimising
module: autograd	DISABLED test_grad_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA)
module: autograd	DISABLED test_fn_gradgrad_linalg_lu_factor_ex_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: autograd	DISABLED test_backward_nn_functional_conv3d_cuda_float32 (__main__.TestCompositeComplianceCUDA)
module: autograd	Error when calculating the Jacobian of torch.conj using forward-mode differentiation
module: autograd	interpolate::trilinear returns wrong gradients on CUDA
module: autograd	[aot_autograd] Handle requires_grad mutation in AOTAutograd
module: autograd	PT2 improperly executes ambient saved_tensors_hooks
module: autograd	Nesting no_grad in autocast causes backwards graph to be (partially) lost outside of no_grad
module: autograd	torch.inference_mode and tensor subclass: RuntimeError: Cannot set version_counter for inference tensor
module: autograd	Higher-order derivatives extremely slow, increasing exponentially
module: autograd	torch.compile x autograd.Function: Make the backward strict mode less srict
module: autograd	backward and grad behave inconsistently w.r.t. set_ on leaf variable
module: autograd	Simple script segfaulting when grad is enabled
module: autograd	NAN appears in the backward results of masked.cumprod on macos
module: autograd	RuntimeError: _Map_base::at when exporting squeeze
module: autograd	stride of gradient is not same as the corresponding tensor
module: autograd	Pytorch: torch.autograd.grad returns NoneType
module: autograd	Improving save_on_cpu's performance by overlapping memory transfers with compute
module: autograd	RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'grad_y'
module: autograd	Avoid incrementing refcount of grad_fn in unpack_list
module: autograd	Strange backward behavior with sparse tensors
module: autograd	Exporting the operator 'aten::grad' to ONNX opset version 18 is not supported.
module: autograd	Significant time difference of calculating Jacobian matrix using jacrev and oracle functions
module: autograd	autocast + torch.no_grad inference cause backward graph nodes to be lost
module: autograd	torch.func.jvp fails with BERT training
module: autograd	Improved error checking for custom Function when saving intermediates
module: autograd	gradcheck produces false positives with sparse inputs when masked=False.
module: autograd	test_functional_autograd_benchmark.py::TestFunctionalAutogradBenchmark::test_fast_tasks passes with all NaNs
module: autograd	_view_func but without keeping original view tensor alive
module: autograd	Tensors that share same underlying storage to also share gradient storage
module: autograd	DISABLED test_inplace_grad_index_put_cuda_complex128 (__main__.TestBwdGradientsCUDA)
module: autograd	DISABLED test_inplace_grad_div_trunc_rounding_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: autograd	DISABLED test_fn_grad_div_trunc_rounding_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: autograd	torch.autograd.detect_anomaly should report the original forward trace as part of the error, rather than as out of band warning
module: autograd	DISABLED test_inplace_gradgrad_remainder_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: autograd	Will Deep Implicit Models ever become first class citizens in PyTorch?
module: autograd	undocumented error on torch.autograd.Function.jvp for non-Tensor forward returns
module: autograd	Backward hook execution order changes when input.requires_grad is False
module: autograd	cat gradgrad tests failing
module: autograd	torch.func.jacrev fails if model contains full_backward_hook
module: autograd	torch.sparse.sum backward fails when reducing over dense dimensions.
module: autograd	Higher order derivatives not working when setting compute device to torch.device("mps")
module: autograd	matmul with CSR matrix in inference mode throws an exception
module: autograd	Memory leak when saving an input tensor returned as-is if mark_dirty and running with dual tensors
module: autograd	Support sparse COO/CSR/CSC/BSR/BSC return values in gradcheck input function
module: autograd	Multi-output derivative formulas can save unnecessary tensors
module: autograd	.set_ operation on a view (detach()) of the view tensor changes grad_fn of the original view tensor from ViewBackward0 to AsStridedBackward0
module: autograd	Incorrect gradient calculation for upsample nearest on CUDA
module: autograd	Doing inplace on a inplace view of tensor that retains_grad triggers internal assert
module: autograd	sparse.mm triggers INTERNAL ASSERT FAILED when backwarding
module: autograd	Follow-ups to do after adding nested checkpoint
module: autograd	Improve checkpoint thread-safety
module: autograd	Wrong return type from operation on custom tensor inside registered hook
module: autograd	views created in __torch_dispatch__ share storage but not version_counter
module: autograd	Reuse autograd.grad graph for rapid, repeated gradient calculation
module: autograd	gradgradcheck does not work with sparse inputs.
module: autograd	torch.where behaves differently from in place replacement
module: autograd	autograd.functional.jacobian : tensor instead of function as input for reverse mode?
module: autograd	copy.deepcopy does not copy gradients of nn.Parameter
module: autograd	It seems that torch.Tensor.addmv and torch.Tensor.addr will check some inputs' dtype if and only if in backward()
module: autograd	Differentiate with regard a subset of the input
module: autograd	interactions between views + autograd.Function + AOTAutograd causes memory leak
module: autograd	jacrev raise "Cannot access storage of TensorWrapper" error when computing the grad of storage
module: autograd	jacfwd and jacrev are fundamentally broken for complex inputs
module: autograd	scatter fails the gradient computation in reverse mode for src when index is empty
module: autograd	svd triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode
module: autograd	MSELoss fails to compute the gradients when inputs have different dtype
module: autograd	DISABLED test_forward_mode_AD_linalg_det_singular_cuda_complex128 (__main__.TestFwdGradientsCUDA)
module: autograd	DISABLED test_fn_grad_linalg_det_singular_cuda_complex128 (__main__.TestBwdGradientsCUDA)
module: autograd	backward(inputs= does not need to execute grad_fn of the inputs
module: autograd	Simplify module backward hooks to use multi-grad hooks instead
module: autograd	autograd.functional.jacobian : Imaginary part is lost for functions with real input and complex output.
module: autograd	Lazily start worker threads in the autograd engine
module: autograd	[discussion] Analyzing a list of tensors stored as intermediate values / saved_for_backward in autograd graph
module: autograd	torch.compile frees computation graph in a GAN training setup and tries to call backward a second time
module: autograd	torch.compile with aotautograd does not support double backwards
module: autograd	torch.compile incorrect when imperative autograd APIs are used
module: autograd	Internal assert when ctx.saved_tensors fails when saving results of an intermediate view tensor with torch.utils.checkpoint and use_reentrant=False
module: autograd	forward-mode AD formula for torch.add (and possibly others) accidentally upcasts float32 to float64
module: autograd	third-order gradient of torch.pow with tensor args and certain input returns NaN
module: autograd	Finish deprecation of autograd decorator over class objects
module: autograd	forward AD for _euclidean_dist
module: autograd	DISABLED test_fn_gradgrad_linalg_lu_factor_cuda_complex128 (__main__.TestBwdGradientsCUDA)
module: autograd	Autograd doesn't stop executing backward graph early enough in situations involving set_
module: autograd	Autograd precision for CONV + BN  between pytorch version 1.11.0 and 1.12.1
module: autograd	diagonal of Jacobian matrix
module: autograd	Semantics of sparse operations clarification - Sparsity of the gradient with respect to a sparse tensor input
module: autograd	gradcheck failure with sparse matrix multiplication
module: autograd	Autograd doc does not mention torch.autograd.set_grad_enabled
module: autograd	Dedicated function for shallow_copy_and_detach
module: autograd	AUTOGRAD is not working on IOS
module: autograd	max_pool2d_with_indices(self, ...) shouldn't need to save self for backward
module: autograd	[Nested Tensor] view + inplace for autograd.
module: autograd	Add a new argument check_inf=True (by default) or check_pos_inf / check_neg_inf to anomaly mode
module: autograd	Cdist backward dependent on compute_mode
module: autograd	Incorrect CPU implementation of CTCLoss backward step
module: autograd	OOM during backward() leads to memory leaks
module: autograd	backward not available for index and mask
module: autograd	set_grad_enabled not respected when running on a web server
module: autograd	Tensor.backward type hints clarification
module: autograd	forward program terminated from __cxa_pure_virtual
module: autograd	torch.utils.checkpoint optimization opportunity
module: autograd	Add a check to detect mutation of the inputs during backward
module: autograd	Inconsistent computation of gradient in MaxUnPooling
module: autograd	torch.renorm gives wrong gradient for 0-valued input when p is even and maxnorm=0.
module: autograd	hardshrink gives wrong gradient for 0 input when lambd is 0.
module: autograd	atan2 will gradcheck fail when other is a tensor with int8 dtype
module: autograd	det will return wrong gradient for 1x1 matrix with 0 value.
module: autograd	Tracker: Slow gradcheck failures possibly indicating incorrect gradients
module: autograd	DISABLED test_lobpcg (__main__.TestAutograd)
module: autograd	soft_margin_loss gives wrong gradient when target with dtype uint8
module: autograd	max_unpool gives wrong gradient when indices has duplicate
module: autograd	pow CUDA tensor raised to CPU scalar tensor result can't backward properly
module: autograd	torch.special.gammainc backward pass with respect to the first argument
module: autograd	test_meta_vstack_cuda_int16 (__main__.TestMetaCUDA) Fails with DEBUG=1
module: autograd	gradgradcheck fails for torch.native_layer_norm
module: autograd	linalg.pinv_singular tests are slow
module: autograd	RuntimeError: Event device type CUDA does not match blocking stream’s device type CPU
module: autograd	layer_norm triggers INTERNAL ASSERT with input requiring grad + zero-size int tensor
module: autograd	index_fill will trigger INTERNAL ASSERT when float tensor requiring grad + int tensor
module: autograd	gradcheck fails for torch.distribution.transform APIs in forward mode
module: autograd	TRACK: integral + floating inputs to an op with floating requiring grad result in INTERNAL_ASSERT
module: autograd	ctc_loss will backward crash
module: autograd	baddmm triggers INTERNAL ASSERT FAILED when input requires grad
module: autograd	matmul, mm triggers INTERNAL ASSERT FAILED when input requires grad
module: autograd	logaddexp2 fails to backward
module: autograd	addmv, mv will trigger INTERNAL ASSERT FAILED when input requiring grad
module: autograd	Functional Jacobian does not work with Torchdiffeq
module: autograd	Inplace Bool API + sum will trigger INTERNAL ASSERT FAILED
module: autograd	fast gradcheck fails when outputs that do not require grad precede outputs that do
module: autograd	gradcheck for torch.solve may trigger INTERNAL ASSERT FAILED
module: autograd	cumprod, prod will backward fail if dtype argument is different than the dtype of input tensor
module: autograd	addr, baddmm, dist, l1_loss will backward fail when input tensors have different dtypes
module: autograd	gradcheck fails for torch.trace
module: autograd	gradcheck should support the comparison of NaN
module: autograd	torch.addmv backward fails
module: autograd	torch.linalg.cond has different results for tensor requiring autograd
module: autograd	Segfault in ~PyFunctionPreHook
module: autograd	torch.clamp does not distribute gradients as element-wisemin/max do
module: autograd	Add post-AccumulateGrad hook as a nice public API
module: autograd	Ensure custom Function are correct in double backward setting
module: autograd	Add ZeroTensor support for mm
module: autograd	Rollup: Top forward-over-reverse formulas
module: autograd	Feature request: fast way to approximate the diagonal of the hessian
module: autograd	check_batched_forward_grad fails for torch.norm and related ops
module: autograd	Training, Forward / backward pass with _different_ batch-size, no speedup observed when backward pass has smaller batch-size
module: autograd	[CTA] Let's Stamp Out Flaky Tests!
module: autograd	Gradients tests are very time consuming
module: autograd	Autograd API to get saved for backwards tensors for an autograd graph
module: autograd	Forward_AD and Torchscript Functions results in Nones or wrong values.
module: autograd	Improving error message RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
module: autograd	Support views in custom autograd functions
module: autograd	Large performance difference of loss.backward() between torch-1.9.0 and torch-1.8.0
module: autograd	create_graph=True results in grad_fn error for differentiable functions
module: autograd	Vectorized Jacobian and Hessian errors with ffts
module: autograd	Toggling deterministic mode for individual autograd backward functions
module: autograd	Mechanism for Tensor subclasses to "disable autograd"
module: autograd	svd_backward: does not handle inputs of rank r < min(m, n).
module: autograd	index_add : Inconsistent between CPU and CUDA
module: autograd	RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward
module: autograd	[libtorch]can not save a  vector<int> to AutogradContex->saved_data.
module: autograd	PyTorch bug: Cannot pass gradient through index_add
module: autograd	Rollup: forward-mode AD operator coverage
module: autograd	Add flag for functional.Jacobian to return output as well
module: autograd	conv3d padding=same gradgradcheck fails on CUDA
module: autograd	backward checks len of inputs before it's converted to a tuple
module: autograd	Empty or NaN data pollute gradient even if they are not involved during backward
module: autograd	API to support combined activation offloading or checkpointing
module: autograd	Derivative for _ctc_loss_backward
module: autograd	Add ability to ignore arguments/outputs in torch.autograd.functional.jacobian
module: autograd	Possibly out of date error in autograd codegen
module: autograd	Performance improvement in Autograd Forward AD using ZeroTensors
module: autograd	[Question] How to extract/expose the complete PyTorch computation graph (forward and backward)?
module: autograd	[feature request] A rank-revealing SVD for better stability in backward.
module: autograd	(svd|pca)_lowrank: backward is unstable when for a matrix A, the parameter q is set to a value q > rank(A).
module: autograd	torch.ldexp generated tests fail on call to torch.mul
module: autograd	Stop gradient option for padding
module: autograd	Transpose of a sparse tensor is not a view operation
module: autograd	istft gradcheck fails on ROCm
module: autograd	Casting real parameter to complex during forward produces warning on backward
module: autograd	Make torch.Tensor.view support autograd for appropriate cases
module: autograd	functorch transforms are silently incorrect with autograd.Function
module: autograd	register_ Hook causes the CUDA out of memory, and remove() is useless
module: autograd	Add more explanation on multithreaded graph building of Autograd
module: autograd	Functions are rendered incorrectly
module: autograd	TSAN issue in autograd "set_next_edges"
module: autograd	Feature request: set_grad_enabled(*mods_or_params) as a safe context manager
module: autograd	XLA not being tested in TestAutogradDeviceType
module: autograd	test_forward_mode_AD hangs for nn.functional.cosine_embedding_loss
module: autograd	Jacobian mismatch for nn.functional.ctc_loss
module: autograd	Missing doc for torch.autograd functions
module: autograd	Feature: Add derivative for channel_shuffle
module: autograd	Expose isDifferentiableType to python
module: autograd	Make the evaluated value of function f(x) accessible from torch.autograd.functional.jacobian(f,x)
module: autograd	OpInfos disabled for batched forward grad computation
module: autograd	Move the queue_callback() API out of Variable._execution_engine and into a public API
module: autograd	Finishing OpInfos: test_autograd.py
module: autograd	Support differentiability through clone and update
module: autograd	torch.nn.functional.l1_loss fails gradgradcheck for complex inputs
module: autograd	Derivative not implemented for narrow_copy
module: autograd	Pass backward flags such as retain_graph to context of custom torch.autograd.Function
module: autograd	Exception thrown in final autograd callback (queue_callback) not caught if not on CPU thread: terminate called after throwing an instance of 'python_error'
module: autograd	Registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: autograd	native functions should not be allowed to take in a grad argument
module: autograd	Create a boxed/templated ADInplaceOrView kernel
module: autograd	GradGrad of max_pool2d fails with empty batch dimension
module: autograd	LeakyReLU and Elu use more VRAM than needed
module: autograd	Raw saved tensors can survive the deletion of the underlying SavedVariable object
module: autograd	Expose a SavedTensorsHooks nn.Module for users to register saved tensors hooks
module: autograd	Preserve tensor subclasses when unpacking a SavedTensor
module: autograd	pytorch 1.8.2  cuda test errors
module: autograd	split up test/test_autograd.py
module: autograd	tools/autograd/derivatives.yaml doesn't support methods on optional tensor
module: autograd	slice_embed, select_embed like diag_embed
module: autograd	Checkpointing without re-entrant autograd
module: autograd	torch.prod internal asserts when passed a tensor that requires_grad (and a dtype)
module: autograd	Sparse updates to logits in distributions.Categorical
module: autograd	Create a boxed fallback / template recipe for Autograd that forwards, but errors on backwards
module: autograd	channel_shuffle output is sometimes aliased with its input
module: autograd	When 'trapezoid' is called with an empty tensor input, it does not produce an output with requires_grad
module: nn	Add BC test for load_state_dict on OptimizerInfos and ModuleInfos
module: nn	incorrect output shape for nn.AvgPool2d with ceil_mode=True
module: nn	Have an option to disable fast path in TransformerEncoderLayer.
module: nn	groups parameter for nn.Linear
module: nn	Dynamo can't parse torch.rrelu or torch.nn.functional.rrelu
module: nn	nn.Module.to(dtype) does not work for XLATensor
module: nn	Optimize cudnn_convolution_out to Reduce Unnecessary Memory Allocation and Copy
module: nn	torch.nn.KLDivLoss fails test_grad and test_gradgrad
module: nn	torch.nn.CTCLoss doesn't seem to properly support noncontiguous inputs for 1d target with target_dtype=torch.long
module: nn	Hidden rule in nn.FractionalMaxPool2d
module: nn	Potential issue with custom transformer masks when using fast path and batch_first=True
module: nn	HigherOrderOp graph capturing is wrong for buffer mutation
module: nn	DISABLED test_noncontiguous_samples_nn_functional_conv3d_cuda_complex64 (__main__.TestCommonCUDA)
module: nn	torch.nn.PairwiseDistance，The results vary widely from version to version
module: nn	InstanceNorm1d throws bad warning
module: nn	nn.LSTM tolerates wrong input shape when hidden state isn't provided.
module: nn	DISABLED test_grad_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA)
module: nn	Add head_mask for transformers
module: nn	Adds syntactic sugar for select which backend to choose for SDPA
module: nn	DISABLED test_backward_nn_functional_conv3d_cuda_float32 (__main__.TestCompositeComplianceCUDA)
module: nn	DISABLED test_conj_view_nn_functional_conv3d_cuda_complex64 (__main__.TestMathBitsCUDA)
module: nn	RNN argument order
module: nn	Transformer with convolutional position wise feed forward network
module: nn	Tracing per-param sharding FSDP: RemovableHandle -> RemovableHandleVariable
module: nn	torch.nn.functional.max_pool2d outputs inf
module: nn	Inconsistency of state_dict loading across devices
module: nn	Add the possibility to pass a Generator to gumbel_softmax
module: nn	Found nn.LazyBatchNorm1d(0) has inconsistency bug between GPU and CPU testing
module: nn	log_softmax could be 2**124 to 2**1021 times more accurate on small outputs
module: nn	Creating Gaussian Mixture Models with MultivariateNormal
module: nn	Should _native_batch_norm_legit_functional be in native_functions.yaml?
module: nn	Multi Scale Deformable Attention Support
module: nn	Strided tensor in backward cause uninitialized output
module: nn	GroupNorm & InstanceNorm does not handle channels_last correctly
module: nn	Propose to add constant padding mode to the torch.nn.functional.grid_sample function
module: nn	[fix] accounting for dilation in pool padding assertion
module: nn	Implement device parameter in Dropout2d
module: nn	model.named_buffers() fails if module not hashable.
module: nn	Depthwise conv3d slower than normal conv3d
module: nn	BCEWithLogitsLoss: Check if labels / targets are within zero and one
module: nn	max_pool3d_with_indices_backward_cuda and avg_pool3d_backward_cuda does not have a deterministic implementation
module: nn	[RFC] Scaled Dot Product Attention  API Changes
module: nn	Dropout signature inconsistent between torch.dropout, torch.nn.Dropout and torch.nn.functional.dropout
module: nn	Make standard container classes satisfy container Protocols.
module: nn	Extends the functionality of  nn.BatchNorm1d.
module: nn	InstanceNorm does not catch dim mismatch
module: nn	Max pool with negative integer inputs and channels_last memory layout gives the wrong values
module: nn	Make Dropout take a dim=... argument
module: nn	Inconsistent Behavior of ConvTranspose2d on CPU and CUDA
module: nn	RNN Documentation is Confusing / Wrong
module: nn	F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
module: nn	Adding Maximal Update Parametrization (µP) to torch.nn.init
module: nn	RuntimeError when calling conv_transpose2d with groups
module: nn	[docs] F.interpolate(uint8_input, mode = 'bicubic', ...) overshoot behavior: adjust the note in docs to explain that for uint8 saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint8 inputs
module: nn	AdaptiveMaxPool documentation is not detailed
module: nn	[BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment
module: nn	Provide a reset_parameters() method for MultiheadAttention to support FSDP meta device initializtion
module: nn	[feature request] [ux proposal] Min-max linear normalization to be supported in F.normalize (or in a new function)
module: nn	torch.nn.functional.cross_entropy different loss when providing one_hot_target and class weights
module: nn	RuntimeError: expected scalar type BFloat16 but found Float with torch.nn.TransformerEncoder
module: nn	make backward function explicit in a layer which is a combination of some ops
module: nn	DISABLED test_memory_format_nn_ConvTranspose1d_cuda_complex32 (__main__.TestModuleCUDA)
module: nn	The difference between input grad computed by channels last backward and the input grad computed by channels first backward of Hardswish on MPS is too large
module: nn	The difference between channels last backward and channels first backward of AvgPool2d on CUDA is too large
module: nn	[Minor Bug] Should consume_prefix_in_state_dict_if_present change ordering of keys?
module: nn	Inconsistency between CPU and GPU for Linear() layer with input size 0
module: nn	softmax to handle dimensions comprised of -inf
module: nn	Increasing batch size makes network forward 1000 times slower
module: nn	nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access
module: nn	torch.nn.utils.clip_grad_norm_() causes H2D sync with foreach ops.
module: nn	Calling ops.aten.embedding_bag() function got silent crash
module: nn	Improve Error Message in MultiMarginLoss for Inconsistent Target Size
module: nn	Misleading error message in multilabel_margin_loss when passing incompatible tensor dimensions
module: nn	Flip default on add_zero_attn in torch.nn.MultiheadAttention to True
module: nn	DISABLED test_cross_entropy_large_tensor_reduction_sum_cuda (__main__.TestNNDeviceTypeCUDA)
module: nn	F.pad will accept 0 and negative values as parameter
module: nn	Will nn.unfold support non-4D-tensor input in future version?
module: nn	DISABLED test_cross_entropy_large_tensor_reduction_none_cuda (__main__.TestNNDeviceTypeCUDA)
module: nn	Quadric Layer
module: nn	[discussion] Integrate widely used utilities from fvcore into the core repo
module: nn	torch.nn.Conv2d/Conv1d's padding mode circular cannot accept 3-dim input
module: nn	Torch's LayerNorm and Adam optimizer vs those in tensorflow
module: nn	Unrelated error messages with torch.nn.AdaptiveAvgPool3d
module: nn	Use isinstance instead of type when checking for torch.nn.Parameter
module: nn	torch.nn.CrossEntropyLoss: class weighting changes label_smoothing
module: nn	Support for eval in functional_call
module: nn	System memory leak when using different input size of torch.nn.Conv3d
module: nn	Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size
module: nn	LSTM built-in dropout not reproducible on GPU
module: nn	F.adaptive_avg_pool3d(input, 1) returns infinity in half precision
module: nn	F.conv1d and F.conv2d propagate nan's incorrectly when minibatch > 15
module: nn	Exported model with dropout incorrectly applies dropout during eval
module: nn	LSTM/RNN operation agnostic
module: nn	Support ByteTensor and ShortTensor for nn.Embedding and nn.EmbeddingBag
module: nn	The document does not emphasize Illegal value in nn.Bilinear
module: nn	binary_cross_entropy (loss) seems to be giving incorrect values for very negative logits
module: nn	Preserve weight_g/weight_v accessors on new weight_norm
module: nn	raise RuntimeError faster when loading an object with a torch CUDA tensor on a CPU-only machine
module: nn	nn.ChannelShuffle1d
module: nn	BCELoss and BCEWithLogitsLoss differ when one of the input logits is float("inf")
module: nn	Best practices clarification for initialization strategies
module: nn	DISABLED test_Conv2d_dilated_cuda_tf32 (__main__.TestNN)
module: nn	Add additional "sigmoid" approximation to GeLu activation?
module: nn	Hooks not working in version 2.0.1+cu118
module: nn	scipy.ndimage.find_objects
module: nn	Parameter gradient is not moved parameter is moved across devices
module: nn	DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU)
module: nn	Nightly torch.compile fails with dynamically patched nn.module.forward
module: nn	Backward hook execution order changes when input.requires_grad is False
module: nn	Question about GRU(RNN/LSTM) outputs shape
module: nn	torch.nn.functional.multilabel_margin_loss cuda lacks checking of "out of bound"
module: nn	Spectral Normalization can not be applied to Conv{1,2,3}d
module: nn	DISABLED test_gradgrad_nn_GroupNorm_cuda_float64 (__main__.TestModuleCUDA)
module: nn	DISABLED test_grad_nn_GroupNorm_cuda_float64 (__main__.TestModuleCUDA)
module: nn	Add a deterministic version of reflection_pad2d_backward_cuda
module: nn	torch.nn.init functions with generator argument
module: nn	A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback
module: nn	GroupNorm cpu/gpu parity tests fail with pretty large differences
module: nn	Is there a recommended implementation of yuv2RGB for the current torch?
module: nn	Investigate Lazy{*}Norm{*}d modules no batch dim support
module: nn	Incorrect gradient calculation for upsample nearest on CUDA
module: nn	nn.Conv function to compute conv formula
module: nn	Many padding Module fail memory_format tests
module: nn	FractionalMaxPool3d INTERNAL ASSERT FAILED when computing jacrev
module: nn	nn.interpolate scale_factor floors output size with floating
module: nn	[torchdistx] Future of the large model initialization
module: nn	copy.deepcopy does not copy gradients of nn.Parameter
module: nn	Regression bug in torch.nn.ReLU6 and torch.nn.Hardtanh that inplace=True doesn't work in PyTorch 1.10.0~1.13.1
module: nn	Changing behavior of module.to() to better support mixed real- and complex-valued parameters
module: nn	Continuous dropout layer
module: nn	DISABLED test_memory_format_nn_ConvTranspose2d_cuda_complex32 (__main__.TestModuleCUDA)
module: nn	torch.nn.LazyLinear crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0
module: nn	grid_sample with relative grid
module: nn	Asking for a LAZYMODULEMIXIN warning
module: nn	MSELoss fails to compute the gradients when inputs have different dtype
module: nn	[RFC] Make more operations inplace (GELU, BatchNorm, LayerNorm)
module: nn	Simplify module backward hooks to use multi-grad hooks instead
module: nn	Adding label smoothing option to nn.BCELoss  and nn.BCEWithLogitsLoss?
module: nn	Implement L1 and L2 gradient as hooks with the option of changing the weight decay value.
module: nn	nn.CrossEntropyLoss error out when the sample size is large
module: nn	Sample Weighted BatchNorm1d
module: nn	Caching a model's weights and state_dict to disk to save RAM
module: nn	torch.nn.TransformerEncoderLayer missing exception description information.
module: nn	torch.nn.functional.embedding_bag throws an exception when it runs on a CPU, but it runs successfully on a GPU.
module: nn	Documentation: torch.nn.functional.embedding docs could more clearly state the requirement that weight be a 2D tensor
module: nn	AdaptiveAvgPool1d failed in the lower version
module: nn	AdaptiveAvgPool1d throws different exceptions when using the gpu
module: nn	torch.nn.ReplicationPad2D Report "invalid configuration argument" Error under Compute Sanitizer
module: nn	torch.nn.LayerNorm Abort with "invalid device ordinal" Error
module: nn	torch.nn.CTCLoss Trigger out-of-bound Read under Compute Sanitizer
module: nn	Implement generic batch normalization layer.
module: nn	MaxPool1D output shapes can be negative when ceil_mode=True
module: nn	Group losses in a common namespace
module: nn	torch.nn.CTCLoss Trigger heap-buffer-overflow under AddressSanitizer
module: nn	Nandense layer for missing values
module: nn	Easy way to "freeze" BatchNorm running_mean/running_var
module: nn	conv_transpose is not similar to nn.grad.conv_input when output_padding is passed with non-default values.
module: nn	torch.nn.functional.one_hot only works for int64
module: nn	Discrepancy in output shape for batch_norm inference mode between CUDA and CPU
module: nn	nn.CrossEntropyLoss overflow with FP16 and minibatch
module: nn	Iterative Global Pruning Cause GPU Memory Leak
module: nn	Feature Request: Deterministic Algorithm for MaxPool3d
module: nn	Add persistent option to nn.Module.buffers.
module: nn	Allow passing dict (as opposed to OrderedDict) to nn.Sequential
module: nn	nn.Softmax should not allow default/implicit/unset dim constructor argument
module: nn	torch.var_mean is slower than layer norm
module: nn	Conv1d: NNPACK SpatialConvolution_updateOutput failed when batchsize or padding is too large
module: nn	Frozen module for transfer learning.
module: nn	torch.nn.Upsample's error message is inconsistent with the documentation
module: nn	torch.nn.TripletMarginLoss margin can be less than 0
module: nn	The type of parameter 'p' in torch.nn.TripletMarginLoss wrong
module: nn	torch.nn.ReplicationPad{1|2}d supports more input dimension than are written on documentation
module: nn	torch.nn.PixelShuffle error message wrong
module: nn	torch.nn.MaxUnpool2d get negative size tensor
module: nn	torch.nn.InstanceNorm{1|2|3}d doesn't verify the value type of parameter num_features
module: nn	torch.nn.GRU runs long time, when num_layers is large
module: nn	torch.nn.functional.softplus / torch.nn.Softplus parameter beta can be set to zero
module: nn	deepcopy of LazyLinear fails
module: nn	torch.nn.functional.log_softmax  parameter '_stacklevel' undocumented
module: nn	torch.nn.Hardtanh allows min_val > max_val
module: nn	When padding is big int, torch.nn.functional.fold runs too long and can't return result
module: nn	torch.nn.functional.avg_pool{1|2|3}d error message does not match what is described in the documentation
module: nn	Support for CSR Tensor with NN layers
module: nn	RReLU doc doesn't specify the eval mode behaving just like LeakyReLU
module: nn	SyncBatchNorm does not work on CPU
module: nn	Minor inconsistency in description of attn_output_weights in MultiheadAttention docs
module: nn	Could be clearer that Cross Entropy takes logits as input
module: nn	upsample_bilinear2d() received an invalid combination of arguments
module: nn	grid_sample and mode='bilinear' induces errors at discrete pixel locations
module: nn	DISABLED test_non_contiguous_tensors_nn_ConvTranspose1d_cuda_complex32 (__main__.TestModuleCUDA)
module: nn	torch._weight_norm with specified dim returns wrong output
module: nn	ExpandedWeights sometimes fail silently and doesn't compute .grad_sample attribute
module: nn	ExpandedWeights can't handle modules with tied weights
module: nn	torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12
module: nn	Position embedding aware global circular convolution
module: nn	Interpolation artifacts when using nn.interpolate, trilinear mode for 3D label images
module: nn	[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer
module: nn	Negative values still produced by torch.nn.functional.kl_div
module: nn	Revisit OpInfo samples for nn.functional.max_poolNd
module: nn	Support for learnable p Values in LPPOOL like Pool
module: nn	AttributeError: 'LinearPackedParams' object has no attribute '_modules'
module: nn	Need "valid" and "same" padding mode for convTranspose2d
module: nn	[complex] dropout and it's variants should support complex tensors
module: nn	test_conv_backend tests OOMing in 10.2 slow_gradcheck CI
module: nn	Support tensor subclasses as UninitializedParameters
module: nn	F.binary_cross_entropy_with_logits unexpected behaviour
module: nn	soft_margin_loss gives wrong gradient when target with dtype uint8
module: nn	max_unpool gives wrong gradient when indices has duplicate
module: nn	GEGLU activation
module: nn	Parameter.__deepcopy__ doesn't preserve view relationships
module: nn	[feature request] Support dataclass derivations of nn.Module
module: nn	Improving clarity in the docs of different losses
module: nn	Deprecate hardtanh type promotion behavior.
module: nn	Unable to programmatically update models using references from model.named_modules()...requires additional parsing
module: nn	Module parameters/submodules can be shadowed by class attributes silently
module: nn	Softmax, LogSoftmax are over parameterized
module: nn	max_unpool2d is not deterministic
module: nn	nn.CosineSimilarity returns value larger than 1
module: nn	stateless.functional_call doesn't work with nn.DataParallel
module: nn	max_pool1d can succeed when padding is negative for tensor requiring grad
module: nn	Subclasses with unwrapping __torch_dispatch__ impls as parameters
module: nn	CrossEntropyLoss computes SoftMax always across the second dimension
module: nn	Unable to continue adding modules to nn.Sequential after using del method
module: nn	Incorrect documentation in gumble_softmax function.
module: nn	Avoid Self-loops on Module Creation
module: onnx	torch.onnx.export has inconsistent results when exported on torch1.8 and torch1.12
module: onnx	We don't have an op for aten::view but it isn't a special case.
module: onnx	Build fails with USE_SYSTEM_ONNX=OFF: C++17 features are used while PyTorch is building with the C++14 option
module: onnx	incorrect output shape for nn.AvgPool2d with ceil_mode=True
module: onnx	RUNTIME_EXCEPTION : Non-zero status code returned while running Mul node. Name:'/time_proj/Mul' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:540 void onnxruntime::BroadcastIterator::Init(ptrdiff_t, ptrdiff_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 2 by 160
module: onnx	[ONNX] Refactor op consistency tests
module: onnx	ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format
module: onnx	quant resnet convert onnx error: orch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::quantize_per_channel' to ONNX opset version 15 is not supported
module: onnx	[ONNX]  GFPGANv1.pth to onnx conversion error
module: onnx	xfail_if_model_type_is_exportedprogram and xfail_if_model_type_is_not_exportedprogram don't catch unexpected success
module: onnx	huggingface gpt2 has missing inputs on torch.onnx.dynamo_export using torch.nn.Module model
module: onnx	[ONNX] Modularize pass does not work for torch.onnx.dynamo_export with ExportedProgram
module: onnx	dynamo_export: Unsupported FX nodes: aten.pixel_shuffle.default
module: onnx	[Release 2.2][ONNX]Store user model to simplify ONNXProgram.{adapt_torch_*,__call__} AP
module: onnx	torch.onnx is not support baichuan_7b 'aten::unflatten' operator
module: onnx	nn.BatchNorm2d layers cause redundant Identity operators in exported ONNX
module: onnx	Export Transformer to ONNX results in SymIntArrayRef Error
module: onnx	Revisit ONNXProgram API due to fake tensor support and additional model_with_state_dict addition
module: onnx	[ONNX] Test the new exporter with torchvision
module: onnx	Add dynamic shape tests for important models to guard against regression
module: onnx	torch.onnx.export to support opset 20.
module: onnx	[ONNX] Refactor op level_debug to catch mismatches between ONNX models and ExportedProgram and nn.Module
module: onnx	[ONNX] Extend test_fx_op_conistency.py to take ExportedProgram converter
module: onnx	torch.onnx.dynamo_export functionalization does not support aten.add_.Tensor
module: onnx	Unsupported operator error: aten::to_mkldnn export to ONNX not supported
module: onnx	[ONNX] ONNX export of simple quantized model fails
module: onnx	[ONNX][dynamo_export] ONNX::Celu Half unsupported but export passed w/ invalid model when opmath disabled
module: onnx	Upsample trilinear onnx
module: onnx	[ONNX] Refactor xfail API to handle conditional failure scenarios
module: onnx	[ONNX] Track dynamic shapes integration for torch.onnx.dynamo_export
module: onnx	[ONNX] Execute ONNX Runtime with IOBindings through ONNXProgram.__call__
module: onnx	[ONNX] ONNX export fails when combining tracing and scripting in the presence of symbolic functions
module: onnx	[ONNX] STFT ExportProgram error
module: onnx	ONNX Export - miscompilation for complex-valued operators
module: onnx	[ONNX] stft export fails with dynamo_export
module: onnx	torch.export does not support torchaudio.transforms.Spectrogram
module: onnx	dynamo_export successfully export model but fails at onnx.checker.check_model
module: onnx	torch._dynamo.export raises Unexpected type in sourceless builder <class 'nemo.core.neural_types.elements.VoidType'> for torchaudio model
module: onnx	'aten::unique_consecutive' to ONNX opset version 14 is not supported
module: onnx	[ONNX] Assertion in models is not supported by fx exporter
module: onnx	USE_SYSTEM_ONNX: undefined references
module: onnx	when convert to onnx ,the jit will merge th outputs, it results to we can't distinguish  what the outputs represents
module: onnx	Constant output from exported ONNX
module: onnx	[ONNX] Result from export_onnx in pytorch returns different result from pytorch
module: onnx	[ONNX] Exporting the operator 'aten::sparse_coo_tensor' to ONNX opset version 17 is not supported
module: onnx	[ONNX] In-place additon not being functionalized by torch.onnx.dynamo_export
module: onnx	[ONNX] Expose the graph module in torch.onnx ExportOutput
module: onnx	Export List/Tuple type inputs with dynamic size
module: onnx	torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::binary_cross_entropy' to ONNX opset version 14 is not supported.
module: onnx	Can't export a pth model to onnx (RuntimeError: Couldn't lower all tuples)
module: onnx	[ONNX][dynamo] Parameter to export flat graphs
module: onnx	[ONNX][dynamo] Failed to export cumsum with dtype=float16
module: onnx	Couldn't export yolov7 quantized model to onnx
module: onnx	[ONNX][Exporter] Maintain support for exporter arguments export_params and keep_initializers_as_inputs
module: onnx	Wrong onnx model from torch.onnx.export when using index_add_ function with duplicate index values.
module: onnx	Gradients (Jacobian) in inference
module: onnx	ONNX converter does not properly trace dynamic axis through graph
module: onnx	onnx export jit.script ShapeInferenceError Unexpected axis value: 1. Expected range [-1, 1)
module: onnx	[ONNX] Figure out aot inline strategy for Dort / onnxrt backend
module: onnx	test_pytorch_onnx_onnxruntime_cuda.py is not run in CI
module: onnx	Using torch.onnx.export from file named onnx.py results in cryptic error message
module: onnx	Torch.onnx.export of module used positional and keyword arguments
module: onnx	ONNX export: TransformerEncoder is exported with fixed input dims
module: onnx	ONNX export of torch.nn.Transformer still fails
module: onnx	[ONNX] Replace torchscript with new graph building
module: onnx	ValueError: args contained 2 None's after flattening. When exporting a ScriptModule or ScriptFunction, no args may be None because that breaks type propagation.
module: onnx	Torch.onnx.dynamo_export stuck at reshape
module: onnx	Some ONNX tests have been disabled because of new tensor.split signature
module: onnx	Fix linalg_vector_norm ONNX export with wrong output dtype
module: onnx	torch.onnx.export causes floating point exception with core dump for empty slice assignment
module: onnx	[WIP] Make ONNX OpSchema function matcher more robust
module: onnx	Large Discrepancies between PyTorch and ONNXRuntime Inference
module: onnx	Incompatible dimensions error for FusedMatMul
module: onnx	LLaMA-2 70b model convert from PyTorch to ONNX format
module: onnx	[ONNX] Remove the deprecated function _export
module: onnx	Add onnx backend to torch.export API
module: onnx	[Torch-Onnx] Exporting the operator 'quantized::conv_transpose2d' to ONNX opset version 13 is not supported.
module: onnx	ONNX Export error
module: onnx	Cannot export a quantized model that permutes a quantized tensor to ONNX
module: onnx	Create static analysis tool to improve ONNX export success
module: onnx	Attribute 'kernel_shape' is expected to have field 'ints' when exporting a module with List[Tensor] inputs/outputs
module: onnx	aten::squeeze exported to ONNX as an If node
module: onnx	ONNX exporter issue: fails to add conversions exporting T5 Transformer model
module: onnx	[ONNX] Provide an option to not generate report_dynamo_export.sarif
module: onnx	Introduce 'backend' concept to torch.export.export API
module: onnx	Exporting the operator 'aten::_convolution_mode' to ONNX opset version 14 is not supported.
module: onnx	Support ONNX export for aten::select_backward and aten::slice_backward
module: onnx	torch.onnx.export does not trace all outputs for the HF BLOOM model
module: onnx	torch model to onnx conversion success but failed when inference
module: onnx	Crash on converting circular padding  to onnx
module: onnx	ONNX export constant folding messes up with shared weight deduplication
module: onnx	ONNX-FX based exporter documentation/tutorial topics for PyTorch 2.1
module: onnx	Unrecognized attribute: axes for operator ReduceMean during onnx model conversion
module: onnx	Exporting the operator 'aten::linalg_inv' to ONNX opset version 18 is not supported.
module: onnx	Torch 1.13 Onnx Scope constant name not correct!
module: onnx	Export to onnx error: RuntimeError: ArrayRef: invalid index Index = 3; Length = 3
module: onnx	Got Expand nodes with static shape input when exporting onnx model with dynamic shape
module: onnx	libtorch vs (onnx+tensorRT) show different object detection results
module: onnx	[ONNX] Retire FXSymbolicTracer in FX exporter
module: onnx	Error in ONNX during Export GLU with Opset 18
module: onnx	repeat_interleave does not support tensor indexes on different devices while repeat does
module: onnx	Export of quantized::linear_relu operator not supported with torch.onnx.export
module: onnx	Getting more human-readable input and output names in the onnx model exported by torch
module: onnx	How to export GNN with dict inputs correctly?
module: onnx	[ONNX] ONNX doesn't support exporting non-persistent buffer included models in FakeMode
module: onnx	[feature request] [onnx] Support QuantLinear/DequantLinear float16 inputs (opset19 and maybe "backport"-support them for opset17)
module: onnx	cov onnx error
module: onnx	RuntimeError: 0 INTERNAL ASSERT FAILED at "../torch/csrc/jit/ir/alias_analysis.cpp":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, NoneType, NoneType, Device, bool,
module: onnx	Facing error while using onnx from scatterelements
module: onnx	RuntimeError: _Map_base::at when exporting squeeze
module: onnx	ONNX Model Producing Different Results Compared to Original PyTorch and JIT Traced Model
module: onnx	Cannot export MiVOLO model into onnx format using torch.onnx.export
module: onnx	[ONNX] Float8 support
module: onnx	[Dynamo] Integration exporter's diagnostic system into ONNXRuntime backend
module: onnx	[Dynamo] revise ONNXRuntime backend's use of CapabilityBasedPartitioner
module: onnx	[Dynam] a graph pass in Dynamo-ONNXRuntime backend needs revision
module: onnx	[Dyanmo] Pre-allocate flag should be a ONNXRuntime inference session level attribute
module: onnx	[Dynamo] ONNXRuntime backend (DORT) requires some guards to re-partition extracted by Dynamo
module: onnx	[Dynamo] ONNXRuntime Backend Shold Allow External Allocator
module: onnx	Please verify 1.14.1 ONNX release candidate on TestPyPI
module: onnx	[FX][ONNX][exporter] Failed to export traced fx graph to onnx model
module: onnx	[ONNX] Keep functional ops as functions in dynamo exported onnx
module: onnx	[ONNX] scatter_reduce does not support include_self=False
module: onnx	Torch.onnx.export a fp16 model but get the output tensor fp32
module: onnx	Exporting the operator 'aten::grad' to ONNX opset version 18 is not supported.
module: onnx	Revisit checkpoint naming mismatch with torch name (and ONNX initializer name as a consequence)
module: onnx	Unable to build documents
module: onnx	[ONNX] fix test_fx_op_consistency.py test failure when running on torch built with cuda
module: onnx	torch.onnx.export error
module: onnx	[ONNX] Exporting the operator 'aten::exponential' to opset version 13 is not supported
module: onnx	[ONNX] FX produce valid node names in models
module: onnx	[ONNX] Support Fake Tensor Mode on new Dynamo based ONNX exporter
module: onnx	torch.onnx.export failed: torch.onnx.errors.SymbolicValueError: Unsupported: ONNX export of convolution for kernel of unknown shape
module: onnx	torch.onnx.export does not support divisor_override in AvgPool2d
module: onnx	[ONNX] Refactor test_fx_op_consistency.py
module: onnx	Failed to convert model that has LeakyReLU to ONNX
module: onnx	Support ONNX opset 20 to export GELU to one single op
module: onnx	TypeError: 'NoneType' object is not subscriptable (Occurred when translating col2im). Can't translate torch.nn.functional.fold in opset_version 18.
module: onnx	[ONNX] Support aten::var_mean
module: onnx	test_view_dynamic_zero_dim no longer testing zero input
module: onnx	ONNX export process  failed to keep consistence of input_names specified
module: onnx	torch.onnx.export does not respect nn.Module.forward API when using export_modules_as_functions=True
module: onnx	torch.compile fails with "INTERNAL ASSERT FAILED" when compiling GPT-2
module: onnx	[ONNX][TypePromo] Automate codegen type promotion rules
module: onnx	Inconsistencies in ONNX exporting of operation torch.full()
module: onnx	[ONNX] Investigate nn.functional.nll_loss skip/xfail reason
module: onnx	[ONNX] Isolate TorchScript-based code-base from Dynamo-based ONNX exporter for easier deprecation
module: onnx	Runtime Error outerNode->outputs().size() == node->inputs().size() INTERNAL ASSERT FAILED when exporting custom operator
module: onnx	[ONNX] FX exporter: replace aten::copy_ with out-place version
module: onnx	Error when building with USE_TENSORRT=1
module: onnx	[ONNX] Handle absence of onnxscript module in PyTorch requirements.txt
module: onnx	[ONNX] Discuss improvements to Diagnostic public API
module: onnx	[ONNX] Support aten::mT
module: onnx	[ONNX] Support aten::linalg_solve_triangular
module: onnx	[ONNX] Support aten::linalg_cholesky_ex
module: onnx	[OOM] Unable to convert 30B model to ONNX, using 4x A100's
module: onnx	[onnx] aten::cumprod cannot be exported to ONNX
module: onnx	torch.onnx.export error ------RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
module: onnx	Exporting the operator \'aten::fused_moving_avg_obs_fake_quant\' to ONNX opset version 13  is not supported
module: onnx	Error when exporting to onnx for albert-base-v2, issue with attention_mask
module: onnx	torch.onnx.errors.CheckerError: The model does not have an ir_version set properly.
module: onnx	Add support ONNX Opset 19
module: onnx	[ONNX] test_op_consistency.py doesn't support constant inputs
module: onnx	Documentation Error of torch.onnx
module: onnx	Error, attribute exists on the Python module, but we failed to convert Python type: 'list' to a TorchScript type
module: onnx	Exporting the operator 'aten::scatter_reduce' to ONNX opset version 15 is not supported
module: onnx	onnx runtime error
module: onnx	ONNX model different to pytorch and jit trace output
module: onnx	[ONNX] OnnxFunction of aten_index_put_bool operation isn't consistent to aten::index_put inx FX exporter
module: onnx	Cannot export quantized model to onnx: cannot call qscheme on UnknownQuantizer
module: onnx	Exporting the operator 'prim::is_cuda' to ONNX opset version 14 is not supported
module: onnx	Unsupported: ONNX export of operator interpolate (with scales) error
module: onnx	ONNX TorchDynamo Exporter  - Ability to export and load ONNX files without parameters
module: onnx	[onnx] UnsupportedOperatorError: Exporting the operator 'aten::l1_loss' to ONNX opset version 17 is not supported
module: onnx	onnx.export fails if do_constant_folding=False
module: onnx	ONNX Opset 16 GridSample Does Not Support 5D Volumetric Input Tensor
module: onnx	[ONNX] Opset 18 support for TorchScript exporter
module: onnx	Can't export onnx model from a torch script model
module: onnx	graph._export_onnx() incorrect data types in the binary string representation
module: onnx	Error saving MONAI pytorch model to ONNX
module: onnx	I encountered an error while trying to save the stylegan2 network as torch. onnx. export
module: onnx	Deformable Convolution export to onnx
module: onnx	when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically
module: onnx	Why nn.Upsample/F.interpolate followed by nn.InstanceNorm2d will report error "Unsupported: ONNX export of instance_norm for unknown channel size."
module: onnx	Please verify 1.14.0 ONNX release candidate on TestPyPI
module: onnx	Pytorch member variable not working after converting to onnx format
module: onnx	[onnx]Unsupported: ONNX export of convolution for kernel of unknown shape
module: onnx	"We don't have an op for aten::bitwise_and but it isn't a special case." when exporting NMS operation as ONNX.
module: onnx	[onnx] AdaptiveMaxPool2d can not convert to GlobalMaxPool
module: onnx	Unable to run session using exported ONNX model using dictionary input
module: onnx	Is there a recommended implementation of yuv2RGB for the current torch?
module: onnx	torch.onnx.errors.OnnxExporterError: Unsupported: ONNX export of operator unsafe_chunk, unknown dimension size.
module: onnx	Document _wrap_fx_args_as_onnxscript_args
module: onnx	torch.onnx.export support sparse tensor format
module: onnx	onnxrt fails with compilations
module: onnx	pytorch dynamic quantized model failed to convert to onnx
module: onnx	Unsupported: ONNX export of operator group_norm, unknown input rank.
module: onnx	torch.onnx.export crashes on ReduceMax operator with onnx opset 18
module: onnx	RuntimeError: NYI: Named tensors are not supported with the tracer
module: onnx	Get error: "tuple index with non-constant index" when exporting a model to ONNX format
module: onnx	[ONNX] Export failed for Module with Keyword-only inputs
module: onnx	torch.onnx.export failed for models with Bernoulli operator
module: onnx	[ONNX] FX exporter 'test_models_onnxruntime.py' tracker
module: onnx	[ONNX] FX exporter 'test_pytorch_onnx_onnxruntime.py' tracker
module: onnx	'aten::affine_grid_generator' to ONNX opset version 14 is not supported
module: onnx	Input names provided three but onnx recognizes two inputs only
module: onnx	[onnx] sort / argsort with stable argument specified cannot be exported to onnx
module: onnx	ONNX Exporter for circular padding mode in convolution ops
module: onnx	Proposal: @capture: Unified API for capturing functions across {fx, proxy_tensor, dynamo}
module: onnx	Make torch.onnx.utils._optimize_graph use several CPU cores
module: onnx	UnsupportedOperatorError, OnnxExporterError and SymbolicValueError related to MultiheadAttention export to onnx with torch.jit.script
module: onnx	Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported
module: onnx	ONNX export produces hundreds of weight/bias/Matmul/etc. files alongside the .onnx file, and the .onnx file seems to be incorrect.
module: onnx	GroupNorm ONNX export does not reproduce same output
module: onnx	Pytorch 2.0: Detection models from torchvision don't work with onnx and tensorrt backends
module: onnx	onnx_torch.ModelProto exceeded maximum protobuf size of 2GB
module: onnx	ONNX Export Fails: Model input type is Dict[str, Tensor]
module: onnx	Quantized Transformer ONNX Export Fails
module: onnx	aten::int_repr not supported in torch.onnx.export
module: onnx	Bad conversion from torch.split(2d_tensor,splitsize_list) to SplitToSequence OP (onnx export)
module: onnx	ONNX export of batch_norm for unknown channel size issue.
module: onnx	An error happend when I convert pytorch model to onnx
module: onnx	Torchdynamo with onnxrt backend generating fake tensor errors
module: onnx	ONNXRuntime outputs numerically incorrect results for mixed precision models.
module: onnx	torch.onnx.export is throwing RuntimeError: prim::TupleUnpack not matched to tuple construct
module: optimizer	Add testing regarding SparseAdam state_dicts
module: optimizer	Add BC test for load_state_dict on OptimizerInfos and ModuleInfos
module: optimizer	test_can_load_older_state_dict_NAdam_cuda_float32 errors with PYTORCH_TEST_WITH_DYNAMO
module: optimizer	Dynamo'd optimizer does not handle closure correctly
module: optimizer	Legitimize/harden differentiable optimizers in torch.optim
module: optimizer	DISABLED test_foreach_matches_forloop_AdamW_cpu_float64 (__main__.TestOptimRenewedCPU)
module: optimizer	Refactor precision handling in test/test_optim.py
module: optimizer	Dynamo'd test_mixed_device_dtype needs higher tolerances for SGD and RMSProp
module: optimizer	dynamo'ing SGD w/ momentum errors for CPU params with empty grads
module: optimizer	Add a capturable impl to ASGD single tensor
module: optimizer	With dynamo, peak memory usage is higher for the Adam family
module: optimizer	With dynamo, test_foreach_matches_forloop recompiles too many times
module: optimizer	[TestOptimRenewed] test_set_default_dtype_works_with_foreach errors with dynamo
module: optimizer	Penalisation of bias and BatchNorm parameters
module: optimizer	arctan2 fp16 error when optimising
module: optimizer	Dynamo'ing Rprop, RMSprop, and Adadelta misses incrementing step due to skipping _init_group
module: optimizer	dynamo does not correctly handle future iterations if a specific iteration is frozen
module: optimizer	Enable skipped MPS OptimizerInfo tests
module: optimizer	Add CoRe optimizer
module: optimizer	Consider adding y/x -> y * 1/x optimization for _foreach_div_.ScalarList and other div Scalar overloads
module: optimizer	New Optimizer
module: optimizer	Pass epoch to SequentialLR and ChainedScheduler
module: optimizer	Failure to resume from a normal (non-FSDP) checkpoint due to the optimizer state dict rekey
module: optimizer	[dynamo + optim] complex, sparse are not on tracing testing path
module: optimizer	LBFGS accuracy difference between CPU and GPU
module: optimizer	[dynamo] Proposal: @init_values_once API for initializing tensors and constants - without tracing the function in Dynamo
module: optimizer	[dynamo] annotate allow_in_graph with soft constraints
module: optimizer	overloads can perhaps be more performant?
module: optimizer	Unprompted UserWarning
module: optimizer	Optim.Adam 'step' default setting bug.
module: optimizer	RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at "../aten/src/ATen/native/cpu/Loops.h":349, ... please report a bug to PyTorch.
module: optimizer	Using ChainedScheduler with ReduceLROnPlateau leads to unexpected keyword argument error
module: optimizer	Fused Adamw RuntimeError: params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout
module: optimizer	[dynamo] Slow compile times for optimizers due to for loops
module: optimizer	RuntimeError: Expected packed scalar Tensor to be of dimension 1. Got 0 instead.
module: optimizer	torch.optim.Adafactor
module: optimizer	RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward()
module: optimizer	Suppport Fused AdamW on CPU
module: optimizer	_foreach_copy_ with scalar second arg
module: optimizer	[Optimizer Perf] Improve speed of _init_group to c++
module: optimizer	[CPP API] Add Adadelta, Adamax, ASGD, NAdam, RAdam and Rprop
module: optimizer	[Dynamo] Unable to Trace AdamW Optimizer when there is LR Scheduler
module: optimizer	stride of gradient is not same as the corresponding tensor
module: optimizer	Other overloads of _foreach_clamp
module: optimizer	Optimizers should use learning rates passed as tensors directly
module: optimizer	OneCycleLR's state_dict includes a full reference to the optimizer
module: optimizer	RProp improvement tracker
module: optimizer	Got error when train models with more than one param_group in torch2.0
module: optimizer	Need support and testing for Adam optimizer for MPS
module: optimizer	Backward pass with sparse parameters results in error "Sparse division requires a scalar or zero-dim dense tensor divisor"
module: optimizer	Torch's LayerNorm and Adam optimizer vs those in tensorflow
module: optimizer	[BE] Evaluate and improve eager for-loop optimizer memory perf
module: optimizer	ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups.
module: optimizer	Asynchronous CUDA AveragedModel
module: optimizer	Deprecation warning on lr_scheduler.step(num_steps)
module: optimizer	Improve _group_tensors_by_device_and_dtype
module: optimizer	Disabling ALL TestOptim on the dynamo config
module: optimizer	ExponentialLR unexpectedly calls step() when init argument last_epoch is larger than -1
module: optimizer	SparseAdam: working with dense parameters but sparse gradients - usecase
module: optimizer	fused torch.optim.AdamW isn't faster than the unfused version
module: optimizer	Multiple Learning Rate Scheduler for Specific Parameters Groups
module: optimizer	Wrong type for get_lr inside lr_scheduler.pyi
module: optimizer	Remove lr_scheduler.print_lr
module: optimizer	PyTorch 1.12, high failure rate for test_optim/test_nadam
module: optimizer	dynamo sometimes hits the cache size limit due to the foreach flag in optimizer.step()
module: optimizer	Small learning rate with capturable=True causes Adam optimizer to blow up model parameters.
module: optimizer	Optimizer "Lion" in  Symbolic Discovery of Optimization Algorithms
module: optimizer	Pickling OneCycleLR.state_dict() with an unpickleable optimizer will result in an error.
module: optimizer	Support for VeLO optimizer.
module: optimizer	LBFGS wolfe exceeds the maximum allowed iterations
module: optimizer	Implement L1 and L2 gradient as hooks with the option of changing the weight decay value.
module: optimizer	Adam (fused=True) issues
module: optimizer	Error in Adam.step(): If capturable=True, params and state_steps must be CUDA tensors.
module: optimizer	WIP: feat: LARS optimizer
module: optimizer	Checkpointing Support for Modularized Optimizers
module: optimizer	C++ Adagrad optimizer doesn't initialize parameter state
module: optimizer	Implementation of CG, and BICGSTAB methods
module: optimizer	Documentation and typing hints for RProp
module: optimizer	Add complex support for SparseAdam and LBFGS optimizers
module: optimizer	Add maximize support to LBFGS optimizer
module: optimizer	[optim] asgd : handling of complex params as real params (NaN vs inf)
module: optimizer	Lack of newly raised optimizers
module: optimizer	pytorch 1.12.1 Adam Optimizer Malfunction!!!
module: optimizer	Adding Levenberg-marquardt optimizer in PyTorch
module: optimizer	Expand Learning rate scheduling to any optimization hyperparameter
module: optimizer	float' object is not callable when using scheduler.step() with MultiplicativeLR
module: optimizer	SparseAdam performance issue during optimizer step
module: optimizer	Overlapping Optimizer.step() with DDP backward
module: optimizer	Nonliner conjugate gradient optimizer + Hager-Zhang line search
module: optimizer	CosineAnnealingWarmRestarts with initial warm up and weight decay applied on consecutive cycles without warm up
module: optimizer	Adam not optimally implemented: unnecessary torch.div
module: optimizer	out-of-place functional optimizers: functional optimizers may not be composite compliant
module: optimizer	Adam is 30% slower than SGD on Apple Metal.
module: optimizer	Potential memory leak in Adam optimizer in AMD chips (CPU)
module: optimizer	[Proposal] Use batched oprations to accelerate PowerSGD
module: optimizer	Adding novel 'AdaFamily' optimizer
module: optimizer	Problematic ASGD Optimizer
module: optimizer	[CTA] Let's Stamp Out Flaky Tests!
module: optimizer	Inconsistent implementation on SWA
module: optimizer	Adam optimizer doesn't work with CyclicLR scheduler but works with OneCycleLR.
module: optimizer	SequentialLR scheduler incorrect initialization
module: optimizer	Add pct_end parameter to OneCycleLR
module: optimizer	Also allow dicts as type of params= field in param groups of optimizers
module: optimizer	EMA optimizer: class-form and function-form (using new foreach_lerp) - can be used for explicit robust updates of BatchNorm stats
module: optimizer	[RFC] Gossip SGD (as a DDP Communication Hook)
module: optimizer	upstream apex.optimizers.FusedAdam to replace torch.optim.AdamW
module: optimizer	UserWarning: Seems like optimizer.step() has been overridden after learning rate scheduler initialization. Please, make sure to call optimizer.step() before lr_scheduler.step(). See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate   warnings.warn("Seems like optimizer.step() has been overridden after learning rate scheduler
module: optimizer	Rprop Optimizer: UnboundLocalError: local variable 'step_size_min' referenced before assignment
module: optimizer	CosineAnnealingWarmRestarts should use integer epoch
module: optimizer	last_epoch parameter of CyclicLR and OneCycleLR is not the number of epochs
module: optimizer	Support for arbitrary schedulers in SequentialLR
module: optimizer	SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments
module: optimizer	SequentialLR object has no attribute '_last_lr'
module: optimizer	[feature request] Provide functional form of scheduler formulas (and reconsider older decisions of not doing it)
module: optimizer	Unstable buggy calculation.
module: optimizer	Some lr-schedulers docs seems to have typos/missing information
module: optimizer	[RFC] APEX style fused optimizers in PyTorch
module: optimizer	The SequentialLR scheduler uses a deprecated pattern
module: optimizer	Allow LRScheduler to take in param_groups directly without an optimizer
module: optimizer	Add an LRScheduler interface for torch schedulers.
module: optimizer	Re-implement Optimizer.__repr__
module: optimizer	SequentialLR have a question and why it use step(epoch)
module: optimizer	lr_scheduler.py  /  list index out of range
module: optimizer	Unexpected behaviour when resuming from checkpoint using CosineAnnealingLR
module: optimizer	Replace clone.detach with detach.clone
module: optimizer	torch.gather with sparse_grad=True does not work with SGD optimizer with momentum; gives bad error message
module: optimizer	MultiStepLR with different gammas for each parameter group
module: optimizer	PyTorch 1.9.0, test_optim fails on Nvidia A10.
module: optimizer	add gamma to CosineAnnealingWarmRestarts so max lr can decrease in cycles
module: optimizer	Learning rate scheduler list index out of range
module: optimizer	Feature Request: Remove Optimizer Lazy State Initialization
module: optimizer	DDP with cuda rpc failed with DistributedOptimizer Adagrad
module: optimizer	[discussion] Should Optimizers be also Modules?
module: optimizer	After the Adam optimizer used weight_decay, the model became extremely slow when tested on the CPU.(Time from 7 seconds to 46 seconds)
module: optimizer	Save checkpoint error
module: optimizer	Add clipnorm parameter to optimizers
module: optimizer	optim.Adadelta: local variable 'lr' referenced before assignment
module: optimizer	[amp scaler] unable to prevent "scheduler before optimizer step" warning
module: optimizer	[Feature Pitch] Full-batch optimization toolkit
module: optimizer	LR scheduler step() behaviour with and without epoch parameter
module: optimizer	[docs] torch.optim.lr_scheduler
module: optimizer	RMSProp documentation is confusing
module: optimizer	ZeroDivisionError: float division by zero in Adam (bias_correction1 is zero)
module: optimizer	Poor support of Optimizer.add_param_group
module: optimizer	lr_scheduler _triangular2_scale_fn calculation is overflowed
module: optimizer	LR Scheduler load_state_dict does not properly update scaling
module: optimizer	CosineAnnealingWarmRestarts LR scheduler fails when lash_epoch != -1
module: optimizer	optim.Optimizer should copy "params" before modifying them
module: optimizer	Additon of levenberg-marquardt optimizer in TORCH.OPTIM
module: optimizer	CosineAnnealingWarmRestarts lacks verbose functionality
module: optimizer	ValueError: optimizer got an empty parameter list
module: optimizer	Optimizer Closure: Enable skip by returning None loss.
module: optimizer	Add MultiScheduler
module: optimizer	LR scheduler get_lr() bug
module: optimizer	Flattening nn.Parameters while maintaining gradients from neural network forward pass
module: optimizer	Bug in CosineAnnealingWarmRestarts
module: optimizer	is this a typo in optimizer.pyi ? it says statue instead of state
module: optimizer	swa_utils.bn_update is too opinionated in how it calls the model
module: optimizer	Weight_decay in torch.Adam
module: optimizer	C++ optimizer check for duplicate parameters
module: optimizer	Minumul LR is never reached in
module: optimizer	ReduceLROnPlateau fails for negative input
module: optimizer	no member named 'beta1' in 'torch::optim::AdamOptions'
module: optimizer	[Feature request] Stochastic Frank-Wolfe optimizer
module: optimizer	[feature request] Show warning if optimizer.zero_grad() was not called before optimizer.step()
module: optimizer	Add Optimistic Mirror Descent (OMD)
module: optimizer	C++ Optimizer: remove warning on Optimizer::size method
module: optimizer	Including AdaBound in the list of Optimizers.
module: optimizer	Extra arguments included in the doc where they are not actually presented in the source code
module: optimizer	Functional interface for optimizers
module: optimizer	Optimization with constraints for torch.optim
module: optimizer	Incorrect documentation of SGD momentum
module: optimizer	checkpoint restore of optimizers changes dtype of Floating-point state
module: optimizer	Remove warning, and update documentation.
module: optimizer	Documentation mistake of Adam in v1.6.0?
module: optimizer	Optimizer support via Libtorch C++ on Android
module: optimizer	OneCycleLR argument pct_start used as a proportion not percentage
module: optimizer	KeyError: xxxxxxxxxx when calling optimizer.state_dict()
module: optimizer	[Feature Request] Add to, cpu, and cuda method to optimizer
module: optimizer	Can we have a way to reset a scheduler back to epoch -1
module: optimizer	SGD documentatiuon detail on g_{t+1}
module: optimizer	Simplify Adam Optimizer
module: optimizer	Is this a bug? The values calculated according to the document isn't equal to the values calculated by framework
module: optimizer	The values calculated according to the document isn't equal to the values calculated by framework
module: optimizer	Inconsistent behaviour when parameter appears multiple times in parameter list
module: optimizer	Learning rate change is not applied at designated iteration with a scheduler
module: optimizer	Updating learning rate with Libtorch 1.5 and optimiser options
module: optimizer	All keys matched successfully missing when loading state dict on optimizers
module: optimizer	Differentiable Optimizers
module: optimizer	Weight decay in AdamW
module: optimizer	weight_decay in Adam is not an L2 Penalty
module: optimizer	Immediate mode API (with functional flavor) for optimizers
module: optimizer	Inconsistent Documentation about Optimizer.step(closure)
module: optimizer	Reset a torch.optim.Optimizer
module: optimizer	pytorch latest update(1.4) broke CosineAnnealingWarmRestarts: T_cur is not define
module: optimizer	Add KFAC optimizer
module: optimizer	Add "strict" flag to ignore missing parameters in Optimizer.load_state_dict
module: optimizer	[C++ API Parity] Incorrect documentation for optim initialization in serialization docs
module: optimizer	SGD optimizer with deprecation warning
module: optimizer	[discussion] Relax optimizer constructor constraints for simplicity
module: optimizer	ReduceLROnPlateau detects a plateau during a steady decrease after a spike
module: optimizer	Exponentiated gradient descent
module: optimizer	SGD fails on sparse matrix
module: optimizer	MultiStepLR does not return good lr after load_state_dict
module: optimizer	New Stochastic Optimization Algorithms in Pytorch
module: optimizer	Default adam epsilon to 1e-7 when on fp16
module: optimizer	(PyTorch1.1 and 1.2) RuntimeError: Can't detach views in-place. Use detach() instead
module: optimizer	New Weight Scheduler Concept for Weight Decay
module: optimizer	Have a different way to check if gradient was computed in the optimizer (not checking for None)
module: optimizer	ReduceLROnPlateau will fail when add new parameter group to the optimizer
module: optimizer	CosineAnnealingLR has unexpected behavior with large step
module: optimizer	Inconsistant values of lr_scheduler.get_lr and lr in optimizer.param_groups
module: optimizer	CosineAnnealingLR giving unexpected learning rates on PyTorch 1.1.
module: optimizer	Add layer-wise adaptive rate scaling (LARS) optimizer
module: optimizer	Optimizer warning when parameters "change"
module: optimizer	[CAPI] Increase of memory usage when exporting a Adam optimzer
module: optimizer	Differentiation through Module parameters updates
module: optimizer	Misleading step method in lr_scheduler.ReduceLROnPlateau
module: optimizer	[feature request] Add COCOB Optimizer
module: optimizer	Incorrect term in _LRScheduler.
module: optimizer	Inconsistency in implementation of _LRScheduler
module: optimizer	[Feature Request] Add to() method for optimizers/schedulers
module: optimizer	[feature request] [PyTorch] More flexible optimizer API
module: optimizer	LBFGS contribution
module: optimizer	[feature request] Stochastic Variance Reduced Gradient (SVRG) optimizer
module: sparse	Add out_dtype support for sparse semi-structured CUTLASS back-end
module: sparse	Add at::sparse::full_coo_indices utility function.
module: sparse	Add batched sparse CSR/CSC/BSR/BSC to sparse COO conversion support
module: sparse	DISABLED test_linear_cutlass_backend_cutlass_cuda_float32 (__main__.TestSparseSemiStructuredCUDA)
module: sparse	Latest PyTorch is not buildable against CUDA-11.2
module: sparse	[Tracker] torch.sparse semi-structured 2.3 beta release
module: sparse	torch.mm() cuSPARSELt/CUTLASS back-end produce result of different dtypes for int8 inputs in case of an input being SparseSemiStructuredTensor
module: sparse	NotImplementedError: Cannot access storage of SparseCsrTensorImpl
module: sparse	[cusparseLt] Unexpectedly High Acceleration Ratio with cusparseLt in PyTorch
module: sparse	RuntimeError: cuSPARSELT not supported on your machine. When I was calling: torch._cslt_compress()
module: sparse	Sparse block tensors (torch.sparse)
module: sparse	SparseTensor index select uses more CUDA memory than Torch index select
module: sparse	Efficient Cholesky and QR updates
module: sparse	Bug in element-wise multiplication of torch.sparse_csr_tensors on GPU - 0's in result considered significant - PyTorch 2.1.1
module: sparse	[cusparseLt] CUDA error: internal error when calling cusparseLtStructuredDescriptorInit
module: sparse	Fake tensor produces incorrect values w/ is_coalesced and sparse_coo
module: sparse	[CUDA-12.2] cuSPARSE deprecated support for sparse BSR
module: sparse	CSR matrix on MPS
module: sparse	sparse allreduce not support  CSR format
module: sparse	Sparse Tensor Sum Still Does Not Work for PyTorch Geometric
module: sparse	I have a trouble with to_symmetric
module: sparse	Indexed batch matrix multiplication to support MoEs and FFFs
module: sparse	Inconsistent behavior for in-place operations on coalesced sparse tensors
module: sparse	Memory usage steadily increasing when using back propagation with sparse CSR parameter matrices on CPU
module: sparse	torch.mean not supported for torch.sparse_coo_tensor, but torch.sum is supported (scipy.sparse.coo_matrix does support both mean and sum)
module: sparse	Cannot construct torch.sparse_coo_tensor (but scipy.sparse.coo_matrix works fine): TypeError: only integer tensors of a single element can be converted to an index
module: sparse	torch.sparse_coo_tensor argname quirks + [feature request] .numpy()/from_numpy method for sparse_coo_tensor/sparse_csr_tensor (or maybe name them as .scipy()/.from_scipy() or at least under some torch.utils.* namespace
module: sparse	[feature request] Provide some sparse eigen solver(s) for PyTorch (maybe via ARPACK as in scipy) + SPD sparse / laplace linear system solver - maybe NVidia AMGx library?
module: sparse	Efficient and robust calculation of diag(sparse @ diag @ sparse)
module: sparse	_sampled_addmm_kernel cause 'misaligned address' with new triton pin
module: sparse	Select on a coalesced COO tensor returns COO tensor with coalesce flag set to False.
module: sparse	Adding batched CSR tensors with different sparsities produces an invalid tensor
module: sparse	Conversion from COO with two sparse dimensions to CSR with dense_dim specified fails
module: sparse	sparse_mask method ignores masked-in elements of sparse compressed input tensors
module: sparse	Error when using sparse_coo tensor with optimizer
module: sparse	Will torch.sparse.mm support multiplying two boolean matrices?
module: sparse	Sparse COO indices are torch.Int64 -- is this necessary?
module: sparse	Missing coalesced flag from torch.autograd.Function.backward
module: sparse	Strange backward behavior with sparse tensors
module: sparse	torch.sparse.mm() with reduce operator for GPU support and COO matrices
module: sparse	torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors
module: sparse	Efficient BMM for sparse-dense tensors
module: sparse	Softmax doesn't support sparse tensors with the CSR layout
module: sparse	Implementation of torch.sparse.sampled_baddmm
module: sparse	Backward pass with sparse parameters results in error "Sparse division requires a scalar or zero-dim dense tensor divisor"
module: sparse	NotImplementedError: Could not run 'aten::_spdiags' with arguments from the 'CUDA' backend.
module: sparse	Conversion of a CSR tensor with batches to COO tensor fails
module: sparse	Implement diag method for sparse COO tensors
module: sparse	Illegal Memory Access on H100 TestSparseCompressedTritonKernelsCUDA.test_triton_sampled_addmm_block_size_16_cuda_bfloat16
module: sparse	Conversion from strided to batched sparse compressed tensor with a non-constant number of zeros in batches fails
module: sparse	Tensor to_sparse fails on large matrices
module: sparse	Please consider the SCFA/dynamic flash attention for your implementation of scaled dot product attention
module: sparse	Upgrading SpGEMM algorithm to resolve Cusparse SpGEMM insufficient resources problem
module: sparse	"addmm_out_sparse_csr_impl_mkl" not implemented for 'Byte'
module: sparse	gradcheck produces false positives with sparse inputs when masked=False.
module: sparse	Support building pytorch using MKL ILP64 model.
module: sparse	Fix sparse windows on CPU with MKL
module: sparse	NotImplementedError in backprop on on dense-sparse matrices
module: sparse	torch.Tensor.is_sparse returns false for non-COO sparse tensors
module: sparse	dense -> sparse compressed to work with empty batches.
module: sparse	Tensor __getitem__ not documented, sparse grad?
module: sparse	Sparse Matrix nnz Overflow when casting from COO to CSR
module: sparse	torch.sparse_csc_tensor matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high
module: sparse	Automatic broadcasting for sparse csr tensors
module: sparse	FakeTensor lacks support for sparse compressed tensors
module: sparse	torch.sparse.sum backward fails when reducing over dense dimensions.
module: sparse	Add torch.cat  support for torch native sparse tensors. (Need for PyG)
module: sparse	torch.sparse_csr_tensor() stops gradients
module: sparse	Sparse Tensor not working for torch.cat
module: sparse	torch.matmul with batched CSR matrix
module: sparse	Strided to batch BSR/BSC conversion fails when the number of zeros per block varies while the number of blocks per patch is constant
module: sparse	matmul with CSR matrix in inference mode throws an exception
module: sparse	Support sparse COO/CSR/CSC/BSR/BSC return values in gradcheck input function
module: sparse	sparse_csr_tensor matmul wrong output in bfloat16
module: sparse	torch.zeros_like on a zero-sized BSR/BSC tensor results invalid tensor
module: sparse	Sparse is not available on Windows
module: sparse	jacrev and jacfwd raise an error that Sparse CSR tensors do not have strides
module: sparse	Adding sparse addmv and triangular_solve support on CPU - Mac OS - Apple Silicon M2
module: sparse	sparse.mm triggers INTERNAL ASSERT FAILED when backwarding
module: sparse	Compressed sparse constructor allows mixed int32/int64 indices which leads to dtype promotion/demotion in conversions.
module: sparse	gradgradcheck does not work with sparse inputs.
module: sparse	The torch.sparse document's typo error
module: sparse	torch.Tensor.is_set_to raises NotImplementedError when inputs contain sparse tensor
module: sparse	add/add_ for CSC: errors when trying to access non-existent crow_indices.
module: sparse	COO @ COO tries to allocate way too much memory on CUDA
module: sparse	CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz
module: sparse	Addition of hybrid CSR tensors produces incorrect and invalid CSR tensor
module: sparse	Addition of CSC/BSR/BSC tensors raises RuntimeError exceptions
module: sparse	Addition of batch CSR tensors produces incorrect and invalid CSR tensor
module: sparse	sparse.mm(coo, dense) produces wrong results on T4/V100 GPUs
module: sparse	mul(CSC, CSC) fails with layout mismatch between the inputs and the output.
module: sparse	torch.empty produces incorrect tensors with layout=sparse_csr|sparse_csc on the CPU
module: sparse	DISABLED test_coalesce_reference_cycle_cpu_float64 (__main__.TestSparseCPU)
module: sparse	Unable to backprop through dense weighted sum of sparse_coo_tensors
module: sparse	torch.svd_lowrank Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sparse	Different behaviour in sparse matmul
module: sparse	Add aten::empty.memory_format for SparseMPS
module: sparse	Semantics of sparse operations clarification - Sparsity of the gradient with respect to a sparse tensor input
module: sparse	index_select() applied in sparse tensor can't backprop
module: sparse	Triangular solver for sparse matrices
module: sparse	gradcheck failure with sparse matrix multiplication
module: sparse	Nandense layer for missing values
module: sparse	Automatic broadcasting for batch addition for sparse tensors
module: sparse	When will the torch.sparse module be usable?
module: sparse	Could not run 'aten::native_batch_norm' with arguments from the 'SparseCUDA' backend.  using batch_norm
module: sparse	Support different NSE in batches of CSR and CSC tensors
module: sparse	Batch multiplication for torch.sparse matrix multiplication
module: sparse	compiling failed from source
module: sparse	Minimal example for torch.optim.SparseAdam
module: sparse	Support for CSR Tensor with NN layers
module: sparse	Implement torch.clamp() on sparse tensors with SparseCPU backend
module: sparse	Stop manually binding sparse factory functions
module: sparse	test_sparse_matmul_cpu_complex128 fails on my local copy
module: sparse	test_sparse_spdiags_cpu_bool fails on my local working copy
module: sparse	RFC: auto-generated plain Tensor argument only sparse primitives
module: sparse	sparse_coo.to_dense() produces different results between CPU and CUDA backends for boolean non-coalesced inputs.
module: sparse	SparseAdam performance issue during optimizer step
module: sparse	Resize/reshape of sparse compressed tensors - design
module: sparse	bmm_sparse_cuda kernel for bfloat16
module: sparse	Allow parameterization of Layouts
module: sparse	Unable to use a parameter with torch.sparse_coo layout with DDP
module: sparse	Permutation of Sparse Tensor
module: sparse	Request for adding the possibility for training on sparse tensors
module: sparse	torch.smm backward fail with strange error message
module: sparse	Design API for accessing sparse tensor indices
module: sparse	Fix layout of masked output when all sparse dimensions are reduced
module: sparse	index_select allows negative index for sparse but not for strided self
module: sparse	Support batch indexing with sparse tensors with torch.sparse
module: sparse	Add BUILD_LAZY_CUDA_SPARSE option
module: sparse	[feature request] coalesce to support dim argument.
module: sparse	Enable dtype keyword argument for to_dense method
module: sparse	[CTA] Let's Stamp Out Flaky Tests!
module: sparse	torch has no attribute sparse_csr_tensor
module: sparse	Error in lobpcg when using largest=False
module: sparse	Slower performance of torch.mm method with sparse CSR tensor
module: sparse	Strange case of empty non-coalesced sparse tensor
module: sparse	Batched sparse-sparse matrix multiplication/ sparse torch.einsum
module: sparse	torch.bmm backward with sparse input
module: sparse	Kernel fusion for Gather, Apply, Scatter (GAS) model
module: sparse	Slow backward for matrix multiplication of two sparse COO tensors on CPU
module: sparse	Feature Request: Implement torch.sparse.spdiags and torch.sparse.diags
module: sparse	Implement torch.*_like tensor creation functions on sparse inputs
module: sparse	Implement aten::equal for sparse tensors
module: sparse	No dtype check for zero sparse tensor!
module: sparse	cuSOLVER backend for Sparse CSR direct linear solvers
module: sparse	torch.sparse.softmax and torch.sparse.log_softmax do not support negative dim.
module: sparse	[feature request] More masked reductions: amin/amax, argmin/argmax, quantile, mean/var/std/std_mean/var_mean
module: sparse	torch.sspaddmm should broadcast the input tensor
module: sparse	Transpose of a sparse tensor is not a view operation
module: sparse	sparse.mm: CUDA error: internal error when calling cusparseSpGEMM_workEstimation [...]
module: sparse	Sparse matrix multiplication (torch.sparse.mm) NotImplementedError
module: sparse	torch.sparse.sum on scalar sparse tensor fails when dim is specified
module: sparse	torch.sparse.sum result has wrong dtype when reducing over all dimensions
module: sparse	RuntimeError: Trying to create tensor with negative dimension -1741885395: [-1741885395]
module: sparse	[feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion
module: sparse	[Feature Request] Any plan to add 'Sparse Convolution' as default nn module?
module: sparse	torch.equal does not support sparse tensors
module: sparse	torch.gather with sparse_grad=True does not work with SGD optimizer with momentum; gives bad error message
module: sparse	torch.cuda.amp fails with torch.sparse.softmax
module: sparse	CUDA error: invalid configuration argument for torch.sparse tensor backward
module: sparse	Sparse updates to logits in distributions.Categorical
module: sparse	Clarify sparse COO tensor coalesce behavior wrt overflow + how to binarize a sparse tensor
module: sparse	cuSPARSELt Integration
module: sparse	Sparse CSR layout CPU backend tracking issue
module: sparse	Sparse CSR layout GPU backend tracking issue
module: sparse	Divergent code is needed to record usage streams on different TensorImpl types
module: sparse	Sparse CSR tensor should not accept equal column indices in the same row
module: sparse	Data access pattern in the loop in add_out_dense_sparse_csr_cuda could be pretty bad
module: sparse	Use of storage_offset is not needed  in add_out_dense_sparse_csr_cuda
module: sparse	CSR: Relaxing constraints to s_addmm_out_sparse_dense_cuda_worker
module: sparse	Issue: support auto generation of device check for sparse tensors
module: sparse	[Feature Pitch] Fast extremal eigensolvers
module: sparse	TensorIterator for sparse layouts
module: sparse	MKL csr matmul issue
module: sparse	CSR construction: safe_get_attr_string suppresses real errors
module: sparse	Avoid no-op suggest_memory_format call in SparseCsrTensorImpl::resize_as_sparse_csr_tensor_
module: sparse	Sparse tensor CSR layout for CUDA
module: sparse	Sparse-sparse matrix multiplication only works with torch.sparse.mm()
module: sparse	C++ sparse_coo_tensor ignores TensorOptions argument
module: sparse	Occured error in loss.backward() when using sparse=True in Embedding layer
module: sparse	[discussion] Support torch.matmul: strided x sparse in addition to sparse x strided
module: sparse	Conjugate gradient Descent, and Linear operator are not present in pytorch.
module: sparse	[OpInfo] Improvements for sparse ops tests
module: sparse	is_non_overlapping_and_dense() does not error out for sparse tensors
module: sparse	Modifying values() tensor of COO tensor requiring grad throws an odd error message
module: sparse	Function request: Sparse matrix inverse
module: sparse	sparse filter layers (more specifically convolutions)
module: sparse	torch.acos not supported for sparse layout
module: sparse	torch.eye not supported for sparse layout
module: sparse	Autograd support for the tensor multiplication of sparse tensors
module: sparse	Deprecate spmm and dsmm functions
module: sparse	Backpropagation for sparse matrix indexing is problematic (colab provided)
module: sparse	Unify matrix multiplications operations
module: sparse	Backward for sparse tensor item select does not work
module: sparse	torch.sparse improvements - tracking issue
module: sparse	Documentation and torch.sparse alias for torch.bmm sparse-dense
module: sparse	[FR] Raise an exception when constructing non-empty index and empty values sparse tensor
module: sparse	backward for dense+sparse does not work
module: sparse	Memory bug for backward on torch.sparse.mm?
module: sparse	Sparse Convolutional support
module: sparse	error when specifying sparse=True in embedding
module: sparse	Dropout on sparse tensors
module: sparse	[feature request] Sparse (hybrid sparse-dense) output option for topk, min, max
module: sparse	.detach() behaves differently for dense tensors vs sparse tensors
module: sparse	empty_sparse shouldn't be called with memory layout but is
module: sparse	Support sparse inputs for torch.block_diag
module: sparse	sparse tensor eliminate_zeros
module: sparse	update embedding at indices, other than those passed as input, in the case of sparse tensors
module: sparse	Document memory characteristics of in-place ops
module: sparse	There is no support for weight_decay/momentum in SGD for sparse tensors.
module: sparse	AdamSparse fails to run
module: sparse	SGD fails on sparse matrix
module: sparse	Tensor.nbytes() returns itemsize * numel for sparse tensors
module: sparse	RuntimeError: !t.is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:591
module: sparse	In-place updating the original value tensor should also update version counter of sparse tensor's values_ tensor
module: sparse	scatter_ supporting different reduction modes
module: sparse	Sparse tensors can't be used in DataLoader running many workers
module: sparse	Hardshrink for Sparse Tensors
module: sparse	Should torch.arange take a layout parameter?
module: sparse	Sparse matrix multiplication is too slow
module: sparse	In-place operations on .data or .detach() of sparse tensor doesn't update the original tensor
module: sparse	[Feature request] create sparse coo matrix w/o index check
module: sparse	Batch matmul with sparse matrix, dense vector
module: sparse	[sparse] add descriptions and examples for methods at torch.sparse doc page
module: sparse	Support calculating grad for dense in sparse @ dense
module: build	The Python module installs cmake files and headers under the Python's site-packages directory that can't be used
module: build	Build fails with USE_SYSTEM_ONNX=OFF: C++17 features are used while PyTorch is building with the C++14 option
module: build	error: cast from 'PyObject *(*)(THPCppFunction *, void *)' (aka '_object *(*)(torch::autograd::THPCppFunction *, void *)') to 'getter' (aka '_object *(*)(_object *, void *)') converts to incompatible function type [-Werror,-Wcast-function-type-strict]
module: build	Issue with Protoc while building PyTorch for RISC-V
module: build	Building extensions with CMake
module: build	Pytorch build from source fails
module: build	ninja: build stopped: subcommand failed.
module: build	Latest PyTorch is not buildable against CUDA-11.2
module: build	Pytorch with GPU support compile error on Jetson Xavier RX
module: build	[Libtorch/iOS] Build for iOS as dynamic library fails in linker phase.
module: build	pytorch cpuinfo submodule to be updated to the latest
module: build	Installation error: 'CMake Error at third_party/benchmark/CMakeLists.txt:304 (message):   Failed to determine the source files for the regular expression backend'
module: build	Respect user-specified USE_ROCM/USE_CUDA
module: build	Compile pytorch for ppc64 redhat8
module: build	manylinux_2_28 support
module: build	Unable to build on CUDA 11.8 due to cutlass incompatibility
module: build	Unknown CUDA Architecture Name 9.0a in CUDA_SELECT_NVCC_ARCH_FLAGS (compiling from source)
module: build	consistency checks for across minor version builds
module: build	/nodefaultlib:vcomp doesn't seem to be set when compiling with Intel OpenMP on Windows
module: build	Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself
module: build	Compilation error on loongarch64
module: build	Can't build PyTorch 2.1 from source by GCC 13.2 on M1 MacOS
module: build	Support for Bazel workspace function or Bazel module
module: build	Vendored FindCUDAToolkit.cmake deviates from upstream in splayed installation support
module: build	Library is included twice QNNPACK
module: build	Failed to compile: null in call to __builtin_memmove(__result, __first, sizeof(_Tp) * _Num); Debian 12, ppc64le, gcc 12.2
module: build	Migration from c10::variant to std::variant causes undefined symbols when linking against older pytorch
module: build	Cannot build static windows libraries
module: build	Static Linking C++, Op not available at runtime
module: build	build: failure when building pytorch with TBB
module: build	nonnull error
module: build	Use of -Wl,--as-needed in cmake config files can leak into third-party users' code and modify their own private libraries
module: build	Build failure with Xcode 15 linker
module: build	MAX_JOBS ignored when compiling pytorch from source
module: build	M2 Failing to build example-app in c++
module: build	Build process failure with torch_shm_manager
module: build	libtorch.so: error adding symbols: file in wrong format
module: build	Local build breakage on AWS cluster
module: build	Pytorch for Python 3.12 not available
module: build	PyTorch with non-shared build (building a single shared lib) is unsupported
module: build	ImportError: libc10_cuda.so: cannot open shared object file: No such file or directory
module: build	Please offer packages with local version torch==2.1.0+cpu for macOS
module: build	PPC64le: vsx_helpers.h errors
module: build	about nccl not work
module: build	Installation with rocm5.6 results in error: assert len(weights) == expected_node_count AssertionError
module: build	ModuleNotFoundError: No module named 'torchgen.code_template'
module: build	caffe does not respect CUDNN_LIB_DIR when building from source (cmake)
module: build	Found two conflicting CUDA installs
module: build	Can't build with non-static protobuf
module: build	Build failure due to C++ version mismatch
module: build	Libtorch linking Error:undefined reference
module: build	Compile error PyTorch 2.0.1 / GCC 13.1.0
module: build	Pytorch -  cpu only & caffe2 build failing
module: build	Enable SLEEF on ARM
module: build	extra information messages for mac in setup.py would help.
module: build	Documentation building fails due to torchgen
module: build	Segmentation fault when tensorrt is imported before torch
module: build	Error when building with USE_TENSORRT=1
module: build	Libtorch compile error when defining D_GLIBCXX_DEBUG
module: build	compilation fails error: invalid argument '-std=c++17' not allowed with 'C'
module: build	PyTorch can not be compiled with MKLDNN if system compiler is clang
module: build	Build fails at linking torch_shm_manager on aarch64
module: build	error: ‘aligned_alloc’ was not declared in this scope        static_cast<char*>(aligned_alloc(FLATBUFFERS_MAX_ALIGNMENT, size)), free);
module: build	how to workaround the error "don't have an op for vulkan_prepack::create_linear_context" ?
module: build	torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu
module: build	Pytorch Build images for RISCV64 Devices in the nightly builds
module: build	Error: no matching constructor for initialization of 'at::OptionalIntArrayRef'
module: build	Building NCCL with make -l $MAX_JOBS slows down builds
module: build	after add /path_to_libtorch/libtorch/lib to LD_LIBRARY_PATH, I can't import torch_scatter.
module: build	Pytorch CXX11 ABI version
module: build	[bazel] add inductor to bazel build
module: build	problem of compilation for torch2.0
module: build	Speed when installing from source is very low with CUDA 11
module: build	Pytorch compile failure on Windows with CUDA 12.1 because of lacking NVTX component
module: build	PyTorch installs the file mkldnn.cmake that looks for the package MKLDNN that doesn't exist
module: build	Error building Pytorch from source
module: build	'pip install triton' from pinned hash gives unreliable triton
module: build	Add compile option -Werror=return-type compile error
module: build	cuda 12.0 support request for building pytorch from source code
module: build	no-duplicate-decl-specifier as a invalid compile flag for CXX in GCC
module: build	Cross compile Pytorch for ARM in Bazel
module: build	add -std=c++20 build-only CI job
module: build	Build error on libstc++ header stl_alogbase.h on riscv
module: build	Issue on building from source: Remove -mfpu=neon option on MacOS with Apple silicon
module: build	Change module to module_ in torch/csrc/api/include/torch/python.h
module: build	PyTorch's packaged libgomp causes significant performance penalties on CPU when used together with other Python packages
module: build	Cannot use AT_CUDA_DRIVER_CHECK from user code
module: build	Power VSX vectorization support disabled
module: build	Fail to pass test HAVE_XXX_REGEX while  building pytorch
module: build	Torch 2.0 import hangs forever
module: build	After the release of pytorch 2.0.0, the compilation of ACLs is problematic.
module: build	Import fails when both USE_TENSORPIPE=OFF and USE_DISTRIBUTED=ON.
module: build	Sparse is not available on Windows
module: build	Building LibTorch on Ubuntu with Mac M1
module: build	ld: error: unknown argument '-force_load' when linking libtorch on Android
module: build	build failed when strictly following the guidelines
module: build	Add local version identifier to wheel file names
module: build	INTERNAL ASSERT FAILED -When using the PyTorch docker environment released by pytorch, a Vulcan support issue occurs
module: build	PR #88607 breaks build for POWER9 CPU
module: build	Build Error: no matching function for call to ‘dnnl::graph::stream::stream(<brace-enclosed initializer list>)’
module: build	Compiling PyTorch from Source on Xavier
module: build	Compiling libtorch from Source on Mac Beyond v1.11.0
module: build	Update PyTorch's default C standard to C17 from C11
module: build	Abort Caused by Virtual Function
module: build	Enable Link Time Optimization in PyTorch 2.0 Release Binaries - Smaller, Faster, Better Binaries
module: build	error: no member named 'residual_with_sum_zero_point' in 'ideep::attr_t
module: build	USE_CUDNN=1 doesn't force cmake to fail if cudnn is not found
module: build	Build from Source Issues on MacOS Ventura 13.2
module: build	Pytorch is using system-installed mkl-dnn.
module: build	Build from source fails: undefined reference to caffe2::DeviceQuery
module: build	Investigate CUDA enabled build-time difference between MSVC and GCC+WSL
module: build	Build Error: OpenMP library could not be found.  Proceeding might lead to highly sub-optimal performance.
module: build	functorch.so is installed back into the source directory
module: build	[bazel] error: use of undeclared identifier 'cudaGraphDebugDotPrint'
module: build	Pytorch clang-tidy header-filter is still broken
module: build	ModuleNotFoundError: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package
module: build	Cannot add target-level dependencies to non-existent target "gloo_cuda".
module: build	LibTorch static build from source missing libshm.so
module: build	Can not access to "sbgemm" routine with user-defined OpenBLAS
module: build	build: cmake: functorch.so not installed at expected location
module: build	build: cmake: ability to disable -Werror* (-Werror considered harmful)
module: build	build: cmake: need to uniformize installation of libraries in CMAKE_INSTALL_LIBDIR (not lib)
module: build	fbgemm_avx512 build failure
module: build	cross compile pytoch using cmake , get an error : protobuf::protoc: command not found
module: build	Glog macro redefinition problem when including headers from both libtorch and glog
module: build	ImportError: libcupti.so.11.2: cannot open shared object file: No such file or directory
module: build	The libtorch test SequentialTest.ModuleForwardMethodOptionalArg fails on Apple Silicon
module: build	Whether to support libtorch source code compilation of C++11 ？
module: build	build: failure when upgrade oneTBB to 2021.7.0
module: build	VS2022Preview ParallelCommon.cpp.obj : fatal error LNK1161: invalid export specification
module: build	link error happen when intergrate libtorch to other tool
module: build	libtorch_cpu.so is exposing some LLVM symbols
module: build	ninja: build stopped: subcommand failed
module: build	#error "Expected GLOO_USE_CUDA to be defined"
module: build	built from source windows static library with multiple "unresolved external symbol"
module: build	pytorch could not build from source with cudnn 8.0.5
module: build	Installing PyTorch with BUILD_SPLIT_CUDA=ON and CUDNN fails on linker error
module: build	RuntimeError: Tensors of type TensorImpl do not have numel
module: build	torch/csrc/utils/python_arg_parser.h:424:94: error: format ‘%ld’ expects argument of type ‘long int’, but argument 7 has type ‘int’
module: build	cppextension host compiler check ignores executable symbolic link in CUDA bin directory
module: build	compile torch from source
module: build	Pytorch built for Jetson errors if CUDA is not found
module: build	libtorch make failed
module: build	Compile failed at allreduce without gloo
module: build	fmt/src/os.cc: error: unknown type name 'error_code'; did you mean 'std::error_code'?
module: build	Figure out the future of Metal backend given the existence of MPS
module: build	Conda Pytorch (Pytorch channel) in WSL2 Ubuntu can't find libcudnn shared objects
module: build	Build from source failed with error of different gpu architecture (compiler shows sm_30-related error but I use sm_86 GPU)
module: build	Solve default argument induced include cycles by not using defaults / moving the defaults to inl
module: build	compiling failed from source
module: build	Installation prefix is not passed to CMake appropriately
module: build	I cannot install pytorch by Bad CRC-32 for file 'torch/lib/libtorch_cpu.so'
module: build	Undefined reference in libtorch_cpu.so ...std::__cxx11::basic_string...
module: build	pytorch 1.12.1 doesn't build with ffmpeg 5.0
module: build	Build from source failed on MacOS 10.6 with CUDA 10.1
module: build	Build and Run QNNPACK on X86
module: build	Fail to install torch for source
module: build	GPU arch 8.6 is not covered by the TORCH_CUDA_ARCH_LIST = All option
module: build	Error building Pytorch 13.1 from Source on OS X 12.5
module: build	build fail when using lto with gcc
module: build	DEBUG=1 env var doesn't actually set DEBUG preprocessor macro
module: build	[feature request] Discover actually loaded shared libraries at runtime
module: build	cant build with USE_VULKAN=1
module: build	Compatibility with newest MKL
module: build	Failed to static link latest cuDNN while compiling
module: build	Provide an option to disable CUDA_GCC_VERSIONS
module: build	Torch does not build with Lazy TS disabled
module: build	Linking pytorch libraries causes sstream behavior to be overridden globally
module: build	[vulkan]compiling VulkanOpContext.cpp with some errors
module: build	[discussion] Consolidation of audio-visual I/O in a new package
module: build	[Releng] Improve the tutorials release process
module: build	Add TorchDynamo as a submodule to Pytorch?
module: build	static builds are broken by MKL_DNN
module: build	caffe2_nvrtc is produced even when it won't be used
module: build	[build] No documented way to install C++ binaries for pure-python development of pytorch
module: build	[bug] Device dispatcher can choose CPU path for CUDA tensors.
module: build	Building PyTorch from Source with BUILD_LAZY_TS_BACKEND_ON
module: build	Libtorch C++ mobile build linking error
module: build	Cannot build master on AWS cluster: error: ‘__fatDeviceText’ was not declared in this scope
module: build	Mac M1 Build Failure on DEBUG=1
module: build	LibTorch cannot be used without nvcc
module: build	USE_NATIVE_ARCH flag causes nvcc build failure due to "'arch=native': expected a number"
module: build	Error occurred , when compile source code setting  BUILD_CAFFE2=ON
module: build	Fails to compile with GCC 12.1.0
module: build	net_observer_reporter_print.h missing
module: build	PyTorch fails to build on gcc 12 due to gloo
module: build	Build check for AVX512 fails with AMD CPU and march=native
module: build	Allow force building with/without AVX
module: build	Building from source results in broken __version__
module: build	Performance bad on ARM AArch64 for PyTorch C++
module: build	Update NCCL to 2.12
module: build	Failed to build on Ubuntu 18.04 due to bad MPI linker flags
module: build	Eliminate uses of deprecated FindCUDA.cmake
module: build	Add build support for GCC 11.2
module: build	'python setup.py build' failed but succeed using  'pip install -v .' which calls 'python setup.py build'.
module: build	add -D_GLIBCXX_ASSERTIONS in debug mode
module: build	make lint should advertise make setup_lint
module: build	PyTorch source code compile fail after "Built target fbgemm_avx2"
module: build	Add BUILD_LAZY_CUDA_SPARSE option
module: build	Install PyTorch from source on power machine
module: build	Can't install Pytorch
module: build	Compilation error on M1 Mac
module: build	can not build pytorch, failing due to missing _ctypes module
module: build	_run_ninja_build failing with clang
module: build	Run libtorch examples, export error "undefined reference to xxx"
module: build	Pytorch Installation from source fails
module: build	Build failure using GCC 11.2.0
module: build	Build from source failed
module: build	Make it easier to figure out if packages need to be interned/mocked/externed
module: build	Review and refactor  the way libcublas static libraries are linked
module: build	PyTorch fails to compile on gcc 11.2 due to breakpad
module: build	Multiple new caffe2-related build failures.
module: build	Clang Compilation Error: more than one constructor applies to convert from "ptrdiff_t" to "c10::Scalar"
module: build	Can't forward pass conv2d with kernel_size=1, and padding=1
module: build	Avoid using thrust:: directly, use THRUST_NS_QUALIFIER:: instead
module: build	Some system-installed headers are mistakenly used.
module: build	_GLIBCXX_USE_CXX11_ABI=0 does not work when building from source code
module: build	Build release binaries with USE_GLOG=ON by default
module: build	Compilation instructions are not exhaustive: <<parameter packs not expanded with ‘...’>> on Fedora 35/CUDA 11.6
module: build	[pytorch1.5.0] subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '6']' returned non-zero exit status 2.
module: build	a lot nightly builds are canceled due to VM errors since Jan14
module: build	Leaky cmake cuda compile options
module: build	Torchvision Installation Logic Python vs C++ "Mismatch"
module: build	1.10.11 fails to compile libtorch_cpu with -fopenmp
module: build	Redefinition of cub namespace misses Debug
module: performance	[discussion] Route pointwise Conv1d/Conv2d to matmul?
module: performance	Performance for TransformerEncoderLayer and TransformerDecoderLayer drops severely if pytorch version changed from pt1.10 to pt2.0
module: performance	Optimize cudnn_convolution_out to Reduce Unnecessary Memory Allocation and Copy
module: performance	Performance regression using latest nightlies and HF transformers
module: performance	max_pool2d_with_indices_backward_out_cuda: remove useless  code gradInput.zero_();
module: performance	Speed up triu_tril_kernel
module: performance	Unexpected poor performance of C++ extension / wish for a fast operator[]
module: performance	[inductor][cpu]basic_gnn_edgecnn performance regression
module: performance	[inductor][cpu]pyhpc_isoneutral_mixing performance regression
module: performance	[inductor][cpu]pyhpc_equation_of_state performance regression
module: performance	[inductor][cpu]swin_base_patch4_window7_224 AMP single thread performance regression
module: performance	[inductor][cpu]cspdarknet53 and tf_mixnet_l AMP single thread performance regression
module: performance	[inductor][cpu]detectron2_fcos_r_50_fpn accuracy and performance failure
module: performance	[PT2] Compile Cold Start (Persistent Cacheing) - AOTAutograd may be bottleneck when TORCHINDUCTOR_FX_GRAPH_CACHE=1
module: performance	Performance of torch.compile is significantly slowed down under torch.inference_mode
module: performance	The cuda batched GEMM has a poor performance for bigger batch size with smaller matrix size
module: performance	t.contiguous() ~10 slower in eager mode compared to torch.compile
module: performance	Training a network SUPER slow with Pytorch 2.1
module: performance	Optim.step() is significantly SLOW on MPS
module: performance	pyTorch 2.1 3x slower than 2.0
module: performance	Tensor .cuda() very slow with specific array sizes
module: performance	pytorch index_select is too slow
module: performance	Regression on CUDA 12.1 for vanilla transformer layer
module: performance	AOTAutograd perf: avoid as_strided() calls when we have intermediate bases
module: performance	Depthwise conv3d slower than normal conv3d
module: performance	Broadcasting matmul is much slower than corresponding einsum
module: performance	Skip cuda kernel launch with torch.sum when dimension length is 0
module: performance	Severe performance regression on deterministic algorithm in torch 2.0
module: performance	Performance degradation on AMD + A800 when computation is small
module: performance	Unnecessary cuda synchronizations that we should remove in PyTorch
module: performance	The CPU version of torch.cummax is slow
module: performance	Suppport Fused AdamW on CPU
module: performance	Transformer performance drop due to slow PyTorch GEMMs
module: performance	[Optimizer Perf] Improve speed of _init_group to c++
module: performance	Fake Tensor error 'lengths' argument should be a 1D CPU int64 tensor, but got 1D meta Long tensor
module: performance	[DDP PT2] TypeError: convert_frame_assert.<locals>._convert_frame_assert() missing 2 required positional arguments: 'hooks' and 'frame_state'
module: performance	Regression in text encoding
module: performance	Can't construct a tensor from List[SymFloat]
module: performance	Dynamo not handling a NamedTuple
module: performance	Optimize PyTorch C++ part with Profile-Guided Optimization (PGO)
module: performance	affine_grid and grid_sample operators merge/accelleration
module: performance	Observed regress in DataLoader spawn from PyTorch1.13 to PyTorch2.0
module: performance	Improve shape padding in training.
module: performance	Investigate the perf drop on timm for dynamic shape when layout optimization is enabled
module: performance	[RFC] Add third-party malloc library to improve pytorch memory performance on Windows
module: performance	Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118)
module: performance	Speed when installing from source is very low with CUDA 11
module: performance	einsum is about 40x slower on CUDA than manually multiplying and summing
module: performance	Lazily format C++ stack trace if it is not used
module: performance	[BUG] Poor torch.bmm performance on H100
module: performance	MPS: unique and unique_consecutive extremely slow when return_counts=True
module: performance	slow torch import on macos
module: performance	[Inductor] [CPU] Huggingface model MobileBertForQuestionAnswering performance regression > 10% on 2023-03-12 nightly release
module: performance	mkldnn matmul kernel may be slower than openblas kernel for very small tensor shapes
module: performance	CPU time performance is unstable
module: performance	Make torch.onnx.utils._optimize_graph use several CPU cores
module: performance	LSTM on CPU is significantly slower on PyTorch compared to other frameworks
module: performance	RFC: Enabling AVX512 dispatch for compute-intensive ATen ops
module: performance	Subclassed Tensors Decrease Training GPU Throughput by ~40%
module: performance	Enable Link Time Optimization in PyTorch 2.0 Release Binaries - Smaller, Faster, Better Binaries
module: performance	Why is AvgPool2D taking longer than Conv2D for the same input?
module: performance	Process get killed when running torch.combinations
module: performance	set_default_device/torch.device has performance impact for non-factory functions
module: performance	Improve make_fx tracing speed
module: performance	_pack_padded_sequence fails in dynamo due to requiring a non-fake 2nd argument
module: performance	Softmax function slows down for data with large range
module: performance	The speed of matrix inversion is relatively slow for many small matrices
module: performance	Large slow down by not calling torch.set_num_threads
module: performance	Adam (fused=True) issues
module: performance	forward-mode AD formula for torch.add (and possibly others) accidentally upcasts float32 to float64
module: performance	Libtorch's CPU inference is much slower on Windows than on Linux
module: performance	Einsum Optimization Tracker
module: performance	Speed of torch.istft
module: performance	Autocast with BF16 on CPU slows down model more than 2X
module: performance	Significantly worse MPS performance between torch 1.13.0.dev20220922 and torch 1.13.0.dev20220930
module: performance	High occupation on GPU 0 when converting Tensor to multi GPU
module: performance	very strange speed of torch.bmm with specific tensor shape
module: performance	torch::quantile performance?
module: performance	libtorch create a tensor is very slow, who can tell me why
module: performance	torch 1.12.1 cuda 10.2 runs slower than torch 1.8.2 cuda 10.2
module: performance	torch.var_mean is slower than layer norm
module: performance	Profiling results on CPU is not reliable
module: performance	Bilinear interpolation with antialiasing is slow in performance
module: performance	SparseAdam performance issue during optimizer step
module: performance	Adam not optimally implemented: unnecessary torch.div
module: performance	scripted fft Convolutions are faster than nn.Conv1d with large kernels
module: performance	Do we really need sampler for IterableDataset?
module: performance	Efficiency of unary operations on CPU for large tensors
module: performance	Performance with MPS on AMD GPUs are worse than CPU
module: performance	Adam is 30% slower than SGD on Apple Metal.
module: performance	torch.randperm uses too much cpu, but not efficient.
module: performance	ENORMOUS OVERHEAD from mp.get_context('spawn')
module: performance	Torch x += y.bmm(z) is faster than x.baddbmm_(y, z)
module: performance	Remove all docstrings when python is running in optimization mode
module: performance	[feature request] no-param sort to exploit parallelization
module: performance	torch.sort does not exploit parallelization when invoked without the dim parameter.
module: performance	[jiterator] perf regression when jiterating few ops for complex dtype
module: performance	kthvalue 20x slower than sort
module: performance	Add ZeroTensor support for mm
module: performance	[feature request] Make index_select parallel.
module: performance	[feature request] coalesce to support dim argument.
module: performance	[Proposal] Use batched oprations to accelerate PowerSGD
module: performance	Group convolution slower than manually running separate convolutions in CUDA streams
module: performance	Matrix multiplication is 30 times slower for integers than floats on CPU
module: performance	Training grouped Conv2D is slow
module: performance	Optimization: convolution_backward doesn't always need to call .contiguous on certain inputs
module: performance	Channels last performance problem
module: performance	Torch function runtime seemingly dependent on scipy call
module: performance	Port MarginRankingLoss to TensorIterator
module: performance	Performance improvement in Autograd Forward AD using ZeroTensors
module: performance	TestProfilerCUDA. test_mem_leak failing for CUDA 11.5 on Linux
module: performance	RFC: Deprecate Bottleneck
module: performance	vmap performance warnings from jacobian
module: performance	'replicate' padding in convolution is 77 times slower on cpu than 'zeros'
module: performance	[RFC] APEX style fused optimizers in PyTorch
module: performance	Conv2d kernel performance regression on CPU since PyTorch 1.9
module: performance	Importing numpy interacts with tensor.sum perf
module: performance	Bug? :Run torch.unique twice get different running time?
module: performance	Significantly difference in execution time when convolution is run as nn.Conv2d and as nn.Sequential
module: performance	Use __slots__ for the nn.Module class
module: performance	loading large model not finished after 16 hours
module: performance	Latency issue with torch.sin
module: performance	Performance problems of eigh operator on CPU
module: performance	at::parallel_for created max_threads for inputs larger than GRAIN_SIZE
module: performance	[Bug] [CUDA IPC] CUDA IPC memory cost
module: performance	Quantile is limited to 16 million elements and have poor performance.
module: performance	Memory leak in multi-thread inference
module: performance	Some linalg operations are not taking advantage from batched computation
module: performance	Direct inversion and linear systems solutions for small matrices
module: performance	More efficient colon backwards in advanced indexing
module: performance	torch median / nanmedian w/ nans speed
module: performance	Automatic mixed precision works worse for 3D neural networks.
module: performance	Bad performance of stock model on Windows compared to Linux
module: performance	Pytorch 1.5+ is slower than pytorch 1.3
module: performance	bilinear interpolate is very slow under mixed precision training mode.
module: performance	PyTorch unfold could be faster
module: performance	Optimize torch.einsum
module: performance	Quantized conv2d with dilation and groups much slower than float32
module: performance	cuda streams run sequentially, expected to run parallel
module: performance	Foreach Functions Tracking Issue
module: performance	Transferring tensor to the gpu and converting dtype in a single call to .to() is slower than first transferring, then converting.
module: performance	After the Adam optimizer used weight_decay, the model became extremely slow when tested on the CPU.(Time from 7 seconds to 46 seconds)
module: performance	scatter_add_ 6000-times slower with int64 compared to int32
module: performance	TrilinearBackward takes 98.4% of total computational time, is this to be expected?
module: performance	[perf] 10x improvement on element-wise operations with manual broadcast
module: performance	[perf] 10x improvement when doing x.sum(-1) manually
module: performance	einsum "jk,ijkl->il" is ~16x slower than numpy
module: performance	masked_select is x3 slower than reshaping and index_select
module: performance	Batched SVD_LOWRANK being much slower than loop implementation (both CPU and GPU)
module: performance	[question] Influence of divisibility of B, C, T by 16 on Conv1d (and Conv2d perf) with CuDNN, including presence of padding
module: performance	Performance debugging / warning mode
module: performance	Small model occupies too much GPU in CUDA11.1 + Torch1.8.1 but is normal in Torch 1.6 + CUDA10.1
module: performance	Cusolver handle may decrease MAGMA performance on GPU
module: performance	High CPU using torch.stack/torch.cat on Windows
module: performance	Structured kernels have increased TensorIterator overhead.
module: performance	Optimizations to TORCH_CHECK change inlining behavior.
module: performance	Regression in Python arg parser performance.
module: performance	Dispatcher TLS to bypass loads of dispatch key from tensor arguments
module: performance	It is strange that PyTorch is slow on RTX 3090
module: performance	Very slow backward speed when using gather with small-range indices
module: performance	Relative performance of histc vs bincount
module: performance	group conv in amp too slower
module: performance	Discrepancy between CPU->GPU and GPU->CPU data transfer speeds
module: performance	Optimize einsum in TorchScript profile guided optimization
module: performance	version1.7.0 is ~1.3x slower than 1.4.0 on ResNet18
module: performance	RTX 2080s performs better than RTX 3080 in Semantic Segmentation inference process(Libtorch,win10),why?
module: performance	[nnc][perf] Performance decrease with CPU fusion on freeze(script(pytorch_mobilenet_v3))
module: performance	Build using Py_LIMITED_API and then build wheels with the stable ABI abi3 tag
module: performance	device (+dtype) arguments for torch.stack / torch.cat
module: performance	CPU eval BatchNorm2d is not threaded
module: performance	Performance bugs of transpose2d on A100
module: performance	Massive Performance bottlenecks in some of the Reduce operations.
module: performance	Add branch predictor hints to prefer Context::deterministicAlgorithms() == false
module: performance	torch.median slower than torch.sort on cpu
module: performance	max_pool2d CPU forward performance is poor
module: performance	DataParallel copies the model onto GPUs sequentially
module: performance	Class-based structured kernels instruction count regression
module: performance	CuDNN 8 with benchmark=True takes minutes to execute for certain configurations
module: performance	DataLoader is slow in spawned processes
module: performance	[TorchScript Performance Deep-dive] problems discovered in TS performance deep-dive
module: performance	Training slowdown from 1.6 to 1.7.1
module: performance	Channels last doesn't improve speed when using SyncBatchNorm
module: performance	for CNN in fp16 execution time depends on input scale
module: performance	RTX3090 performs no better than 1080ti
module: performance	Magma functions that don't have queue argument create cublas handles for each call
module: performance	channels_last format convolution is slower than normal NCHW
module: performance	A significant overhead when running fastrnns with autograd.profiler
module: performance	Caching for autoregressive decoding of Transformer
module: performance	c10::scalar_to_tensor(...) uses should be audited for performance and type promotion impact
module: performance	clip_grad_norm_ performance regression
module: performance	Investigate torch.linalg.norm performance
module: performance	RTX 3090 setup vs 2x RTX 2080TI setup slower? Help..
module: performance	Pytorch streams API don't execute concurrently, However Same code in CUDA does.
module: performance	torch.eye(d) is slow and hogs cpu for d >= 182
module: performance	nn.functional.interpolate backward in fp16 is extremely slow
module: performance	Linear algebra GPU backend tracking issue [magma/cusolver/cublas]
module: performance	Code becomes more than x20 slower after upgrading torch version
module: performance	The speed of pytorch with cudatoolkit 11.0 is slower than cudatoolkit 10.2
module: performance	Eliminate redundant device guards in generic dispatch key kernel wrappers
module: performance	GPU acceleration for Apple's M1 chip?
module: performance	Training slows down and memory usage increases when upgrading from PyTorch 1.6 to 1.7
module: performance	resize_(0) is very expensive
module: performance	Why is RTX3080 slower than RTX2020-Ti?
module: performance	Training fast with small dataset, slow with large dataset
module: performance	Mention accessor/data_ptr for raw memory access in Libtorch index API document and discuss performance implications
module: performance	Median / quantile / mode / rank / percentile pooling
module: performance	In pytorch 1.6。Run model with input no contiguous tensor will become very slow.
module: performance	[Feature] Fused Matmul & Min/Max/Sum/Prod
module: performance	FP16 inference latency after sleeping
module: performance	Slower speeds when using half().
module: performance	CUDA memory leak in multi-processing
module: performance	torch.cuda.synchronize Influence distributed training
module: performance	AMP much worse performance with groupped Conv2d than fp32
module: performance	Inference performance regression caused by hacky_wrapper_for_legacy_signatures
module: performance	Accessing tensor by element is super slow
module: performance	[feature request] Faster specialized int16->float32 conversions to match speed with NumPy
module: performance	[quant] Quantized AdaptivePool3d is much slower for ChannelsLast3d.
module: performance	Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D
module: performance	slow data loading in VisionDataset  - need to allow batch loading.
module: performance	Reading data speed slower than tensorflow
module: performance	Slow index_add_ on torch.long tensors
module: performance	Difference in inference time between CUDA 10.0 & 10.2
module: performance	Implement LSH Optimizations for Enhanced CPU-Only Performance
module: performance	Bottleneck when publishing the model using flask about 3 times slower.
module: performance	Large overhead (7 microseconds) for PyTorch operation
module: dataloader	Address huge memory usage on large dataset
module: dataloader	WeightedRandomSampler doesnt keep expected ratio betwen samples
module: dataloader	NotImplementedError: Cannot access storage of SparseCsrTensorImpl
module: dataloader	[Help Needed] [DataParallel/DataLoader/AMP] Severe slowdown on large dataset with fp16 precision and DataParallel wrapper
module: dataloader	New feature: Balanced Sampler
module: dataloader	Segmentation fault in RPC worker when DataLoader has num_workers > 0
module: dataloader	Multiprocess. DataLoader worker  is killed by signal: Segmentation fault.
module: dataloader	"Expected a 'mps:0' generator device but found 'cpu'" using shuffle=True on DataLoader
module: dataloader	Add _worker_end_fn_t to the DataLoader
module: dataloader	Dataloader resetting with num_workers=1 and persistent_workers=True
module: dataloader	RuntimeError: DataLoader worker (pid 11011) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit
module: dataloader	"file_descriptor" multiprocessing sharing strategy works incorrectly in dataloading
module: dataloader	Determinism by using datapipes shuffle
module: dataloader	Overly strict type hints for torch.utils.data.random_split
module: dataloader	Dataloader extremely slow on in-memory datasets
module: dataloader	Confusing error message for DataLoader with num_workers=0 and non-zero timeout
module: dataloader	Dataset  with Queue issue
module: dataloader	inconsistent signature for dataloader in docs/source/data.rst
module: dataloader	Pytorch dataloader not loading first-available data with multiple workers
module: dataloader	Faster BatchSampler with big batch size
module: dataloader	Issue with ShufflerIterDataPipe in torch 1.13.1
module: dataloader	Dataloader multiprocess loading with num_worker > 0 calls __main__ file to run
module: dataloader	Training Faster R-CNN model with COCO dataset has been consistently unsuccessful.
module: dataloader	DataLoader doesn't accept non-cpu device for loading.
module: dataloader	DataLoader with collate_fn that returns tensors in GPU memory raises warnings when deleted
module: dataloader	Function Registry for extending collate_fn
module: dataloader	suspicious memory leak when increase DataLoader's prefetch_factor and enable pin_memory
module: dataloader	Dataloader should kill & restart workers when timeout is hit
module: dataloader	Add support for __collate__ attrib on dataset elements in default_collate
module: dataloader	NCCL backend can't be used with a dataset that is IterDataPipe
module: dataloader	faster WeightedRandomSampler implementation based on alias method
module: dataloader	large number of temporary files generated when using dataloader with num_workers>0
module: dataloader	jit.fork stalls multiprocessing dataloader
module: dataloader	[Bug][Dataloader] unable to mmap 2048 bytes from file <filename not specified>: Cannot allocate memory (12)
module: dataloader	Perf reduction due to munmap with dataloader pinning thread ?
module: dataloader	Open file leak when dataloader is using persistent_workers and pin_memory AND you create multiple dataloaders.
module: dataloader	Segmentation faults in DataLoader (in latest torch version).
module: dataloader	multi-node distributed training rank0 hang at dataloader after a few epochs
module: dataloader	AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'
module: dataloader	Multiprocessing DataLoader pickles multiprocessing.Queues incorrectly
module: dataloader	Timed out receiving the shared seed from the distribtued store on Rank 2
module: dataloader	DISABLED test_random_seed (__main__.TestDataLoaderUtils)
module: dataloader	DataLoader parameter pin_memory_device should accept torch.device type
module: dataloader	[feature request] DataLoader to accept num_threads argument to auto-set number of threads for OpenMP / intra-op parallelism
module: dataloader	DataLoader: pin_memory should respect object attributes before object collection type
module: dataloader	Using DDP with num_workers > 0 hangs before entering the first training epoch loop
module: dataloader	num_worker and prefetch_factor in DataLoader do not scale
module: dataloader	RuntimeError: DataLoader worker (pid 22822) is killed by signal: Aborted.
module: dataloader	[feature request] Add support for a custom DatasetFetcher in DataLoader
module: dataloader	DataLoader leaking resources?
module: dataloader	If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac
module: dataloader	Do we really need sampler for IterableDataset?
module: dataloader	Three memory copies of every dataloader cpu tensor
module: dataloader	Werror=nonnull in dataloader.cpp (part of tests)
module: dataloader	A somewhat cryptic error message (for newcomers) - "Cannot re-initialize CUDA in forked subprocess" - report and suggestion for a possible solution
module: dataloader	Multiprocessing DataLoader hangs on exception inside iterator when using a simple queue and a producer thread
module: dataloader	Bug in dataloader iterator found by mypy
module: dataloader	Dataloader hangs. Potential deadlock with set_num_threads in worker processes?
module: dataloader	Store SourceDataset in MapDataset using pointer
module: dataloader	[CTA] Let's Stamp Out Flaky Tests!
module: dataloader	None returned from data loader causes debugging difficulty from collate function
module: dataloader	MaybeEncodingError: Error sending result
module: dataloader	pin_memory hangs instead of throwing
module: dataloader	DistributedDataParallel creates too many threads
module: dataloader	Clarify the behavior of DataLoader sampler and batch_sampler parameters
module: dataloader	DataLoader tests are quite flaky
module: dataloader	AT_ASSERT fail with DataLoaderOptions().drop_last()
module: dataloader	pin_memory *still* destroys custom containers
module: dataloader	_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training
module: dataloader	I want to know how to read the LMDB file once when using DDP
module: dataloader	How to implement bucket_by_sequence_length with IterableDataset and DataLoader
module: dataloader	torch.utils.data.Sampler is not recognized as a collections.abc.Sized
module: dataloader	DISABLED test_ind_worker_queue (__main__.TestIndividualWorkerQueue)
module: dataloader	dataloader will miss batch data when num worker>0
module: dataloader	[feature request] Add no-op set_epoch on Sampler class
module: dataloader	Exception in thread when using dataloader
module: dataloader	BAR1 memory of GPU is not released when main process is killed.
module: dataloader	Should the drop_last parameter of Dataloader be mutually exclusive with the batch samplers?
module: dataloader	Data Loader tests hang when run in ASAN test job
module: dataloader	libtorch: collate_fn equivalent
module: dataloader	Pipe method for torch tensors
module: dataloader	Add function that retrieves a batch from a DataLoader
module: dataloader	Multi-processing leaking file descriptors
module: dataloader	RandomSampler / DistributedSampler does not seem really random
module: dataloader	Add support for GPU based data loading and transformation
module: dataloader	Ability to explicitly close/dispose a DataLoader
module: dataloader	Type of first constructor parameter of ElasticDistributedSampler and DistributedSampler
module: dataloader	Memory Leak Found in Persistent DataLoader
module: dataloader	Default Collate doesn't work for subtypes of ndarray
module: dataloader	Pytorch model load failure in Gunicorn with Gevent workers
module: dataloader	UCI Data Sets
module: dataloader	using DistributedSampler   occur RuntimeError: Expected a 'cuda:0' generator device but found 'cpu'
module: dataloader	DataLoader with IterativeDataset throws an error when providing a BatchSampler
module: dataloader	DataLoader worker (pid(s) 18056, 20540, 4512) exited unexpectedly
module: dataloader	Dataloader Rerunning with num_workers=0 may give better error trace
module: dataloader	GPU 0 context created on GPU 1 worker when using pin_memory=True
module: dataloader	DataLoader performance drop on 4-channel images?
module: dataloader	Not possible to save dataloader in C++
module: dataloader	Segmentation fault in PyTorch dataloader
module: dataloader	[Feature Request] Optionally specify batch dimension in DataLoader and collate_fn
module: dataloader	Should we remind users not to use the dataset  on GPU when it's the argument of DataLoader?
module: dataloader	How to skip the images in a custom dataset and deal with None values?
module: dataloader	Torch DataLoader fails to reraise error from sqlalchemy.
module: dataloader	Pytorch num_worker>0 code worked first time and then it never worked with same setting again
module: dataloader	DataLoader is slow in spawned processes
module: dataloader	SequentialSampler getting slower as time passing by
module: dataloader	dataloader bug need help
module: dataloader	Dataloader Prefetch data to GPU by cudaMemPrefetchAsync
module: dataloader	RandomSampler is very slow with huge dataset
module: dataloader	Excessive memory usage caused by Samplers storing lists of indices
module: dataloader	[dataloader] RuntimeError: Too many open files when yielding integers
module: dataloader	RandomSampler generator created in every iteration
module: dataloader	[RFC] DataLoader architecture updates and TarDataset implementation
module: dataloader	torch.utils.data.DistributedSampler allow uneven inputs
module: dataloader	provide example for distributed training with iterative dataloaders
module: dataloader	pin_memory=True in DataLoader converts a tuple to list automatically
module: dataloader	[dataloader] Worker threads to print the signal they received before they die
module: dataloader	Handling multiple large-scale datasets efficiently
module: dataloader	Change "_next_index()" in DataLoader to a public and stable API
module: dataloader	DataLoader gives "Broken pipe" error on Linux platform
module: dataloader	[Request] Batched Dataset->DataLoader interface
module: dataloader	Custom ops get stuck in multiprocess data loader under certain environments
module: dataloader	BucketSampler for easy variable-length input batching
module: dataloader	TestDataLoader.test_proper_exit takes 2.5min to finish
module: dataloader	Low shared memory
module: dataloader	DataLoader with cv2 and some numpy/cv2 import order causes workers to not work
module: dataloader	Multi-process Dataloader and multi-parameter exceptions
module: dataloader	DataLoader consumes extremely large shared memory (shm) in its initialization.
module: dataloader	Useless Exception traces when DataSet timing out
module: dataloader	Data loader struct pack issue(overflow)?
module: dataloader	Implement map-style caching DataSet as PyTorch build-in DataSet.
module: dataloader	RuntimeError: each element in list of batch should be of equal size
module: dataloader	slow data loading in VisionDataset  - need to allow batch loading.
module: dataloader	Reading data speed slower than tensorflow
module: dataloader	Reference the randomness issue in DataLoader & Dataset documentation.
module: dataloader	[RFC, Tracker] DataLoader improvements
module: dataloader	len of dataloader when using iterable dataset does not reflect batch size
module: dataloader	Cannot re-initialize CUDA in forked subprocess
module: dataloader	Dataloader._shutdown_workers hangs
module: dataloader	[RFC] Add tar-based IterableDataset implementation to PyTorch
module: dataloader	Compatibility of subset dataset with disabled batch sampling
module: dataloader	state_dict and load_state_dict methods for DataLoader and Sampler to continue training at specific epoch and batch
module: dataloader	how to do 3d data augmentation in parallel on the gpu?
module: dataloader	Caching Support for class Dataset
module: dataloader	ExceptionWrapper cannot handle exceptions with more than one argument
module: dataloader	IterableDataset with num_workers > 0 and drop_last=True drops more instances than expected
module: dataloader	Training got stuck due to timeout from dataloader
module: dataloader	[feature request] [dataloader] Introduce Dataset.__collate__
module: dataloader	Silent failing of batch_sampler when the data points are lists of tensors.
module: dataloader	pin_memory may change the type of instance returned by collate_fn.
module: dataloader	Extend ConcatDataset to return dataset index
module: dataloader	The model training time is increasing between runs if the same DataLoader reused to train multiple models.
module: dataloader	DataLoader: Segmentation Fault (core dumped)
module: dataloader	DataLoader does not consider default floating point type
module: dataloader	Use of Sequence collections for abstract classes in Dataset
module: dataloader	Default shuffle behavior of DistributedSampler
module: dataloader	c10:Error: could not unlink the shared memory file
module: dataloader	Parallel data loader performance degradation for IterableDataset with num_workers > 1 (but not for Dataset).
module: dataloader	ConnectionResetError when using dataLoader with pin_memory=True
module: dataloader	Sampler for IterableDataset
module: dataloader	[feature request] [dataloader] Pad variable-sized tensors in default_collate
module: dataloader	[dataloader] Sampler abstract constructor API minor proposal
module: dataloader	Batched Dataloader
module: dataloader	DataLoader workers fail to die
module: dataloader	Custom sampler for Seq2Seq models to avoid padding
module: dataloader	[dataloader] Hang because of too many open files (and probably some process dead)
module: dataloader	[dataloader] Problem in exception reraise mechanism
module: dataloader	torch.as_tensor(bytearray(...)) seems to leak memory
module: dataloader	Error in python3: double free or corruption (fasttop)
module: dataloader	Incorrect Validation Accuracy Due to Distributed Sampler
module: dataloader	Data worker should fetch a sample instead of a batch.
module: dataloader	DataLoader slow down when pin_memory=False
module: dataloader	Shared Dataset Functionality
module: dataloader	segmentation faults when using multiprocessing_context='spawn' with large number of processes
module: dataloader	[Feature request] Let DistributedSampler take a Sampler as input
module: dataloader	Add automatic tuning flags to utils.data.dataloader
module: dataloader	[data loader] Graceful data loader threads exit on KeyboardInterrupt
module: dataloader	[dataloader] Mysterious error when using spawn start_method
module: dataloader	[dataloader] SIGCHLD handler should poll the queue for exception first
module: dataloader	Batch Dataloader and Dataset
module: dataloader	Pytorch hangs when dataloader multiprocessing workers are killed
module: dataloader	[utils.bottleneck] throws initialization error for cuda profiling
module: dataloader	[Proposal] Data reading framework for PyTorch (Hive, MySQL, S3 etc.)
module: dataloader	Dataloader's memory usage keeps increasing during one single epoch.
module: dataloader	Class based Sampler for Class Incremental/Continual Learning research
module: dataloader	Add a RandomBatchSampler ?
module: dataloader	collate_fn returns subclass of torch.Tensor, but DataLoader transforms back to torch.Tensor
module: dataloader	Issue with dataloader using pin_memory = True
module: dataloader	DataLoader with option to re-use worker processes
module: dataloader	The speed of scatter is influenced by the data size while using nn.DataParallel
module: dataloader	DataLoader num_workers > 0 causes CPU memory from parent process to be replicated in all worker processes
module: dataloader	Generalized Data Class
module: dataloader	Python dataloader Improvements
module: dataloader	Move collate_fn functionality / responsibility into Dataset object
module: dataloader	set num_workers on the dataloader make the jupyter kernel crash at the almost end of the epoch
module: dataloader	Semaphore leaks in dataloader
module: dataloader	DataLoader: Could not wrapper a exception in threads
module: dataloader	Suggest: DataLoader add device parameter
module: dataloader	non-shuffling data loaders can affect random states, thus the results of shuffling data loaders.
module: dataloader	[feature request] batch_first option in torch.utils.data
module: dataloader	torch.utils.data.random_split() returns dataset index as tensor
module: dataloader	dataloader stuck at sched_yield =0
module: dataloader	[feature request] Rename Subset -> Resample to reflect wider use
module: dataloader	Mismatch in behaviour of WeightedRandomSampler and other samplers
module: dataloader	Multi queue for dataloader when workers > 1
module: dataloader	[feature request] [PyTorch] Dynamic Samplers.
module: dataloader	worker assignments in torch.utils.dataloader.py
module: dataloader	[feature request] Stratified splits in random_split function
module: dataloader	Speed up data loading for TensorDataset if the underlying dataset supports index by a list of indices
module: dataloader	Data sampling seems to be more complicated than necessary
module: dataloader	DataLoader "casting" non statndard objects to lists
module: dataloader	DataLoader gets stuck after model initialization
module: dataloader	DataLoader converts cuda FloatTensor into cpu DoubleTensor when shape is (n,)
module: dataloader	Hard-negative mining using __getitem__ directive in Dataset class
module: dataloader	[feature request] Support tensors of different sizes as batch elements in DataLoader
module: dataloader	dataloader parallels over elements vs over batches
module: ci	CI fails due to missing lxml for Python-3.9
module: ci	Linux runners occasionally dies with out-of-disk space, often during distributed tests
module: ci	GPU tests can fail with invalid memory access due to compiler generating invalid code
module: ci	test consistently failing locally, but it doesn't fail in CI
module: ci	Clean up AWS credentials from PyTorch CI and replace them with OIDC
module: ci	[CI] Should test_public_bindings be included in CI by default?
module: ci	[CI] Lintrunner takes 60+ min
module: ci	[RFC] Add Intel GPU support into PyTorch CI/CD
module: ci	[RFC] macOS x86 builds / test deprecation
module: ci	inductor/test_aot_inductor.py gets segfault but test ultimately passes
module: ci	Tests modify global state cause later tests to fail
module: ci	Sourceforge outage causing multiple CI failures
module: ci	FlashAttentionV2 will OOM when building on ci/cd with default settings
module: ci	Add caffe2 ideep/onednn tests to OSS CI
module: ci	Conda configuration shouldn't pollute $PATH variable
module: ci	Branch name in double quotes ""
module: ci	DISABLED test_cuda_assert_should_not_stop_common_distributed_test_suite_cuda (__main__.TestTestingCUDA)
module: ci	Many tests in test/dynamo fail if run in the context of just 'pytest test/dynamo'
module: ci	ghstack + mergebot race condition
module: ci	Potential lack of CI testing on older NVIDIA GPU
module: ci	Dynamo test pipeline failed on MaxPool2d test when changed to use f-string
module: ci	Multiple linux jobs are failing with version `GLIBCXX_3.4.30' not found
module: ci	MacOS arm64 runners are not available in CI
module: ci	PyTorch built with CuDNN-8.8.1 crashes if CuDNN-8.9.2 is installed on the system
module: ci	Re-enable test_typing
module: ci	Long PR description leads to "Argument list too long" error from docker
module: ci	PyTorch should not use windows.8xlarge.nvidia.gpu to test binary builds
module: ci	[cuda] Switching CI to CUDA 12.1 timing out linux-bionic-cuda12.1-py3.10-gcc7 / test (distributed, 3, 3, linux.8xlarge.nvidia.gpu)
module: ci	Migrate windows runners to non-ephemeral instances
module: ci	Fork run CI from upstream remote (more than 10,000 emails)
module: ci	Use a label instead of body text for merge blocking CI SEVs
module: ci	add -std=c++20 build-only CI job
module: ci	CI for s390x
module: ci	Need better error message when a merge cancelled because of timeout
module: ci	Nightly conda binaries failed to pass tests since 2023-03-17
module: ci	Not allow force merge when lint fails and not because of broken trunk
module: ci	Create a new Docker image with all inductor benchmarks and pre-trained models downloaded
module: ci	A100 Perf Job artifact zipfiles unzip to generic folder that loses job information
module: ci	Investigate queue disparity between windows.4xlarge and linux.4xlarge
module: ci	[CI]  PyTorch Windows Test AMIs contains CUDA-11.3 installation
module: ci	"multi device" tests get skipped in standard CI
module: ci	Cross-compiled libtorch Windows Arm64 binaries
module: ci	M1 runner i-090e1df32b6f48a20 run out of disk space
module: ci	test/test_ops.py is segfaulting on master build with DEBUG assets
module: ci	Consolidate binary build matrix for core and validation workflows
module: ci	GitHub first-time contributors box pops up unexpectedly
module: ci	Flaky dynamo test_indexing flaky with SIGKILL
module: ci	Unit test with --subprocess command doesn't respect the -k filter flag and runs all available sub tests
module: ci	ASAN shard 4 started to OOM after unrelated commit
module: ci	"No CUDA GPUs are available" coming from GHA g5 runners
module: ci	Libtorch windows binaries publishing
module: ci	Synchronize domain builds to be executed after core build have completed
module: ci	NVFuser FusionRootMappingMultipleBroadcast_CUDA raises exception on sm_80+
module: ci	NVFuser FusionComputeAtMultiBCast_CUDA and FusionDetectSelfMappedDomains_CUDA does not raise exception on sm_80+
module: ci	Move functorch tests from functorch/test/* to test/*; delete functorch CI configs
module: ci	Debuggability++: Share instructions for building exotic CI configurations
module: ci	CUDA OOM issue when running tests in CI
module: ci	Setup ssh sometimes fail
module: ci	Execute smoke test for Better Transformer feature
module: ci	Have NVIDIA driver and other related dependencies as part of the Linux AMI
module: ci	Re-Running PR Sanity Check after Adding skip-pr-sanity-checks Label Still Fails
module: ci	Set up tests to run periodically and surface them on HUD
module: ci	would you like  upload to the cpp libtorch to  vcpkg  package repo?
module: ci	CUDA 11.6 linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck failure
module: ci	linux-bionic-cuda10.2-py3.9-gcc7  multigpu test are broken
module: ci	Linux cuda-11.x binary build  jobs intermittently take more than 4 hours
module: ci	PyTorch EC2 runners can not be used with standard actions
module: ci	Are PyTorch Android nightly builds getting automatically published
module: ci	Add torch nightly builds pipeline for aarch64 linux
module: ci	Pytorch/Nova CI should monitor service outages for major dependencies
module: ci	distributed tests take a long time
module: ci	Find way to add comments to merge_rules json
module: ci	Test public bindings in CI gives weird output on error
module: ci	Redirect the old metrics.pytorch.org url to the new page
module: ci	[CI] Create periodic fuzzy testing for PyTorch build flags
module: ci	[CI] Split up periodic.yml into forward-fixable.yml and periodic.yml
module: ci	functorch slow tests not being run in slow CI
module: ci	linalg and lu tests fail when run in parallel on linux cuda
module: ci	Automating release process - Binary validation, Automatically generating get started page
module: ci	Offer a way to really force merges via pytorchbot
module: ci	[Reproducibility] Make tests say when unusual environment variables are set that change behavior of the test
module: ci	Re-enable DynamicQuantModule in iOS simulator tests
module: ci	[PyTorch/XLA] Improve the XLA PR landing process
module: ci	backwards compatibility ALLOWLIST is misused
module: ci	Investigate adding shell linter/checker to CI
module: ci	Investigate adding Dockerfile linter hadolint to CI
module: ci	Workflows fail silently when the workflow file is invalid
module: ci	TestTagsCPU.test_tags__refs_constant_pad_nd_cpu_float32 flaky with dynamo & pytest
module: ci	Docker updates cause subsequent builds to fail
module: ci	CI: Run cpu tests in parallel processes?
module: ci	[Releng] Improve the tutorials release process
module: ci	slow test infra cannot handle nested suites
module: ci	test_conv_backend tests OOMing in 10.2 slow_gradcheck CI
module: ci	Allow a user provided "test name - test time" mapping file work with pytorch's test sharding mechanism
module: ci	[CI] Do we run all cpp tests on CI?
module: ci	Modify update-viable-strict GHA to use internal version of checkout
module: ci	Write lint for isGreen
module: ci	Improve clarity by making sharding a static nightly update
module: ci	android-tests is often flaky
module: ci	[META] Sign up to discuss significantly modifying CI
module: ci	Mismatch in clang toolchain lead to binary incompatibilities on M1 between torch and torchvision
module: ci	CI workflow creates too many tags in RSS feed
module: ci	Debug job does not build in debug mode
module: ci	[CI] Detect when tests are no longer running from CI
module: ci	Disable issue doesn't disable multiple dtypes correctly
module: ci	Display EC2 information
module: ci	Windows CUDA TTS tracking task
module: ci	A bug in instructions for building PyTorch with ASAN
module: ci	Let's host NVIDIA dependencies in our own S3
module: ci	Improve sharding algorithm for ASAN (any maybe other jobs as well)
module: ci	Gradients tests are very time consuming
module: ci	Move torchbench workflow to workflow_dispatch?
module: ci	Create secure credential storage for metrics credentials and associated documentation on how to regenerate them if needed
module: ci	CI: Bake as many dependencies as we can in the AMI (windows)
module: ci	Mention docker build process in RELEASE.md and automate building those for release
module: ci	Improve test_test_history.py
module: ci	Why there are 8 flavors of iOS build jobs for every commit
module: ci	Standardize Naming for Workflows/Jobs
module: ci	pytorchmergebot doesn't react to comments left from "files" tab
module: ci	Pin dependencies + expand the current linter
module: ci	Clarify test dependencies (e.g., into a test-requirements.txt file)
module: ci	Enforce quotas on CI users
module: ci	test_fn_fwgrad_bwgrad_[trapezoid|trapz]_cuda_complex128 causes CUDA memory exception
module: ci	Update MKL version used with MAGMA
module: ci	Push to fork failed with cryptic "refusing to allow a Personal Access Token to create or update workflow .github/workflows/run_torchbench.yml without workflow scope"
module: ci	a lot nightly builds are canceled due to VM errors since Jan14
module: ci	clang format hash mismatched for linux64
module: ci	Could we leave the _two_ most recent nightlies in the conda channel?
module: ci	Multigpu test configs intermittently timeout
module: ci	TestProfilerCUDA. test_mem_leak failing for CUDA 11.5 on Linux
module: ci	[cpp extension] making it possible to parallelize the building process
module: ci	clang-tidy shows false positive failure on PRs that rename files
module: ci	[rfc] Target Determinator
module: ci	pytorch/ExampleRepo
module: ci	Decorate tests with a "deadline"
module: ci	Modify Dr. CI so it could detect runner disconnection failures
module: ci	Feature request: FFT operations on Metal
module: ci	RFC: Create unified CI experience for pytorch and domain libraries
module: ci	Migrate current Windows CI scripts off of batch
module: ci	Add at least one config running GPU testing on sm_80+ cards
module: ci	BC CI error message should link to some information about how to squash the warning
module: ci	CMake outputs lots of warnings in CI
module: ci	linalg.lstsq out variant fails internal assert because it uses non-inplace view op for some inputs
module: ci	[clang-tidy] Errors aren't reported, runner fails if no ranges are found
module: ci	Execute C++ based GTest unit tests from a python wrapper
module: ci	Add a GitHub actions workflow for Macos
module: ci	libtorch on Apple m1
module: ci	Automate bumping of clang-tidy docker image tag on CI
module: ci	Improvement to CUDA mem leak check
module: ci	[Meta] PyTorch features build/test matrix
module: ci	Attempt to use jited torch.isnan hit internal assert
module: ci	CI not surfacing some failures (-Werror?) on PR
module: ci	[tests] cumprod OpInfo tests take long time to run (around 1min)
module: ci	Some Numba tests are failing
module: ci	enable parallel test execution for GPU tests
module: ci	There should be a CI build with OpenBLAS in addition to MKL
module: ci	Create CI test checker to ensure # of test ran is the same as # of test reported in xmlrunner
module: ci	TestVectorizedMemoryAccess.CopyKernel starting to fail after driver to 460.39
module: ci	Turn deprecation warnings into errors in CI
module: ci	Uninitialized variable was not detected in ASAN CI config
module: ci	Pytorch 1.7.0 with cuda 11.1.1 and cudnn 8.0.5
module: ci	[RFC] Add test execution time analysis CI workflow
module: ci	[chore] Test pybind11 2.6.0 (RC 2?)
module: ci	TestXNNPACKConv1dTransformPass.test_conv1d_with_relu_fc takes 2+ min to finsh
module: ci	TestDataLoader.test_proper_exit takes 2.5min to finish
module: ci	When one distributed test fails in CI, the next one can fail spuriously
module: ci	[Macos][CircleCI] Test failed in test_dataloader.py
module: ci	Python autograd engine threads never terminate in Python 3.5-3.8
module: ci	CUDA sources are not cached with sccache
module: ci	[RFC] Don't install CI dependencies in build scripts, install them in underlying docker images
module: ci	llvmlite version issue when upgrading CI docker image to python 3.8
module: ci	Improve visibility in test suite timings
module: ci	Add a CI configuration to test USE_DISTRIBUTED=0
module: ci	Memory leak issue still exists in CI
module: ci	Add 32-bit CI (e.g., Raspberry PI CI)
module: ci	pytorch_linux_xenial_cuda10_1_cudnn7_py3_slow_test triggered on all PRs
module: ci	CI test should use PR commit instead of pulling the latest master
module: ci	backward_compatibility_check_test doesn't play well with reverts
module: ci	Re-enable test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda
module: ci	Tests for pytorch_macos_10_13_cuda9_2_cudnn7_py3_build fail
module: ci	clang-tidy job merges with master which can lead to hard to understand errors
module: ci	CI Standardization for Domain APIs
module: ci	Pin flake8 version in CI
module: ci	Refactor CircleCI config for version 2.1
module: ci	Error out during compilation if USE_FBGEMM=1 is ignored
module: ci	We should run clang-tidy on all of master
module: ci	Make it easier to bisect on PyTorch
module: ci	No continuous integration coverage for Python 2 CUDA
module: ci	Stop using "AAA" prefix for builds
module: ci	Split libtorch binary build CI job into separate variants
module: ci	Change devtoolset7 CUDA 9.0 nightlies to use a lower devtoolset
module: ci	CI with >8G CUDA memory
module: ci	Add build tests for feature environment vars
module: ci	download_mnist.py causes flaky tests
module: ci	Tests on CI are not printing exceptions as they occur
module: ci	Code review on .circleci/
module: ci	Be able to use "@pytorchbot retest this please" to re-run both CircleCI and Jenkins jobs
module: ci	Proposal for build system under many system configuration testing
module: ci	Test OpenCV4 in CI
module: ci	CI: Flaky download from download.pytorch.org
module: ci	arm64 port for PyTorch, libtorch
module: ci	Tests that download from internet should retry on failure
module: ci	Test that (cd build && ninja) immediately after build is no-op in CI
module: ci	Assert that some tests must not be skipped under certain CI configurations
module: fsdp	RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype
module: fsdp	FSDP FULL_SHARD Avoid Eviction for Final Layer
module: fsdp	Custom Process Group for Each Module in FSDP
module: fsdp	Per-Parameter-Sharding FSDP Tracker
module: fsdp	Tracing per-param sharding FSDP: User Defined Objects as inputs to HOPs (autograd.Function, specifically)
module: fsdp	Tracing per-param sharding FSDP: Tensor keys in dicts over-rely on specialized_value, potentially soundness bug
module: fsdp	Tracing per-param sharding FSDP: RemovableHandle -> RemovableHandleVariable
module: fsdp	Tracing per-param sharding FSDP: Streams, Stream reconstruction
module: fsdp	Tracing per-param sharding FSDP
module: fsdp	[FSDP] Raise error when applying FSDP to nn.ModuleList or nn.ModuleDict
module: fsdp	device_mesh documentation in FSDP ctor
module: fsdp	Failure to resume from a normal (non-FSDP) checkpoint due to the optimizer state dict rekey
module: fsdp	Docs: Fixes for FDSP Files
module: fsdp	FSDP.forward() fails "_is_root should not have been set" error after saving a distributed checkpoint
module: fsdp	Fixed docstring errors in _common_utils.py, _optim_utils.py, _wrap_utils.py, _unshard_param_utils.py, _fsdp_extensions.py, api.py, _debug_utils.py, _utils.py, wrap.py, sharded_grad_scaler.py
module: fsdp	inference_mode before training results in FSDP AssertionError
module: fsdp	FSDP does not move modules without parameters to device
module: fsdp	FSDP requires global device context
module: fsdp	[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.
module: fsdp	Implement clip grad value on FSDP
module: fsdp	FSDP load sharded state dict + multi-backend init + bf16 + gloo (?) crashes
module: fsdp	torch 2.1 FSDP only some layers might not be working with training only a couple of layers
module: fsdp	OOM when saving model(lora adapter), seems the clause "FullyShardedDataParallel(model,...)" will directly cause the OOM.
module: fsdp	[Bug]: some parameters' grad is None when using FSDP with torch2.1.0
module: fsdp	Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.
module: fsdp	[FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM
module: fsdp	[FSDP ]How to convert sharded_state_dict files into full_state_dict offline without distributed process
module: fsdp	[FSDP] UnpicklingError when calling save_state_dict in distributed run
module: fsdp	FSDP: ShardedStateDict support for world_size = 1
module: fsdp	[FSDP] supports QLora finetuning
module: fsdp	[FSDP] Implement additional check for turn on 2D TP + FSDP extension
module: fsdp	FSDP crashes when submodule calls method that isn't forward()
module: fsdp	[FSDP] Simplify _fully_sharded_module_to_handle
module: fsdp	FSDP vs. MiCS
module: fsdp	FSDP should have tests for partial state_dict and optim state_dict loading
module: fsdp	FSDP do not support ignored_parameters when auto_wrap_policy is specified
module: fsdp	[FSDP] How can I wrap a model that has both nn.Parameter and nn.Module
module: fsdp	[Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding
module: fsdp	[FSDP] incorrect backward prefetch order when using BackwardPrefetch.BACKWARD_POST
module: fsdp	[FSDP] Ignored modules on meta device seem to be initialized on CUDA device
module: fsdp	Provide a reset_parameters() method for MultiheadAttention to support FSDP meta device initializtion
module: fsdp	FSDP custom args per module
module: fsdp	[FSDP]coding to multi-node save optimizer error
module: fsdp	[FSDP] summon_full_params won't change parameters
module: fsdp	Using retain_graph in backward() with FSDP
module: fsdp	[FSDP] Investigate sharded GPU gradient lifetime when CPU offloading
module: fsdp	[FSDP] FSDP doesn't work (random accuracy performance) when using param_init_fn and sync_module_states=True
module: fsdp	[FSDP] using CPUOffload cannot make the code runing stop
module: fsdp	FSDP with gradient checkpointing lead to redundant allgathers during backward
module: fsdp	[FSDP] Revisit mixed-precision casting logic
module: fsdp	FSDP loading with a partial state triggers KeyError
module: fsdp	FSDP Full Shard compatibility with BF16 AMP
module: fsdp	DISABLED test_homogeneous_attributes (__main__.TestFSDPMiscMultiThread)
module: fsdp	fsdp load model causing insufficient CPU memory
module: fsdp	Issue with FSDP does not reduce memory footprint  when scaling up GPUs
module: fsdp	FSDP Optimizer Overlap - follow ups
module: fsdp	[FSDP] ignored_states is broken with auto wrap
module: fsdp	[RFC] Make _HYBRID_SHARD_ZERO2 public as HYBRID_SHARD_GRAD_OP
module: fsdp	Distributing HSDP checkpoint writing for load balancing
module: fsdp	Is memory-efficient FSDP initialization intended to be possible with torch.device('meta')?
module: fsdp	How to unwrap after auto_wrap in FSDP?
module: fsdp	How to modify gradients of an FSDP model?
module: fsdp	[FSDP] train throughput become slow down when loaded shard optimizer dict
module: fsdp	[FSDP] save model checkpoint with StateDictType.LOCAL_STATE_DICT and LocalStateDictConfig(offload_to_cpu=True) fail
module: fsdp	(fsdp) Support for accessing unsharded parameters for methods other than forward()
module: fsdp	RuntimeError: CUDA error: unknown error
module: fsdp	Unexpected High PCIe traffic in Distributed Training since PT 2
module: fsdp	(fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data
module: fsdp	Inconsistent memory allocation using FSDP between PT 2.0 and Nightlies
module: fsdp	Unable to checkpoint model and optimizer state when using Hybrid Sharding Strategy
module: fsdp	Unable to resume job using FSDP with 64 nodes, errors appeared during loading sharded optimizer state dict
module: fsdp	[FSDP]  When amp is enabled, there is a noticeable difference during training between FSDP and DDP
module: fsdp	Duplicate parameters (_flat_params and original params) in the state_dict when using use_orig_params=True and StateDictType.LOCAL_STATE_DICT
module: fsdp	[FSDP] Ensure full precision checkpoints
module: fsdp	[FSDP] Summon buffers in full precision
module: fsdp	multiple values for argument softmax_scale
module: fsdp	Enhance FSDP debugability
module: fsdp	fsdp training with the seq2seqTranier module gets stuck during evaluation.
module: fsdp	Docs suggestion FullyShardedDataParallel.summon_full_params must be called on all ranks/processes
module: fsdp	Issue with FSDP + HuggingFace generate
module: fsdp	FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0
module: fsdp	Sharded Grad Scaler Issue Tracker
module: fsdp	[FSDP] summon_full_params with_grad=True CPU offload can crash
module: fsdp	[FSDP] move up the first all gather
module: fsdp	Improvements to FSDP debugability
module: fsdp	Support backward hook optimizers in FSDP
module: fsdp	[FSDP] Consolidate test_fsdp_state_dict.py
module: fsdp	[composable FSDP] clip_grad_norm
module: fsdp	[FSDP][optim_state_dict] Need more comprehensive tests for optim_state_dict interface
module: fsdp	[FSDP] Make FSDP support local optimizer state_dict
module: fsdp	Harden composable fully_shard: Checklist
module: fsdp	[BE] Avoid .data usage in FSDP buffer casting
module: fsdp	[torchdistx] Future of the large model initialization
module: fsdp	FSDP fails to load state dict under inference_mode
module: fsdp	[FSDP] Gradients not propagating for mixed precision case
module: fsdp	[RFC] Add a static_graph mode for FSDP
module: fsdp	Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading
module: fsdp	[RFC]FSDP API should make limit_all_gathers and forward_prefetch both default to be True
module: fsdp	[FSDP] summon_full_params(writeback=True, rank0_only=True)
module: fsdp	Errors when running the fsdp benchmarks for hf_Bert and hf_T5
module: fsdp	[BE] Improve FSDP <> AC Unit Tests
module: fsdp	Ability to manually set the gradient in FSDP while inside summon_full_params and make it persistent
module: fsdp	[FSDP] Add foreach support to FSDP.clip_grad_norm_()
module: fsdp	ddp vs fsdp
module: fsdp	[FSDP][BE] Add check that compute device equals current device
module: fsdp	[FSDP] FSDP with CPU offload consumes 1.65X more GPU memory when training models with most of the params frozen
module: fsdp	[Composable] Enable summon_full_params for fully_shard
module: fsdp	[BE] Investigate FSDP test _zero_model
module: fsdp	[Composable] Enable setting state_dict_type
module: fsdp	[FSDP] Prepare to deprecate FullyShardedDataParallel.<attrs>
module: fsdp	[FSDP] fully_shard Follow-Ups & Known Issues
module: fsdp	[Composable API] Add fully_shard state dict unit test after manual "wrapping" is supported
module: fsdp	[FSDP] Investigate test_fsdp_pure_fp16.py inaccuracy
module: fsdp	[FSDP][BE] test_fsdp_comm_hooks.py cleanup
module: fsdp	Multiprocessing "Error Propagation" doesn't work for FullyShardedDataParallelism.
module: fsdp	Abort called in FSDP tests
module: fsdp	[FSDP] Revisit meta device initialization
module: fsdp	[RFC] Allow FSDP mixed precision for only certain type of submodules
module: fsdp	[Tracking Issue] Mixed precision does not work with ignored modules
module: fsdp	[FSDP] Adam Gives Different Results Where Only Difference Is Flattening
module: fsdp	[FSDP] Investigate Unit Testing when Gradient Computation Differs on CPU/GPU
module: fsdp	[WIP] Composable FSDP Follow-Ups
module: fsdp	[FSDP] Investigate torch.cuda.current_stream() usage in post-backward
module: fsdp	FSDP support to load DDP optim checkpoints
module: fsdp	[FSDP] use_orig_params=True Follow-Ups & Known Issues
module: fsdp	Composer inductor errors
module: fsdp	[CheckpointWrapper] Revamp API design
module: fsdp	FSDP Forward order differs from that of first run
module: fsdp	Move self.subtest calls in FSDP test suite to run_subtests utility
module: fsdp	[FSDP] Make sharded / unsharded check more robust
module: fsdp	Improve FSDP error msg on wrong attr access
module: fsdp	DDP + FSDP: Investigate behavior for nn.Module APIs
module: fsdp	Enable freezing parts of the model in Fully Sharded Data Parallel
module: fsdp	Check support of FSDP + set_materialize_grads(False)
module: fsdp	Make FSDP easier to debug when erroring in backward pass
module: fsdp	[FSDP] test_summon_single_param() is misleading
module: fsdp	FSDP init can crash with shared parameters
module: fsdp	[FSDP] deepcopy FSDP model for EMA results in error
module: fsdp	[FSDP] test_mp_embedding_reduce() fails with transformer_auto_wrap_policy()
module: fsdp	[FSDP] Avoid explicit replace of activation checkpoint prefixes
module: fsdp	[BE] Refactor FSDP Unit Tests
module: fsdp	[FSDP] Test that module using mixed precision can be loaded into non-mp module
module: fsdp	[FSDP] Progress of ParamExecOrderWrapPolicy
module: fsdp	[FSDP] Verify that FSDP-managed parameters are the same across ranks
module: fsdp	DISABLED test_checkpoint_wrapper_parity (__main__.CheckpointWrapperTest)
module: fsdp	[BE] Generalize recursive wrapping utility
module: fsdp	[FSDP] Customizable gradient pre-divide for mixed precision training
module: fsdp	[FSDP] Enhance sync_module_states for auto wrapping
module: fsdp	Torchdynamo for Deepspeed and FSDP
module: fsdp	FSDP: enhanced shared parameter support
module: fsdp	FSDP: test mixed precision with checkpoint
module: fsdp	Investigate sharded gradscaler OOM on CPU workloads
module: fsdp	Standalone unittests for checkpoint_wrapper
module: fsdp	FSDP: Mixed precision should not cast ignored buffers
module: fsdp	FSDP: ability to ignore parameters
module: fsdp	[FSDP] ignored_modules follow-ups
module: fsdp	broadcast_object_list with GPU tensors can lead to deadlock on PyTorch CI machines
module: fsdp	[RFC] Upstream current implementation of ssd_offload from fairscale FSDP to Torch Distributed FSDP
module: fsdp	Feature requests for optimizer overlapping
module: fsdp	FSDP remove the requirement of all trainable parameters
module: fsdp	[FSDP] Verify buffer checkpointing
module: fsdp	[FSDP]  using CPUOffload creates 3-10x slowdown due to slow cpu optimizer step/update
module: fsdp	[BE][Docs][FSDP] Clarify microbatching support and tradeoffs
module: fsdp	FSDP does not work on GLOO backend
module: fsdp	[FSDP][BE] TestAutoWrap should inherit from MultiProcessTestCase
module: fsdp	[FSDP] Add Gradient Accumulation Outside no_sync() Compatibility with CPU Offloading
module: fsdp	[Discussion][FSDP] Enhancements to auto_wrap_policy
module: fsdp	[FSDP] test full_state_dict if we are already in full parameter summoning context
module: fsdp	[FSDP] test state_dict APIs with arguments such as destination, prefix
module: fsdp	[FSDP checkpoint] Test replace_by_prefix util
module: fsdp	Enhanced local_state_dict FSDP checkpoint tests
module: fsdp	Run test_fsdp_core parity test for FSDP model checkpoint
module: fsdp	[Main Issue] FSDP Model Checkpoint
module: fsdp	[FSDP] Run parity tests for activation checkpoint and offload
module: fsdp	API to support combined activation offloading or checkpointing
module: fsdp	[RFC] Activation Checkpoint API improvements
module: fsdp	[FSDP] Enable tests for Gloo backend
module: fsdp	[FSDP] Wrap API improvements
module: fsdp	[Better Engineering] Generalize DDPSink and consolidate it with FSDP's pre-backward hooks
module: fsdp	[Comment] remove torch.cuda.synchronize() in state_dict()
module: fsdp	[Efficiency] No shard + CPU offload in FSDP
module: fsdp	[Beta] Have a generic no_sync for all synchronized training features
module: fsdp	[Tools] Extend generic join to support FSDP
module: fsdp	[Tools] Improve FSDP debuggability
module: fsdp	[Tools] Add FSDP logging data
module: fsdp	[Efficiency] Support optimizer overlap with backward pass in FSDP
module: fsdp	[Beta] FSDP initialization redesign
module: fsdp	[Prototype][RFC] PyTorch FullyShardedDataParallel(FSDP) API Proposal
module: cpu	[Intel GPU] Enable oneDNN GEMM Primitive
module: cpu	More efficient multi-threading in Softmax & LogSoftmax CPU kernels
module: cpu	WIP Add pooling support for 3d channels last
module: cpu	[inductor][cpu] RuntimeError: quantized_resize_cpu_ does not have a deterministic implementation
module: cpu	optimize (u)int8 vectorized operator*
module: cpu	Remove use of math_compat.h
module: cpu	[Draft] Batch Norm Consolidation
module: cpu	enable mkldnn bf32 matmul
module: cpu	[CPU] Add flash attention mask version
module: cpu	[inductor][cpu] Some huggingface models cpp wrapper accuracy crash
module: cpu	[inductor][cpu] cpp/default wrapper static/dynamic models performance regression
module: cpu	[inductor][cpu] hf_T5_generate cpp wrapper static/dynamic shape multiple threads performance drop
module: cpu	enable mkldnn op weight pre-packing on aarch64 (cherrypick #115037)
module: cpu	additional support for float8_e4m3fnuz and _e5m2fnuz
module: cpu	[Quant] Enable QConv2d with hardswish  post op
module: cpu	[inductor][cpu] few models show performance regression on CPP Wrapper
module: cpu	[inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper
module: cpu	[Quant] Add int8 linear op gelu for quantization PT2E with Inductor. input is an int8 CPU tensor; weight is an int8 MdkldnnCPU tensor
module: cpu	Arm64 windows
module: cpu	[CUDNN] Add an option to force cuDNN usage
module: cpu	[BE]: ruff enable SLOT checks and add fixes
module: cpu	[AOTInductor] Support freezing model in aoti
module: cpu	The 'out'-parameter in torch.matmul() works for 'cuda' device but not for 'cpu'
module: cpu	address comments
module: cpu	torch.matrix_exp(x) get inf and nan
module: cpu	Fix integer overflow in quantization
module: cpu	Optimize batch_norm_cpu_collect_stats_channels_last_impl when N <= num_threads
module: cpu	S390x ci
module: cpu	Add vectorized norm fill for ppc64le
module: cpu	Add vectorized batch_norm_cpu_collect_stats_contiguous_impl
module: cpu	torch.div on empty tensors causes segmentation fault
module: cpu	log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16
module: cpu	CPU flash attention with mask
module: cpu	Reland: Use fmt::format in NCCLUtils and ProcessGroupNCCL instead of c10::str
module: cpu	Enable cupti
module: cpu	torch.autocast() hangs on CPUs
module: cpu	Fix flake8-bugbear B019
module: cpu	[inductor][cpu] [dynamic shapes][cppwrapper] performance regression
module: cpu	Broadcasting matmul is much slower than corresponding einsum
module: cpu	Use aligned_alloc
module: cpu	Create nested _sdpa_flash
module: cpu	[cpu] explicitly vectorize trigamma & polygamma
module: cpu	add GradScaler on CPU
module: cpu	Fix CPU bitwise shifts for out-of-limit values in VSX-vec
module: cpu	ensure uint8 is honoured for cpu operations in dynamo
module: cpu	add _amp_foreach_non_finite_check_and_unscale_cpu_ and _amp_update_scale_cpu_ kernels on CPU
module: cpu	Fix the max pool kernel with channels_last memory layout
module: cpu	Eliminate calls of c10::guts::conjunction,c10::guts::disjunction,c10::guts::negation,c10::guts::void_t, c10::invoke and c10::guts::apply
module: cpu	The CPU version of torch.cummax is slow
module: cpu	quantized module serialization through prepack function registration
module: cpu	DISABLED test_conv_with_as_strided_cpu (__main__.FreezingCpuTests)
module: cpu	Add drop_remainder & redistribute to torch.chunk and drop_remainder for torch.split
module: cpu	[cpu] vectorize acosh
module: cpu	No checks when running torch.nn.functional.ctc_loss with bogus inputs
module: cpu	Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0)
module: cpu	Add half specializations for load of sum
module: cpu	Add UT for NEON implementation of vec_reduce_all
module: cpu	https://pytorch.org/docs/stable/backends.html does not describe torch.backends.cpu
module: cpu	add Half support for interpolate operators on CPU
module: cpu	[Inductor] Add support for NEON ISA in the Inductor C++ backend
module: cpu	F.adaptive_avg_pool3d(input, 1) returns infinity in half precision
module: cpu	Remove cpp_custom_type_hack
module: cpu	vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division
module: cpu	Support building pytorch using MKL ILP64 model.
module: cpu	[RFC] Add third-party malloc library to improve pytorch memory performance on Windows
module: cpu	AddressSanitizer: heap-buffer-overflow in test_comprehensive_nn_functional_embedding_bag_cpu_bfloat16
module: cpu	RuntimeError in Scaled Dot Product Attention Tutorial Code
module: cpu	torch.concat fails with float16 input in autocast(device_type=cpu) context
module: cpu	gpu training  work well, but cpu training not work
module: cpu	'Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search
module: cpu	torch.fmod produces inconsistent results in eager and compile mode
module: cpu	[RFC] CPU float16 performance optimization on eager mode.
module: cpu	CPU time performance is unstable
module: cpu	[BE] [cuDNN] Always build assuming cuDNN >= 8.1
module: cpu	LSTM on CPU is significantly slower on PyTorch compared to other frameworks
module: cpu	cpu log1p for bfloat16 gives wrong result.
module: cpu	RFC: Enabling AVX512 dispatch for compute-intensive ATen ops
module: cpu	Type promotion for accumulate operation differs between eager and CPP dynamo
module: cpu	Why is AvgPool2D taking longer than Conv2D for the same input?
module: cpu	Fails to build on ppc64le with clang
module: cpu	quantile fails for float16/half inputs
module: cpu	Compare oneDNN and OpenBLAS backend of PyTorch on arm64 architecture
module: cpu	Large slow down by not calling torch.set_num_threads
module: cpu	torch.empty produces incorrect tensors with layout=sparse_csr|sparse_csc on the CPU
module: cpu	Basic math operations produce a "floating point exception"
module: cpu	prod_cpu not implemented for 'BFloat16'
module: cpu	[PT][1.13] torch .numpy() fn broke for some scenario
module: cpu	[feature request] Get/set fastmath CPU bit (and some other FPU flags?)
module: cpu	nn.Linear allocate too many space which lead to CPUAllocator "allocate memory failure" if it's BF16. good for FP32.
module: cpu	What causes CPU to degrade when I load the weight with torch.hub.load()
module: cpu	unique will reverse the input when sort=False on cpu (not sorting)
module: cpu	bfloat16 matmul gives incorrect result on CPU (without mkldnn)
module: cpu	[Mac M1] torch.mm sometimes produces incorrect results
module: cpu	[feature request] no-param sort to exploit parallelization
module: cpu	torch.sort does not exploit parallelization when invoked without the dim parameter.
module: cpu	Failure to set number of threads on AWS Lambda
module: cpu	Support unscaling grad on CPU
module: cpu	Segmentation fault in fractional_max_pool3d
module: cpu	Segmentation fault in fractional_max_pool2d
module: cpu	Floating point exception in _nnpack_spatial_convolution
module: cpu	LayerNorm triggers INTERNAL ASSERT
module: cpu	max_unpool2d returns a tensor with negative dimension
module: cpu	Matrix multiplication is 30 times slower for integers than floats on CPU
module: cpu	pytorch hangs during interaction with ray package
module: cpu	RFC: Deprecate Bottleneck
module: cpu	INTERNAL ASSERT FAILED at "../aten/src/ATen/MapAllocator.cpp":323, please report a bug to PyTorch. unable to mmap 68 bytes from file </torch_530808_7992>: Cannot allocate memory (12) ---------------------------------------------------------------------------
module: cpu	SIGILL, Illegal Instruction from libtorch_cpu.so when callling backward() function in a VM.
module: cpu	Memory Leak in MKL OpenMP on AVX2 machine
module: cpu	pytorch 1.8.1+cu111 used much more CPU RAM than pytorch 1.8.1 after run import torch
module: cpu	Bad performance of stock model on Windows compared to Linux
module: cpu	"LayerNormKernelImpl" not implemented for 'Half' - CPU
module: cpu	torch.median slower than torch.sort on cpu
module: cpu	[bug] torch.{sinh, cosh}: Incorrect values for vectorized path
module: cpu	torch.eye(d) is slow and hogs cpu for d >= 182
module: cpu	RuntimeError: "threshold_cpu" not implemented for 'Half'
module: cpu	thread blocked when moving a tensor from GPU to CPU, by calling the function .cpu() in pytorch. This kind of block can be stop by any window event like mouse moving/clicking or keyboard pressing.
module: cpu	MacOS CPU torch.tan and torch.tanh do not compute some values properly
module: cpu	Asynchronous Execution on CPU
module: cpu	Vec256<int64_t> does not handle LONG_MAX on minimum
module: cpu	[feature request] Faster specialized int16->float32 conversions to match speed with NumPy
module: cpu	libtorch 1.5 crashes when used on macs when using torch::max without AVX support
module: cpu	Implement LSH Optimizations for Enhanced CPU-Only Performance
module: cpu	LSTMs leak memory in CPU PyTorch 1.5.1, 1.6, and 1.7 on Linux
module: cpu	Bottleneck when publishing the model using flask about 3 times slower.
module: cpu	Generalized CPU vector reductions
module: cpu	TopK implementation slower than a custom divide and conquer implementation
module: cpu	MKLDNN_conv2d 2X slower than the native TH implementation
module: cpu	CPU softmax performance poor when dim is not the last dimension
module: cpu	Tensor.random_ is not implemented for bfloat16 on CPU(but implemented on CUDA)
module: cpu	Inaccurate batched GRU results on CPU
module: cpu	logsumexp: two little-impact perf suggestions
module: cpu	Optimizing DLRM for CPU
module: cpu	Pytorch openmp thread number tuning option for CPU trainning
module: cpu	Increasing memory usage on CPU
module: cpu	CPU MaxPool2d is very slow
module: cpu	torch.multinominal ignores elements from cumulative distribution
module: cpu	MKLDNN+AMD BLIS path for PyTorch
module: cpu	CPU version of PyTorch on PyPI
module: cpu	Port masked_fill operator from the TH code to Aten
module: cpu	[feature request] Subset of eigenvalues/eigenvectors
module: cpu	Error out during compilation if USE_FBGEMM=1 is ignored
module: cpu	Enable PyTorch Bfloat16 for CPU and add MKL-DNN bfloat16 optimization for Cooper Lake
module: cpu	Port fmod operator from the TH code to Aten
module: cpu	DispatchStub should report what operator it failed to find kernel for
module: cpu	Eigen Tensor library for convolutions on CPU
module: cpu	Illegal instruction (core dumped) when running in qemu
module: cpu	Integer division by Zero giving large number results instead of NaN/inf on Windows
module: cpu	Improve multithreaded random number generation (RNG)
module: cpu	Strange latency overhead of F.conv2d
module: cpu	Overhead performance regression over time umbrella issue.
module: cpu	vectorized convert_to_int_of_same_size <int64_t> can't handle nan
module: cpu	Performance issue master (a25b79531)
module: cpu	The unary_kernel call should be pushed into CopyKernel.cpp and it should completely replace the current copy_kernel.
module: cpu	LayerNorm is very slow (almost frozen) in CPU of multiprocessing
module: cpu	Performance issue of unique on CPU
module: cpu	Speed-up torch.cat on CPU
module: cpu	Performance regression on CPU from 0.4.1 to 1.0.0 on ResNet inference
module: cpu	depthwise convolution are slow on cpu
module: cpu	Improved performance for torch.multinomial with small batches
module: cpu	Pytorch is slow when only using CPU, and cannot utilize multicore of CPU
module: cpu	[feature request]Support AVX512F intrinstics to vectorize operations
module: cpu	Very slow on CPU
module: cpu	improve performance of common CPU clone / contiguous calls with HPTT
module: cpu	Discrepancy in BCEWithLogitsLoss and ClassNLLLoss
module: internals	In the func Tensor.to, how can I make privateuse lazy init
module: internals	Implement Copy-on-write (COW) tensors
module: internals	Optimize PyTorch C++ part with Profile-Guided Optimization (PGO)
module: internals	aot_export_joint_simple on plain callable (not graph module) doesn't attach stack traces
module: internals	CPU Fallback does not convert Tensor?[]
module: internals	[c++17] Replace lock_guard with scoped_lock
module: internals	Cannot use AT_CUDA_DRIVER_CHECK from user code
module: internals	Memory corruption using torch.ops.* to access re-registered operator
module: internals	Bring CudaPluggableAllocator to feature parity with the Native Allocator
module: internals	Contribute to the privateuse1 backend.
module: internals	make tensor data const correct
module: internals	torch.ops.aten.pow(2.0, 3) return unexpected value with complex type
module: internals	warn on future reshape alias mutation violations
module: internals	PyObject preservation and resurrection for StorageImpl
module: internals	IValue(c10::List<IValue>) constructor is confusing and undocumented
module: internals	Solve default argument induced include cycles by not using defaults / moving the defaults to inl
module: internals	Rename DispatchKey Dense/Sparse/etc to DenseFunctionality/SparseFunctionality, use original name for alias
module: internals	Modernize logging tensor in torch.testing._internal
module: internals	New c10 constants
module: internals	Automate cleanup of header includes
module: internals	Stop calling sizes/numel/dim/is_contiguous on undefined tensors
module: internals	Private API for accessing all "internal" attributes on Tensors
module: internals	Clean up PyTorch's private operators
module: internals	Remove _log_softmax/_softmax in favor of log_softmax and softmax respectively.
module: internals	Off main thread symbolic evaluation
module: internals	Runtime configuration to disable TORCH_WARN temporally?
module: internals	Idiom for extensible string printing for TensorImpl subclasses
module: internals	Test functionalization doesn't run
module: internals	Composite Compliance Problems Tracker
module: internals	[RFC] Support MemoryView for Tensors
module: internals	torch.flip, torch.roll, torch.tile has arguments dims= for both dim/multidim while other functions have argument dim= for the same usage
module: internals	Give a better error message when REGISTER_DISPATCH is used in improper context
module: internals	Invalid handling of out args with type Tensor[] in native_functions.yaml
module: internals	API torch.ops.image.read_file reports RuntimeError - No such operator image::read_file
module: internals	THPVariable_subclass_dealloc (i.e. tp_dealloc for torch.Tensor) should handle Python error state gracefully
module: internals	Create a new OptionalArrayRef template class to replace Optional<ArrayRef>
module: internals	Key already registered with the same priority: uv
module: internals	allow specification of variadic arguments in native_functions.yaml
module: internals	meta tensor set_ seems fishy
module: internals	Dynamic tensor rematerialization
module: internals	Create a boxed fallback / template recipe for Autograd that forwards, but errors on backwards
module: internals	Caffe2 tensor to ATen conversion doesn't initialize PyTorch CUDA state
module: internals	Support adding new keyword-only parameters without breaking FC
module: internals	Allow creation of pseudo devices for testing purposes
module: internals	backward compatibility - need a way to find out when a certain API was added/modified/etc.
module: internals	m.fallback(torch::CppFunction::makeFromBoxedFunction<&my_fallback>) gives bad error message
module: internals	Get rid of weak_intrusive_ptr
module: internals	unsafe_reclaim_from_nonowning is not that unsafe
module: internals	Figure out what mojo FB common/process/StackTrace.h has that we don't
module: internals	Alternative backend fallback like mechanism which has higher precedence than operator-specific composite implementations
module: internals	Let backends specify a schema version when registering kernels
module: internals	Add Quantized{CPU|CUDA} support to Structured Kernels
module: internals	Omitting mutability annotations in op schemas can lead to subtle errors
module: internals	std::type_index doesn't provide reliable equality for getCustomClassTypeMap
module: internals	Class-based structured kernels instruction count regression
module: internals	Dispatch-less structured wrapper / composite / alias kernels
module: internals	[question] How hard would it be to implement 4-bit precision training?
module: internals	Remove remaining native:: references from non-native ATen
module: internals	Operator registration doesn't work with noexcept functions on some compilers (c10::guts::is_function_type rejects noexcept)
module: internals	[feature request] Keyword-only device argument (and maybe dtype) for torch.meshgrid
module: internals	[Discussion] Use the unicode variant of the Windows API
module: internals	How to define a new data type in  native_functions.yaml?
module: internals	The documentation for c10::Dict is completely empty.
module: internals	Code asserts when register new aten operations' implementation thru "c10::RegisterOperators::op" API.
module: internals	Remove header declarations for CPUType/TypeDefault
module: internals	Compile error when use c10::RegisterOperators::Options::kernel with the return type "std::tuple<Tensor&, Tensor&>"
module: internals	Retire usages of CUDA_tensor_apply helpers in ATen
module: internals	Get rid of copy_from
module: internals	[tools.codegen] Rename api.legacy_dispatcher to api.native
module: internals	[FX] Schema normalization tooling
module: internals	Inference performance regression caused by hacky_wrapper_for_legacy_signatures
module: internals	OffsetCalculator.cuh(and THCIntegerDivider.cuh) should be available with PyTorch cpu-only binaries
module: internals	Reduce number of stack frames used up by dispatcher
module: internals	BusError memory-mapped tensor
module: internals	Implement backend fallback for Tracer
module: internals	Stop registering kernels that use DispatchStub as catch all
module: internals	Print values (but not strings) when STRIP_ERROR_MESSAGES is defined
module: internals	[discussion] Expressing tensor dimension semantics / constraints through typing / constraints blocks. Constraints block could be scripted/traced and help for tracing/script execution and codegen
module: internals	Tracking output dimensions of the convolutional layers
module: internals	Move torch cpp Errors to c10::Error
module: internals	Move all torch.Tensor methods to codegen
module: internals	Negative stride values in as_strided
module: internals	Use general ATen dispatch mechanism
module: internals	ATen registrable operator list
module: internals	ATen operator API versioning
module: internals	Propagation of channels-last layout leads to massive slowdowns in 1.5 compared to 1.4
module: internals	Integration of Large Model Support in PyTorch
module: internals	Enable OpaqueTensor to possess Storage then allow it to view from CPUTensor
module: internals	Make ArrayRef::size() return int64_t rather than size_t
module: internals	__cuda_array_interface__ conversion does not support readonly arrays
module: internals	[C++] Don't use DeprecatedTypeProperties in torch::utils::reorder_tensors_like
module: internals	error C3203: “templated_iterator”: 未专用化的 类 模板 不能用作 模板 变量，该变量属于 模板 参数“_Ty1”，应为 real 类型
module: internals	false CHECK FAILED at ../aten/src/ATen/core/function_schema_inl.h
module: internals	Remove Ops bound in Python Layer for Legacy Reasons
module: internals	Natively Declarable Fast-path Functions
module: internals	Dispatch key reorganization
module: internals	Unsafe use of at::parallel_for in current codebase
module: internals	RPC couldn't match torch.ones with requires_grad=True
module: internals	Tensor.__reversed__ breaks protocol for reversible objects
module: internals	Unified management of thread local variables
module: internals	Unify warning logging mechanism
module: internals	[c10] c10 dispatch doesn't support tracing of scalars
module: internals	Statically checked tensor shapes
module: internals	Remove TensorOptions logic from generated code
module: internals	c10 List API hard to use
module: internals	Delete TensorOptions::operator==
module: internals	Ability to tell whether a tensor might be changed in TH/Aten impl
module: internals	[RPC] Fix logging initialization warning in ProcessGroupAgent
module: internals	TensorIterator "builder" options should be documented.
module: internals	"PyTorch core" thread local flag
module: internals	TensorIterator stubs are designed for merge conflicts.
module: internals	suggest_memory_format has ambiguity & cannot represent intended layout format for corner cases
module: internals	TensorImpl de-virtualization
module: internals	assert_no_internal_overlap should pass const char*
module: internals	DispatchStub should report what operator it failed to find kernel for
module: internals	scatter_ supporting different reduction modes
module: internals	Consolidate definition of operators/gradients where possible
module: internals	Logging mode for saying when tensor broadcast occurs
module: internals	How about add torch::end for slicing in c++ frontend
module: internals	Undefined symbols for architecture x86_64: "testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith(void const*)" on Mac OS X
module: internals	Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase
module: internals	String in tensor
module: internals	Overhead performance regression over time umbrella issue.
module: internals	Define portable M_PI replacement, use it instead of non-standard M_PI in math.h
module: internals	TensorIterator resizes output to a scalar if there are no inputs
module: internals	RuntimeError: invalid argument 10: ldb should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:36
module: internals	install error from source
module: internals	Importing matlab.engine after torch causes bad_alloc
module: internals	Massive memory overhead over NumPy
module: internals	[RFC] Memory format (aka layout aka NHWC) support
module: internals	Completion of error handling
module: internals	Add support for tuple type deduction in C++ custom operators
module: internals	Proposal: Add __tensor_wrap__ method similar to numpy __array_wrap__
module: internals	Modern interface for Storage
module: internals	Stop using make_intrusive directly; provide some make_tensor
module: internals	Unexpected Behavior when Pointwise Operations Write to Expanded Tensors
module: internals	Stop passing inplace/out arguments as (non-const) Tensor& to functions
module: internals	Come with a better strategy for TensorArg (error reporting)
module: internals	Better header hygiene in ATen
module: internals	ATen explicitly differentiated native function resolution hazard (call is ambiguous)
module: internals	Use the int64 version of MKL calls
module: cudnn	Mixed precision is not working with CudnnLSTM on rocm
module: cudnn	Optimize cudnn_convolution_out to Reduce Unnecessary Memory Allocation and Copy
module: cudnn	[Bug] Big difference between the output of Conv float precision and double precision
module: cudnn	Runtime Errors with convolution_backward_out When Handling Optional Bias Gradient
module: cudnn	[CUDNN] Add an option to force cuDNN usage
module: cudnn	[cuDNN][cuDNN V8 API] cuDNN Flash-Attention Upstreaming RFC/tracking issue
module: cudnn	Implement FlashFFTConv algorithm
module: cudnn	cuDNN error: CUDNN_STATUS_MAPPING_ERROR on gtx_1080/A10 when conv1d is called
module: cudnn	ncu python conv2d.py runs indefinitely after activating cudnn.benchmark
module: cudnn	F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
module: cudnn	The following will always fail on NixOS
module: cudnn	conv cudnn support integers
module: cudnn	Increasing batch size makes network forward 1000 times slower
module: cudnn	Differences in the results of conv2d calculations in PyTorch 1.8
module: cudnn	PyTorch built with CuDNN-8.8.1 crashes if CuDNN-8.9.2 is installed on the system
module: cudnn	System memory leak when using different input size of torch.nn.Conv3d
module: cudnn	LSTM built-in dropout not reproducible on GPU
module: cudnn	Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118)
module: cudnn	CUDA 10.2 cudnn 8.2.4 run Conv2d error
module: cudnn	Performance bugs exists in multiple convolution operations(e.g., Convtranspose2d) when useing the groups argument
module: cudnn	cuDNN doesn't support convolutions with more than INT_MAX elements and native kernel uses too much memory
module: cudnn	aten::cudnn_convolution chooses different conv implementation given the same inputs.
module: cudnn	Unable to find an engine to execute when using pip to install but not with conda
module: cudnn	benchmark cache persist
module: cudnn	cuDNN error (CUDNN_STATUS_NOT_SUPPORTED) for torch.nn.functional.grid_sample()
module: cudnn	pytorch could not build from source with cudnn 8.0.5
module: cudnn	Installing PyTorch with BUILD_SPLIT_CUDA=ON and CUDNN fails on linker error
module: cudnn	torch 1.12.1 cuda 10.2 runs slower than torch 1.8.2 cuda 10.2
module: cudnn	Failed to static link latest cuDNN while compiling
module: cudnn	PyTorch 1.12 cu113 Illegal Memory Access or Internal Error instead of Out of Memory cases
module: cudnn	Cudnn batch norm kernel (batchnorm_bwtr_nhwc_semiPersist) gets blocked by overlapping NCCL all_reduce calls
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	[1.9.1] [collect_env] collect_env does not collect actual runtime-loaded cudnn version
module: cudnn	with torch.backends.cudnn.flags(deterministic=True) doesn't give an exception for ctc_loss backward on CUDA
module: cudnn	NaN tensor values problem for GTX16xx users  (no problem on other devices)
module: cudnn	Conv2D with large different number of input and output channels gives a CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	Process hangs after calling conv2d() in pytorch 1.11.0 with CUDA 11.3
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_VERSION_MISMATCH for torchvision models
module: cudnn	RuntimeError: CUDA error: unspecified launch failure
module: cudnn	Group convolution slower than manually running separate convolutions in CUDA streams
module: cudnn	Training grouped Conv2D is slow
module: cudnn	Fusion of Convolution and BatchNorm
module: cudnn	Tool for detecting inefficent striding for nn.Conv2d
module: cudnn	cudnn_convolution_relu is 17x slower for 7x7 convolutions and channels last
module: cudnn	Anomaly detection: Error detected in CudnnRnnBackward0
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	cudnn_convolution_add_relu fails under basic conditions
module: cudnn	cudnn_batch_norm_backward is extremely imprecise for some input shapes
module: cudnn	Can I set the algorithm of torch.nn.Conv to CUDNN_CONVOLUTION FWD_ALGO_GEMM by myself?
module: cudnn	matmul causing cuDNN error: CUDNN_STATUS_INTERNAL_ERROR at future, unrelated Conv2d
module: cudnn	[cuDNN v8] Improve cuDNN convolution v8 API error reporting
module: cudnn	[cuDNN v8] Extend current cuDNN convolution v8 API binding to support conv-bias-activation fusion
module: cudnn	[cuDNN v8] Extend current cuDNN convolution v8 API binding to support cuDNN benchmark
module: cudnn	[cuDNN v8] Extend current cuDNN v8 API binding to support convolution backward and transposed convolution forward
module: cudnn	Ability to enabling/disabling cuDNN and cuBLAS API logging in PyTorch API directly
module: cudnn	nn.Upsample result mismatch in 1.1.0a0+828a6a3 and 1.9.0
module: cudnn	cuDNN v8 API tracking issue
module: cudnn	Backward pass for a nn.Conv2d with half-precision on Quadro RTX 8000 leads to CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	cuDNN error upon the backward method with a sliced tensor output from nn.GRU.
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED in pytorch lightning
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	Exceedingly different F.conv2d outputs on cuda/cpu
module: cudnn	[proposal] Way to export/display the convolution algorithm found with cudnn.benchmark = True + allow to override algo choice with conv/matmul module/function algo/hint arguments
module: cudnn	GPU (+CPU/RAM?) self-check script or instructions in core
module: cudnn	Conv3D error : CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	Performance bugs of transpose2d on A100
module: cudnn	BatchNorm3d error : CUDNN_STATUS_NOT_SUPPORTED
module: cudnn	CuDNN 8 with benchmark=True takes minutes to execute for certain configurations
module: cudnn	Unexpected slow dropout in stacked RNN/LSTM/GRU
module: cudnn	Calling emptyCache in Conv_v7.cpp causes performance degradation
module: cudnn	This error occurs occasionally during the run
module: cudnn	Provide a mechanism to limit the workspace size of cudnn convolution
module: cudnn	CUDA error: illegal memory access Conv3d
module: cudnn	cudnn cannot be pickled by cloudpickle
module: cudnn	cudnn convolution modifies the input Tensor metadata inplace when it tries to .resize_() it
module: cudnn	ConvTranspose1d groups=channels is very slow!!!
module: cudnn	No improvement gain between sm_86 (cuda 11.1) and sm_80 (cuda 11.0) on 3090 or 3080 GPUs.
module: cudnn	F.conv2d() causes RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: cudnn	CuDNN version not found
module: cudnn	about benchmark issue
module: cudnn	PyTorch 1.6 DataParallel causes CUDNN_STATUS_BAD_PARAM in backward pass
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.
module: cudnn	torch.cuda.synchronize Influence distributed training
module: cudnn	Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D
module: cudnn	CuDNN RNN bindings are basically all deprecated in cudnn 8
module: cudnn	Inconsistent results when trying to enable Tensor Cores on NVIDIA T4
module: cudnn	Difference in inference time between CUDA 10.0 & 10.2
module: cudnn	Conv3d with specific kernel size outputs inconsistent results between FP16 and FP32 in V100 GPU
module: cudnn	About torch.backends.cudnn.deterministic issue
module: cudnn	Memory Leak with Docker GPU
module: cudnn	RuntimeError: CUDA error: an illegal memory access was encountered with channels_last
module: cudnn	3D grouped & depthwise convolution very slow on backward pass
module: cudnn	Illegal memory access / CUDNN_STATUS_MAPPING_ERROR on Quadro 8000
module: cudnn	CUDNN_STATUS_INTERNAL_ERROR with GPU RTX 8000
module: cudnn	nn.LSTM gives nondeterministic results with dropout and multiple layers, OR cuDNN version mismatch
module: cudnn	Performance bug with convolutions with weights and inputs of similar spatial size
module: cudnn	conv2d is slow with specific shapes of channels_last tensors
module: cudnn	cuDNN batchnorm with non-contiguous running mean silently discards updates
module: cudnn	CUDNN_STATUS_EXECUTION_FAILED
module: cudnn	Backward Functional.conv3d is slow when cuDNN is enabled
module: cudnn	cuDNN convolution does not handle empty input tensor
module: cudnn	affine_grid CUDA / cuDNN support for Half removed in 1.3.x
module: cudnn	cudnn.determinstic=True causes dilated convolution to be >10x slower
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
module: cudnn	conv2d Memory usage is too large； pytorch 1.1.0
module: cudnn	Conv2D 2x~20x slower than Tensorflow when channel count is small
module: cudnn	Error in python3: double free or corruption (fasttop)
module: cudnn	Benchmark cuDNN affine_grid_generator vs native
module: cudnn	Benchmark cudnn version of grid sampler
module: cudnn	model use dilated conv backward in v1.1.0 is ~3x slower than in v0.4.1 on 1080Ti
module: cudnn	Slow convolution with large kernels, should be using FFT
module: cudnn	Performance issue when accessing an extremely large (10GB) longtensor
module: cudnn	BatchNorm1d does not support batchsize>65535 in eval mode with 3 dimension (NxCxL), raise CUDNN_STATUS_NOT_SUPPORTED
module: cudnn	cudnn conv doesn't check batch_size > 0
module: cudnn	FP32 depthwise convolution is slow in GPU
module: cudnn	Make it easier to figure out what CuDNN convolution algorithm we actually chose
module: cudnn	cuDNN error when using 3d convolutions
module: cudnn	RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when batch size is too large
module: cudnn	C++ API: Crash in cudnnDestroy() when deconstructing
module: cudnn	Check whether _cudnn_rnn_flatten_weight can avoid changing the TensorImpl or Storage pointer of tensors in weight_arr
module: cudnn	CUDA cache doubles on the second batch and causes OOM, empty_cache doesn't empty it
module: cudnn	cudnn not found
module: cudnn	CuDNN convolution on some CUDA devices will not preserve NaN weights (upstream bug)
module: cudnn	OOM Exception when using torch.nn.grad.conv2d_weight (apparently because CuDNN backwards is not used)
module: cudnn	CuDNN version not supported
module: cudnn	[Caffe2] cudnn versions compatibility issue.
module: cudnn	BatchNorm1d raises RuntimeError (CUDNN_STATUS_BAD_PARAM) on 3D input.
module: cudnn	[feature request] Type-1 Multi-layer bidirectional RNN
module: cudnn	CUDNN_STATUS_INTERNAL_ERROR when training with conv3d
module: cudnn	Met 'cudnnDestroyDropoutDescriptor' while run multiply gpu-based models in multiply processes
module: cudnn	Carefully audit contiguity requirements of code
module: cudnn	Cache CuDNN benchmark selection, turn it on by default, use it across PyTorch runs
module: cudnn	single-gpu works but multi-gpu hangs
module: cudnn	Fused RNN refactor plan
module: cudnn	GridSampler behaviours
module: cudnn	Exposing CuDNN benchmark strategy selection
module: cudnn	expose backend selection and cudnn settings to the end user
module: crash	PyTorch Distributed Elastic Launch Segmentation Fault with Python 3.12
module: crash	torch.quantize_per_channel: FPE
module: crash	Quantisation results in a "Illegal instruction (core dumped)"
module: crash	torch.nn.functional.embedding segmentation fault on large negative ids
module: crash	[MPS only] failed assertion `New volume: xxx should match old volume: xxx [reshapeWithCommandBuffer] MPSNDArrayIdentity.'
module: crash	Tensors Can't be  Overwritten in Visual Studio Windows
module: crash	torch.div on empty tensors causes segmentation fault
module: crash	Possible use-after-free of Tensor in JIT generated code
module: crash	Multiprocess. DataLoader worker  is killed by signal: Segmentation fault.
module: crash	Breaking incompatibility with Cuda 12.2, pytorch stable, torchvision
module: crash	Abort when running torch.set_num_interop_threads
module: crash	C++ API torch::nn::MultiheadAttention Crashes by division by zero
module: crash	Calling ops.aten.embedding_bag() function got silent crash
module: crash	distributed.batch_isend_irecv() crash when send/recv refers to itself
module: crash	libtorch > 1.9.1 produces segfault on Qt5 gui application exit
module: crash	Fine-tuning HuggingFace wav2vec 2.0 with torch.compile
module: crash	torch.cuda.is_available() crashes python in systems with disabled gpu
module: crash	'Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search
module: crash	Unused import torch followed by cuml.NearestNeighbors leads to nondeterministic segfault (during Python process exit?)
module: crash	Segfault when using torch.ops.* to access de-registered op
module: crash	A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback
module: crash	A crash due to Floating Point Exception can be triggered in torch.index_select
module: crash	Segmentation fault (core dumped) during Torch finetuning (at random step)
module: crash	A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case
module: crash	A Segment Fault can be triggered in torch.geqrf with an edge case
module: crash	A Segment Fault can be triggered in torch.pinverse
module: crash	Memory Corruption in torch.lstm caused by edge cases
module: crash	MPS internal error in torch.gather when last dimension is a singleton dimension
module: crash	A segment fault can be triggered in torch.avg_pool1d
module: crash	A segment fault can be triggered in torch.max_pool1d_with_indices
module: crash	A segment fault can be triggered in torch.svd
module: crash	A segment fault can be triggered in torch.lstm with edge cases
module: crash	A segment fault can be triggered in torch.histogramdd
module: crash	A Floating Point Exception can be trigerred in torch._C._nn.slow_conv3d
module: crash	Segmentation fault between Numpy and Pytorch using torch.bmm
module: crash	segfault when running torch.lu_unpack
module: crash	Segmentation fault when running torch.nn.functional.fractional_max_pool3d on torch 1.13.1
module: crash	Crashes of linalg.ldl_solve on different edge cases not coming from linalg.ldl_factor
module: crash	Cannot cast float64 to float32
module: crash	RuntimeError: Error in dlopen: libnvJitLink.so.12: cannot open shared object file: No such file or directory
module: crash	Illegal hardware instruction using torch.nn.Conv2d on aarch64 (Raspberry Pi 4)
module: crash	Basic math operations produce a "floating point exception"
module: crash	torch.Tensor.flatten Trigger Segmentation Fault when trying to provide and output named dim
module: crash	Segfault on torch.nn.functional.one_hot with large tensor on Python 3.9
module: crash	A segment fault can be triggered in fbgemm_pack_gemm_matrix_fp16
module: crash	torch.distributed crash with abort only inside if
module: crash	crash when call torch.set_num_interop_threads twice
module: crash	ipykernel crash importing torch after scipy in .ipynb file
module: crash	Reproducible "CUDA error: an illegal memory access was encountered"
module: crash	JIT model with relu+div+sgn will crash when computing the gradient
module: crash	JIT model with mean will crash when computing the gradients on cuda
module: crash	Conv2d will crash by using jit.trace
module: crash	Crash in torch.package.PackageExporter
module: crash	Segmentation fault in native_batch_norm
module: crash	Floating point exception in gather gradient computation.
module: crash	Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight
module: crash	Segmentation fault in torch.futures.collect_all
module: crash	Session of Google Colab crashes when torch.utils::SummaryWriter is called after importing torchaudio
module: crash	torch.linalg.eigh crashe for matrices of size 2895×2895 or larger on eigen and M1
module: crash	libtorch malloc cause coredump
module: crash	make_fx + aot_autograd segfaults
module: crash	functionalize and make_fx are not composable resulting in segfault and cuda error
module: crash	PyTorch crashes when running with OpenACC
module: crash	make_fx is broken for all tracing modes
module: crash	forward program terminated from __cxa_pure_virtual
module: crash	Illegal Memory Access from nonzero method when Tensor is Too Large
module: crash	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: crash	Certain import order triggers segmentation fault
module: crash	Segfault in ~PyFunctionPreHook
module: crash	[ONNX] Error when exporting adaptive_max_pool2d to ONNX
module: crash	importing open3d before pytorch causes matmul to produce a segfault
module: crash	Embedding Pytorch in C++ using pybind fails on interpreter shutdown
module: crash	operation not supported crash when initializing RPC tensorpipe
module: crash	Segfault on unloading a model
module: crash	Internal assert failed at rref_context.cpp
module: crash	test_fn_fwgrad_bwgrad_[trapezoid|trapz]_cuda_complex128 causes CUDA memory exception
module: crash	test_fn_fwgrad_bwgrad_special_ndtr_cuda_float64 fails
module: crash	Core dumped with large matmul on aarch64
module: crash	SEGFAULT on "import torch"
module: crash	SIGILL, Illegal Instruction from libtorch_cpu.so when callling backward() function in a VM.
module: crash	Segmentation Fault when importing Torch, 2021 version
module: crash	Floating point exception in mkl_vml_serv_GetMinN () on a specific computer
module: crash	Crashed with terminating with uncaught exception of type std::__1::system_error: condition_variable wait failed: Invalid argument
module: crash	Exception thrown in final autograd callback (queue_callback) not caught if not on CPU thread: terminate called after throwing an instance of 'python_error'
module: crash	broken pipe error
module: crash	Bus error (core dumped) when import torch
module: crash	Error while running FINETUNING TORCHVISION MODELS
module: crash	Segmentation fault when using CUDA with RNN
module: crash	Tensor.type() does not work with meta tensors
module: crash	Segmentation fault on Pytorch 1.8.1 + cuda 11.1 on  GTX 1050 Ti
module: crash	Segmentation fault when using torch.profiler
module: crash	Attempt to use jited torch.isnan hit internal assert
module: crash	Segmentation fault when loss.backward()
module: crash	cublasSgemmStridedBatched failure when calling grad of grad
module: crash	terminate called after throwing an instance of 'c10::Error'   what():  isTuple() INTERNAL ASSERT FAILED at "/home/wenda/libtorch/include/ATen/core/ivalue_inl.h":927, please report a bug to PyTorch. Expected Tuple but got GenericList
module: crash	torch.nn.functional.ctc_loss crash(segfault)
module: crash	segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large
module: crash	Segmentation fault encountered when using nn.MultiheadAttention with v1.7.1
module: crash	Torch  _remove_batch_dim OP out-of-bounds access
module: crash	Torch quantized_lstm_cell op out-of-bounds access
module: crash	from_blob segfaults when given CUDA pointer
module: crash	Segfault in torch.bincount
module: crash	This error occurs occasionally during the run
module: crash	segment fault for Image model training by four GPUs
module: crash	SegmentationFault when pytorch is installed from source.
module: crash	segfault during shutdown with torch1.7
module: crash	pybind11_object_dealloc error
module: crash	torch.distributions.Categorical crashes with illegal instruction
module: crash	libtorch 1.5 crashes when used on macs when using torch::max without AVX support
module: crash	device-side assert triggered - when probability > 1.0
module: crash	Segment Fault after use cusolverDnDestroy() with torch1.5
module: crash	RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8
module: crash	Providing CUDA tensor to model on CPU causes a crash
module: crash	Python autograd engine threads never terminate in Python 3.5-3.8
module: crash	pybind11::gil_scoped_release: crash on exit with daemon threads
module: crash	Seg Fault: import vaex with torch
module: crash	Python 3 build PyTorch got core dump
module: crash	Segfault when using misaligned data pointer (from joblib)
module: crash	DataLoader: Segmentation Fault (core dumped)
module: crash	sklearn and pytorch incompatibility issue
module: crash	"malloc(): memory corruption (fast)", action=3) at malloc.c
module: crash	Seg-fault in LayerNormKernelImpl
module: crash	scatter_add allows index tensor that doesn't match input size in forward pass but fails on backward pass
module: crash	Process fails with assertion error in magma-cuda100
module: crash	Assign torch.cuda.FloatTensor to List tensor
module: crash	Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results
module: crash	Libtorch with deeplabv3_resnet101 will not forward.
module: crash	Illegal instruction (core dumped) when running in qemu
module: crash	Segmentation fault when use torch::from_blob
module: crash	Importing open3d after PyTorch causes free(): invalid pointer
module: crash	Seg fault with test_rnn_retain_variables on ppc64le
module: crash	[Caffe2] compilation linker error libtbb.so.2: undefined reference to std::__exception_ptr
module: crash	[pytorch] Not handling python reload properly
module: crash	Bugs: Score Function approach in REINFORCE for PONG
module: crash	ImportError: dlopen: cannot load any more object with static TLS
module: rpc	RPC all_gather doesn't work with dynamic world size (world_size=None)
module: rpc	'CUDA out of memory' when using a GPU services for reinforcement learning in Torch rpc tutorial
module: rpc	torch.distributed.rpc.backend_registry.register_backend fails to update BackendType enum
module: rpc	Init_rpc() errors when running the test code in the TorchPRC document on two different machines
module: rpc	RPC Tutorial can not profile the  rpc operations communication between workers
module: rpc	Torch RPC on multiple nodes with GPU returns a EOF error
module: rpc	PyTorch RPC crashed when using IB
module: rpc	[Distributed: RPC] Failed to initialize RPC with >18 workers
module: rpc	RPC: wait method of Future object return 0 sometimes in rpc framework
module: rpc	RPC: Make RRefProxy callable
module: rpc	Provide error message when thread pool is exhausted in RPC
module: rpc	RPC init fails and crashes when world_size is greater than 18
module: rpc	Split up and reorganize RPC tests
module: rpc	[CTA] Let's Stamp Out Flaky Tests!
module: rpc	DISABLED test_init_rpc_twice (__main__.TensorPipeRpcTest)
module: rpc	Why is distributed RPC using the default pickler?
module: rpc	Audit exception rewrapping to ensure stack traces are preserved
module: rpc	remote failure INTERNAL ASSERT FAILED at "../torch/csrc/distributed/rpc/rref_context.cpp":389
module: rpc	DISABLED test_gpu_simple (__main__.TensorPipeCudaDistAutogradTest)
module: rpc	DISABLED test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest)
module: rpc	DISABLED test_builtin_remote_message_dropped_timeout_to_self (__main__.FaultyFaultyAgentRpcTest)
module: rpc	DISABLED test_remote_timeout_to_here_in_jit (__main__.FaultyJitFaultyAgentRpcTest)
module: rpc	Separate libtorch and non-libtorch specific files under torch/csrc/distributed
module: rpc	[AWS EC2 P3DN, EFA is enabled] Torch RPC tensorpipe/common/ibv.h:172 "": Operation not supported
module: rpc	test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky
module: rpc	test_remote_module_py_pickle_not_supported_script is flaky
module: rpc	test_forward_async_script is flaky
module: rpc	Test timeout and assertion failure in distributed/rpc/test_tensorpipe_agent
module: rpc	[deploy] Enable torch.distributed.rpc Python bindings in deploy
module: rpc	Use the same CUDA stream for all RPCs within the same dist autograd context
module: rpc	CUDA RPC stream synchronization does not work with @rpc.functions.async_execution
module: rpc	Synchronize RRef.to_here() CUDA Streams properly when the profiler is enabled
module: rpc	Support GPU/CPU communication in RPC
module: rpc	Assertion on the existence of RRef user-facing methods' docstrings
module: rpc	[RPC] rref.get_type() should have default return type of future
module: rpc	test_profiler_with_remote_builtin (__main__.TensorPipeRpcTestWithSpawn) is flaky
module: rpc	[RFC] Make RRef proxy APIs non-blocking
module: rpc	Standalone OSS RPC benchmark
module: rpc	how to save weights when using RPC framework
module: rpc	rfc: automating the switching of inputs to the device of the params
module: rpc	[RFC] Speed up python function and arg serialization in RPC APIs
module: rpc	Support DDP find_unused_parameters=True mode when combined with Pipe
module: rpc	[RFC] RemoteTensor: Use Remote Devices as Local Devices
module: rpc	[RFC] CUDA-aware future for distributed
module: rpc	Authentication for RPC
module: rpc	Future returned by RPC should print a warning message on destruction if it's not waited
module: rpc	Clean up request_callback_no_python.cpp
module: rpc	Exception raised in rpc_async context is silently handled
module: rpc	cannot call rpc.init_rpc twice within a single process
module: rpc	torch rpc cannot handle UnsupportedNodeError exception
module: rpc	[RFC] Pipeline Parallelism in PyTorch
module: rpc	Timed out RRef can still be used in subsequent RPCs
module: rpc	Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.
module: rpc	[RFC] Manage CUDA Stream in TensorPipe RPC Agent
module: rpc	[RFC] RPC BENCHMARK
module: rpc	torch.distributed.rpc package not work well with generator and lambda
module: rpc	torch rpc lose device information between rpc calls
module: rpc	Generate unique worker_id for each node in the RPC framework for the store.
module: rpc	[RPC] Should we support users _not_ calling rpc.shutdown()?
module: rpc	[RFC] Device Placement API for RPC
module: rpc	Add a launching script for RPC
module: rpc	[RFC] [RPC] Automatic retries of all requests in TensorPipe agent
module: rpc	Allow TLS to keep distributed autograd context alive
module: rpc	RemoteModule enhancements
module: rpc	CUDA error: out of memory when running tensorpipe test_cuda
module: rpc	Improve RPC test debugability
module: rpc	[RFC] Add a RPC context manager that collects/waits for all RPC futures created in scope
module: rpc	Parallelize arguments serde for RPC with TorchScript functions.
module: rpc	Torch Distributed Asynch docs with invalid ENV args
module: rpc	Allow to_here to interrupt blocking wait in the case of rpc.remote timeout
module: rpc	Update RPC doc  to recommend async user functions for non-blocking server execution
module: rpc	Clean up GIL that used to guard deleted RRef destructions
module: rpc	Clean up RPC request callback implementation
module: rpc	Let future.wait() take in an optional timeout
module: rpc	[TensorPipe] Errors in pipeWrite should clear out the future in pendingResponseMessage
module: rpc	Add CUDA callback to Python API
module: rpc	Add Distributed LR Scheduler to RPC
module: rpc	[DISCUSSION] RPC server-side ThreadLocalState
module: rpc	Make torch.rpc accept store as optional parameter
module: rpc	Add helpers to save/load RPC-based models
module: rpc	Guard Gloo and TensorPipe related code in RPC with #ifdef
module: rpc	Add name to Class Parameter()
module: rpc	Implement generic function scheduler in c10/util
module: rpc	[Design][RFC] RemoteModule API Design
module: rpc	Clarify tensor storage communication behavior in RPC
module: rpc	UserRRef should store error if it sees any and prevent subsequent usage
module: rpc	Using profiler to profile distributed autograd code can lead to misleading results
module: rpc	General Purpose Faulty RPC Agent
module: rpc	RPC API Changes for TensorPipes
module: rpc	TensorPipes RPC Agent Default Args/Result Device Mapping
module: rpc	TensorPipes RPC Agent Multiple Placement Retries
module: rpc	TensorPipes RPC Agent Message Acknowledgements
module: rpc	TensorPipes RPC Agent Listener shortcut
module: rpc	Initialize TensorPipe RPC Agent Transport
module: rpc	TensorPipes RPC Agent CUDA Support
module: rpc	Support pipelining the backward pass and optimizer.step() for distributed autograd.
module: rpc	Distributed Data Parallel for computation graphs that make RPCs in forward()
module: rpc	Make it easier to add new messages in RPC layer
module: rpc	[RFC] Add ability to get all remote parameters when constructing DistributedOptimizer.
module: rpc	[RFC] Nested scopes in autograd profiler should support RPC calls properly.
module: rpc	Implement Backend-Agnostic RPC functionality in RpcAgent
module: rpc	RPC mock mode for unit tests.
module: rpc	Reduce RPC branches for Python/Built-inOp/TorchScript
module: rpc	Don't unnecesarily send cleanup dist autograd context RPCs to other nodes
module: rpc	Possible ProcessGroup::Work::abort correctness issue
module: rpc	Support strategy to train large model that exceeds GPU mem and DRAM mem
module: rpc	Idempotency Keys for RPC Retry
module: rpc	Reuse spawned subprocesses in RPC tests
module: rpc	RPC and dist_autograd should respect no_grad mode
module: rpc	Support rpc/remote torch script call with script class/module name and class/module method name
module: rpc	Need a launch utility for Distributed RPC framework.
module: rpc	Make RRef.to_here() non-blocking
module: rpc	[RPC] Support nn.Module pickling with share memory
module: rpc	Sending sparse tensors over RPC not yet supported
module: rpc	Remove RPC internal helper that overrides the default pickler
module: rpc	[RFC] RPC timeout
module: rpc	Provide rpc, remote and dist autograd C++ APIs and register them as Prim::ops
module: rpc	Support RRef[T].__call__(*args) which invokes T.__call__(*args) on owner
module: rpc	RPC couldn't match torch.ones with requires_grad=True
module: rpc	PyTorch RPC should expose critical metrics to the application.
module: rpc	Avoid RTTI in DistEngine
module: rpc	🚀 Graceful RPCAgent termination in multi-driver scenario
module: rpc	Test re-entrant backward works with torch.distributed.autograd.backward()
module: rpc	torch.distributed.autograd.backward() should populate .grad field on Tensors by default.
module: rpc	Support Anomaly detection for distributed autograd.
module: rpc	Allow explicit gradients in torch.distributed.autograd.backward() API
module: rpc	Support implicit RRef type conversion
module: rpc	[RFC] RRef Protocol
module: rpc	Avoid sending zero grads over the wire in distributed autograd backward pass
module: rpc	DistAutogradContext should be cleaned up in case of node failures.
module: rpc	Make GloballyUniqueId a common type for both rpc and dist autograd
module: rpc	[RPC] Make ProcessGroupAgent send task non-blocking
module: rpc	[RPC] Add type annotations for RPC-related Python files
module: rpc	[RFC] RPC Based Distributed Model Parallel
module: distributions	Creating Gaussian Mixture Models with MultivariateNormal
module: distributions	Categorical Simplex constraint throws error for valid values
module: distributions	backend-friendly distributions
module: distributions	Tensor.uniform_ uses illegal argument name from
module: distributions	torch.distributions.Pareto.sample sometimes gives inf
module: distributions	Support for efficiently processing categorical distributions with varying dimensions
module: distributions	torch.poisson(torch.tensor([torch.inf)) returns 0
module: distributions	torch.distributions.categorical.Categorical samples indices with zero probability
module: distributions	Parameterisation of MultivariateNormal distribution using Cholesky decomposition of precision matrix
module: distributions	Memory allocation issues in distributions.multivariate_normal.MultivariateNormal
module: distributions	torch.dist with minus norm returns tensor(0.), while with -inf can return result
module: distributions	[bug] Internal assert failed when using pyro
module: distributions	Implement torch.distributions.Poisson.cdf()
module: distributions	The sign of torch.distributions.transforms.PowerTransform seems to be incorrect
module: distributions	torch.distributions.kumaraswamy.Kumaraswamy generates samples outside its support (0,1)
module: distributions	Default value of validate_args is set to True when passed as None in Multinomial
module: distributions	Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities
module: distributions	distributions.Beta returning incorrect results at 0 and 1
module: distributions	torch.Categorical samples indexes with 0 probability when given logits as argument
module: distributions	Add BlockWise Distribution Support to the torch.distributions Package
module: distributions	[🚀 Feature Request] pdf and sampling from Alpha-stable distribution
module: distributions	RuntimeError: Error in dlopen: libnvJitLink.so.12: cannot open shared object file: No such file or directory
module: distributions	[Feature Request] An alternative sampling routine for Dirichlet to fix Dirichlet and Beta sampling bugs
module: distributions	positive_semidefinite constraint fails on CUDA 11.7
module: distributions	Categorical fails simplex validation after its own normalisation on CUDA
module: distributions	Inconsistency between geometric distributions
module: distributions	Beta distribution behaves incorrectly for small parameters
module: distributions	KL-divergence of two Generalized Dirichlet distributions
module: distributions	Complex-Valued Gaussian distributions
module: distributions	torch.special.gammainc backward pass with respect to the first argument
module: distributions	gradcheck fails for torch.distribution.transform APIs in forward mode
module: distributions	[typing] distribution.lazy_property is not typed
module: distributions	reshape for distributions.
module: distributions	test_wishart_log_prob fails locally for me
module: distributions	Dirichlet with small concentration
module: distributions	LowRankMultivariateNormal doesn't work with 0 diagonal
module: distributions	Invalid code inrandom_ kernel
module: distributions	torch.distributions.multinomial.Multinomial (an example mistake of docs)?
module: distributions	Better Error report in torch.distribution.*.sample (when passing a non-iterable)
module: distributions	Bug: torch.distributions.mixture_same_distribution._pad_mixture_dimension
module: distributions	Add optional log_scale argument for torch.distributions.Normal
module: distributions	enumerate_support for continuous distributions
module: distributions	Split up torch.distributions docs into multiple pages
module: distributions	KL divergence between two Continuous Bernoulli is negative
module: distributions	NotImplementedError in torch.distributions
module: distributions	Add support for complex mean to the normal operator
module: distributions	Multivariate normal defined by eigendecomposition
module: distributions	torch.distributions.categorical.Categorical does not work with 0 batch size
module: distributions	Gamma distribution returns some wrong extreme values
module: distributions	Memory leak in distributions.multivariate_normal.MultivariateNormal
module: distributions	Distribution covariance property
module: distributions	Distribution cross_entropy method
module: distributions	Implement reparameterized sampling for LKJCholesky distribution w.r.t. concentration parameter
module: distributions	Add kl_divergence between Normal and Laplace distribution.
module: distributions	Missing doc for torch.distributions functions
module: distributions	Beta distribution yields wrong results for a,b >1 and x=0 or x=1
module: distributions	Feature request: Generate Sobol points directly on the GPU
module: distributions	Sparse updates to logits in distributions.Categorical
module: distributions	Wrong implementation of method log_prob in torch.distributions.negative_binomial
module: distributions	torch.distributions.Categorical unintended  log_prob gradient w.r.t probs
module: distributions	Scipy 1.7.0 may cause some test failures
module: distributions	TransformedDistribution's log_prob gradient is inconsistent w.r.t. cache_size
module: distributions	[FR] integral discrete distributions should support dtype=int64
module: distributions	Add Dirichlet Multinomial to PyTorch Distributions
module: distributions	Half Normal Log_Prob not defined for 0
module: distributions	Add numerically stable methods to torch.distributions (e.g. logcdf)
module: distributions	Add slicing to distributions
module: distributions	[FR} Implement Skellam distribution
module: distributions	Analytical metric package
module: distributions	Use unified type for distributions.constraint API
module: distributions	Multinomial without replacement produces samples that have zero probability
module: distributions	add inverse cdf for Chi-square Distribution
module: distributions	torch.multinomial example is incorrect
module: distributions	torch.distributions.half_normal.HalfNormal.cdf returns negative values
module: distributions	[Feature Request] Add support for Hidden Markov Models in torch.distributions
module: distributions	Standardized Distributions
module: distributions	Support for Multi-Categorical in torch.distributions
module: distributions	torch.multinomial with replacement=True produces inaccurate results for large number of categories
module: distributions	torch.distributions.Categorical crashes with illegal instruction
module: distributions	torch.distributions.multinomial.Multinomial cannot be used in batch
module: distributions	add a reparameterized version of inverse Gaussian distribution
module: distributions	Add support for rsample to MixtureSameFamily Distribution
module: distributions	Investigate using log1p instead of log in transformation functions(TransformationHelper.h)
module: distributions	Investigate exponential distribution improvements
module: distributions	Investigate using -cospi(u) / sinpi(u) instead of tan(pi * (u - 0.5)) in transformation::cauchy
module: distributions	Scale parameter downcasted and rounded down in pytorch.distributions.Normal
module: distributions	torch.distributions bug in RelaxedOneHotCategorical.log_prob
module: distributions	torch.multinomial is misnamed.
module: distributions	torch.bernoulli and torch.rand and torch.randint to support dtype=torch.bool kwarg (and operation of tensor.uniform_ / tensor.random_ on bool tensors)
module: distributions	Equality operator for torch.distribution.*
module: distributions	Docs for uniform_ don't make any sense
module: distributions	[feature request] make torch.multinomial behaviour compliant with rnn output dimension
module: distributions	MultivariateNormal.rsample: use eigen-decomposition when Cholesky fails
module: distributions	KL divergence for diagonal Gaussian distributions
module: distributions	Truncated normal distribution
module: distributions	torch.poisson returns floating point tensor
module: distributions	Categorical.sample too slow
module: distributions	[feature request] Multivariate normal CDF
module: distributions	Add Wishart and inverse-Wishart distributions
module: distributions	gradient of Dirichlet.log_prob gives nan
module: distributions	Supporting "cdf" for Student-T distribution
module: distributions	Detaching a distribution's log_prob to block gradients only w.r.t its parameters
module: distributions	Construction of MultivariateNormal much slower on GPU than CPU
module: distributions	[feature request] Core API for invertible/inplace and flow-like ops + memory-saving (hookless?) reversible sequential container for RevNets to allow for much larger batch-sizes in academic setting
module: distributions	[FR] Diagonal Transform for Distributions
module: distributions	Feature Request: beta cdf
module: distributions	[distribution] Support for various domain for AffineTransform
module: distributions	torch.distributions.Binomial.sample() uses a massive amount of memory
module: distributions	add stable distribution in torch.distributions
module: distributions	Provide option to use alias method in Categorical.sample()
module: distributions	Numerical instability KL divergence RelaxedOneHotCategorical
module: distributions	[feature request] build and move distributions w/ device and/or dtype
module: distributions	Confusing documentation with distributions.Categorical about logits
module: distributions	Beta Distribution values wrong for a=b---> 0
module: distributions	possible unsafety in torch.distributions.kl_divergence for Bernoullis
module: distributions	Feature request: von Mises-Fisher distribution
module: distributions	RelaxedOneHotCategorical not implementing entropy (and other abstract methods)
module: distributions	cdf in torch.distributions.bernoulli throws NotImplementedError
module: distributions	[feature request] Kumaraswamy distribution
module: distributions	[distributions] Torch distribution samplers slow on expanded parameters
module: distributions	Feature Request: Logistic Distribution
module: distributions	[feature request] Add cudaification API for distributions
module: distributions	from keyword in random_ gives error
module: distributions	Variable outputs of stochastic functions should never require grad
module: distributions	type of torch.bernoulli and torch.multinomial inconsistent
module: distributions	CUDA multinomial is limited to 2^24 categories
module: distributions	In-place bernoulli_ has more functionality than torch.bernoulli with output parameter
module: ddp	DDPOptimizer lazy compile causes shape mismatch error
module: ddp	Torch.compile crashes for UNet model when using DDP and activation checkpointing
module: ddp	torch.compile CUBLAS_STATUS_EXECUTION_FAILED
module: ddp	torch.compile CUDNN_STATUS_MAPPING_ERROR
module: ddp	Graph breaks in APEX FusedRMSNorm causes bad interaction between NCCL allreduce and cudagraph tree
module: ddp	Torch compile with DDP errors on parameterized modules
module: ddp	torch2.1.0 compile+amp+ddp cause NotImplementedError
module: ddp	[DDP + Dynamo] Tracing DDP AllReduce
module: ddp	[dynamo] Disable DDPOptimizer or error out if DDPOptimizer + static_graph is detected
module: ddp	Really slow compilation times for torch.compile causing distributed training errors
module: ddp	torch.compile graph breaks should be independent of DDP buckets
module: ddp	[optimize_ddp] moco - NameError: name 's2' is not defined
module: ddp	[DDP PT2] TypeError: convert_frame_assert.<locals>._convert_frame_assert() missing 2 required positional arguments: 'hooks' and 'frame_state'
module: ddp	[compile] DDPOptimizer + activation checkpointing not supported
module: ddp	Regressions with torch.compile + amp + ddp with recent nightly builds
module: ddp	torch.compile is not compatible with DPP with torch.nn.SyncBatchNorm.convert_sync_batchnorm()
module: ddp	DDP static graph fails for static model
module: ddp	corrupted size vs prev size error
module: ddp	[BE] move _apply_to_tensors from FSDP to torch.distributed.utils, use in _recursive_to
module: ddp	vmap + nn.SyncBatchNorm.convert_sync_batchnorm
module: ddp	Adopt full_backward_pre_hook in DDP
module: ddp	DDP overlapped optimizer: set grads to None enhancements
module: ddp	Random K compression hook in PyTorch DDP
module: ddp	nn.Embedding weights are not synced across processes with DistributedDataParallel when other parameters are present
module: ddp	several questions about pytorch DDP
module: ddp	Grad strides do not match bucket view strides
module: ddp	DDP + FSDP: Investigate behavior for nn.Module APIs
module: ddp	Torch1.10.2 is slower than torch1.9.1
module: ddp	During DDP training timm densenet121, mobilenetv2(v3) models do not save state_dict correctly.
module: ddp	DPP training incompatibility with checkpoint and detach
module: ddp	DistributedDataParallel hangs when not using GPU 0
module: ddp	[DDP] doesn't support multiple backwards when static_graph=True
module: ddp	[DDP] output_device argument appears completely unused
module: ddp	Complex support in DDP
module: ddp	Missing the time unit in duration time of DDP logging
module: ddp	Unable to use a parameter with torch.sparse_coo layout with DDP
module: ddp	Iteration # 1-offset in DDP logging
module: ddp	PyTorch gets stuck when using an NVLink/A6000 and more than two GPUs
module: ddp	DistributedDataParallel static_graph=True fails to handle unused parameters
module: ddp	PyTorch/XLA's DDP XLABackend is broken by upstream change
module: ddp	DDP Freezes w/ No Output for PyTorch Geometric GNN Multi-GPU Node Classification
module: ddp	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: ddp	[Optimizer Overlap] Parameter group support
module: ddp	[Optimizer Overlap] Proper checkpointing support
module: ddp	[Optimizer Overlap] Custom optimizer registration
module: ddp	Can't pickle model torch._C._distributed_c10d.ProcessGroupNCCL' object
module: ddp	Feature requests for optimizer overlapping
module: ddp	RuntimeError: bucket_count == per_bucket_sizes.size() INTERNAL ASSERT FAILED
module: ddp	comm hook error in BWD pass
module: ddp	[Proposal] Use batched oprations to accelerate PowerSGD
module: ddp	[DDP] Parallelize initialization collectives
module: ddp	consume_prefix_in_state_dict_if_present can not remove prefix of _metadata.
module: ddp	powersgd can not get linear growth due to  extra 2 times allreduce
module: ddp	DistributedDataParallel high peak memory usage with find_unused_parameters=True
module: ddp	TypeError: cannot pickle 'torch._C._distributed_c10d._ProcessGroupGloo' object
module: ddp	Add a section in DDP tutorial to explain why DDP sometimes is slower than local training and how to improve it
module: ddp	Investigate why tensor shapes are not populated when printing WorkNCCL for broadcast
module: ddp	Hanging Validation
module: ddp	ProcessGroupWrapper: Additional Improvements
module: ddp	Custom ProcessGroup Destructor Not Correctly Called in PT 1.10
module: ddp	DistributedDataParallel creates too many threads
module: ddp	Regression in multi-node training speed with Transformers + PyTorch
module: ddp	Optimizer Overlap: Follow up features
module: ddp	is_alias_of support for storageless tensors
module: ddp	[RFC] Gossip SGD (as a DDP Communication Hook)
module: ddp	[RFC] Cross-Process Performance Analysis: Straggler Detection
module: ddp	Support hooks-based checkpointing API with DDP
module: ddp	Grad strides do not match bucket view strides
module: ddp	API to support combined activation offloading or checkpointing
module: ddp	[RFC] Activation Checkpoint API improvements
module: ddp	Make DDP + Zero have the same communication volume as regular DDP
module: ddp	Enable DDP checkpointing tests for Gloo backend
module: ddp	DDP only syncs parameters used in most recent pass when find_unused_parameters is True.
module: ddp	Bugs related to NCCL on RTX 6000, code freezes with no output when using DistributedDataParallel
module: ddp	find_unused_parameters of DDP
module: ddp	[docs] Clarify DDP activation checkpointing support
module: ddp	[Runtime Error in ddp_pipeline.py] RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.
module: ddp	[DDP] Debug mode should ensure reduction is finished in backward pass
module: ddp	Add TORCHELASTIC_RESTART_COUNT env variable to DDP logging
module: ddp	RFC: Overlap optimizer computation with DDP/FSDP backward
module: ddp	BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation
module: ddp	[RFC] Low-level speed optimizations for PowerSGD
module: ddp	Test ZeRO on gloo backend for GPU
module: ddp	Make DDP uneven inputs work with custom buffer reduction
module: ddp	Improve DDP tutorial
module: ddp	Evaluating on single GPU (DDP)
module: ddp	[docs or feature request] torchelastic: OOM recovery / skipping batches (e.g. if inf loss or nan gradients)
module: ddp	[Poll] Support higher-order gradient computation in DDP
module: ddp	Add tests for DDP broadcast_buffers=True
module: ddp	Calling backward with create_graph on the output of a DistributedDataParallel throws error
module: ddp	getNcclVersion() in NCCLUtils.cpp handles "2.10.3" incorrectly
module: ddp	Consolidate distributed benchmark folders
module: ddp	DistributedDataParallel documentation
module: ddp	Memory leak in ddp_zero_hook.py hooks
module: ddp	[RFC] Should DDP support custom reduction logic for registered module buffers?
module: ddp	Convert unset variables in Reducer::Timer to use c10::optional
module: ddp	Remove c10d::kDefaultFirstBucketBytes
module: ddp	3 Times memory cost when loading the model to torch.nn.parallel.DistributedDataParallel
module: ddp	DDP
module: ddp	Dedupe code in functional optim classes
module: ddp	torch.nn.utils.weight_norm fails with DDP
module: ddp	DDP fails if you have multiple forward passes and a single backwards pass with find_unused_parameters=True
module: ddp	What changes we need to make in metrics calculation and visualization part when we use Distributed Data Parallel for distributed training
module: ddp	Dataloader Rerunning with num_workers=0 may give better error trace
module: ddp	Single-Process Multi-GPU is not the recommended mode for DDP
module: ddp	DDP grads dont have parity with local training when grads are undefined
module: ddp	Static graph training fails if forward is called multiple times before backward
module: ddp	[RFC] Provide an API for Structural Performance Tips in DDP
module: ddp	I meet an error assert key in deserialized_objects when I torch.load(pthname),and pth file is trained on multi gpu
module: ddp	Distributed tests don't always check the exit code of worker processes
module: ddp	[rfc] Trigger callback when backwards begins for DDP with custom autograd function
module: ddp	multiprocessing function cannot pass cuda objects use when calling inside from a DDP process
module: ddp	add tests to cover DDP on mixed dense-sparse models
module: ddp	Numpy.float64 vs native python float breaks DDP
module: ddp	DistributedDataParallel: DDP(model) hangs on non-master node
module: ddp	Add comprehensive subgroup testing to torch.distributed
module: ddp	pytorch DDP hangs at .backward() call
module: ddp	Support DDP find_unused_parameters=True mode when combined with Pipe
module: ddp	Backward hangs with DDP during training.
module: ddp	[Poll] Support DistributedDataParallel (DDP) in PyTorch C++ API (libtorch)
module: ddp	Callbacks of Futures shouldn't wait inline on another future
module: ddp	DDP error, when I use "if" branch in forward function.
module: ddp	Grad strides do not match bucket view strides.
module: ddp	"distributed" NCCL tests fail when having more than 3 GPUs
module: ddp	Improve distributed documentation for NCCL_BLOCKING_WAIT
module: multiprocessing	Operating on CUDA tensors on multiprocessing queue hangs
module: multiprocessing	Using a torch.compile'd function in a multiprocessing subprocess causes process hang on exit
module: multiprocessing	PYTORCH_NO_CUDA_MEMORY_CACHING=1 with torch.multiprocessing shared tensors seems to perform use-after-free
module: multiprocessing	IndexError: map::at with MPI CUDA collectives
module: multiprocessing	torch.einsum is stuck in mp.Process
module: multiprocessing	Multi-Threaded GraphModule / torch.fx inference raises an exception
module: multiprocessing	Unexpected behaviour with shared modules in multiprocessing on WSL2
module: multiprocessing	Multiprocessing takes forever after on .get()  with mp.Queue() (Possible Deadlock)
module: multiprocessing	Parameters of cuda module zero out when used in multiprocessing
module: multiprocessing	ray multiprocessing interference by torch import
module: multiprocessing	Torch Filename Storage hangs on "file_system" sharing strategy after in-place fill
module: multiprocessing	LayerNorm freeze processes using torch multiprocessing
module: multiprocessing	[REQUEST] - Update Multiprocessing best practices with CPU device
module: multiprocessing	Import of torch breaks standard multiprocessing
module: multiprocessing	Unexpected modification to CPU affinity of Dataloader workers
module: multiprocessing	Operations to shared tensors in the forked process could lead to silent crash
module: multiprocessing	Multiple model init using OpenMP in c++ does not speed up
module: multiprocessing	multiprocessing not work on WSL2
module: multiprocessing	[Bug][Dataloader] unable to mmap 2048 bytes from file <filename not specified>: Cannot allocate memory (12)
module: multiprocessing	ToTensor deadlock in subprocess
module: multiprocessing	DISABLED test_fs_preserve_sharing (__main__.TestMultiprocessing)
module: multiprocessing	Pipe conveys inconsistent value in GPU env
module: multiprocessing	torch.tensor obj automatically moved to shared memory upon Process launch
module: multiprocessing	Setting the cuda device when using start_processes in Jupyter on Ampere leads to CUDA reinitialization error
module: multiprocessing	Cuda tensor is zero when passed through multiprocessing queue
module: multiprocessing	module: multiprocessing SimpleQueue put cannot bigger 716 in windows.And it is not has any info.The program is blocked and does not move.
module: multiprocessing	model.load_state_dict won't work in Child process if a sufficiently large tensor was padded in the Parent (even if empty padded)
module: multiprocessing	Tensor operation hangs when used with multiprocessing
module: multiprocessing	num_worker and prefetch_factor in DataLoader do not scale
module: multiprocessing	Let torch.utils.tensorboard support multiprocessing
module: multiprocessing	Abnormal GPU memory usage when using CUDA tensors with multiprocessing
module: multiprocessing	torch.multiprocessing.spawn raise PicklingError inside a decorator
module: multiprocessing	Three memory copies of every dataloader cpu tensor
module: multiprocessing	ENORMOUS OVERHEAD from mp.get_context('spawn')
module: multiprocessing	Multiprocessing DataLoader hangs on exception inside iterator when using a simple queue and a producer thread
module: multiprocessing	multiprocessing and torch.tensor, Cannot allocate memory error
module: multiprocessing	[CTA] Let's Stamp Out Flaky Tests!
module: multiprocessing	MaybeEncodingError: Error sending result
module: multiprocessing	Error during training: falseINTERNAL ASSERT FAILED
module: multiprocessing	Multiprocessing - shared memory
module: multiprocessing	DISABLED test_fs_pool (__main__.TestMultiprocessing)
module: multiprocessing	_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training
module: multiprocessing	repeat_interleave hangs in a forked subprocess
module: multiprocessing	[In Gunicorn & multiprocessing environment] Cannot re-initialize CUDA in forked subprocess
module: multiprocessing	Multiprocessing hang and queue short circuiting
module: multiprocessing	INTERNAL ASSERT FAILED at "../aten/src/ATen/MapAllocator.cpp":323, please report a bug to PyTorch. unable to mmap 68 bytes from file </torch_530808_7992>: Cannot allocate memory (12) ---------------------------------------------------------------------------
module: multiprocessing	Add more explanation on multithreaded graph building of Autograd
module: multiprocessing	Deadlock in test_multiprocessing_spawn.py
module: multiprocessing	pytorch_linux_xenial_py3_6_gcc5_4_test may timeout during test_multiprocessing_spawn
module: multiprocessing	Sending tensors from short-lived multiprocessing process fails on Linux
module: multiprocessing	Multi-processing leaking file descriptors
module: multiprocessing	[Bug] [CUDA IPC] CUDA IPC memory cost
module: multiprocessing	Impossible to raise the limit on number of shared memory tensors
module: multiprocessing	Need workaround to support multiprocess CUDA tensor sharing on Jetson Platforms
module: multiprocessing	Multiprocessing model evaluation gets stuck
module: multiprocessing	Operating on a shared memory tensor with multiple threads hangs
module: multiprocessing	torch.multiprocessing implement SyncManager
module: multiprocessing	shared torch.tensor with multiprocesses using python Queue cause coredump
module: multiprocessing	at::globalContext().hasCUDA()'s forked tongue tells lies
module: multiprocessing	qsize not implemented error
module: multiprocessing	torch.arange() issue
module: multiprocessing	DataLoader is slow in spawned processes
module: multiprocessing	Launching two processes causes hanging
module: multiprocessing	ModuleNotFoundError: No module named '__torch__'
module: multiprocessing	Model passed through queue hangs while loading in sub process
module: multiprocessing	test_fs_pool fails
module: multiprocessing	[FR] torch.load should support loading directly to pinned/shared memory
module: multiprocessing	hangs indefinitely at os.waitpid()
module: multiprocessing	CPU Tensor with Python MP Freezing in Docker Container
module: multiprocessing	DataLoader gives "Broken pipe" error on Linux platform
module: multiprocessing	Spawn wrapper should catch BaseException, not Exception
module: multiprocessing	too large data in Queue cause dead lock in Multiprocessing
module: multiprocessing	Data loader struct pack issue(overflow)?
module: multiprocessing	mp.spawn 'args' are not clear
module: multiprocessing	torch.random.randperm stuck in multiprocess
module: multiprocessing	Code hangs when using set_start_method('spawn', force=True) in torch.multiprocessing.pool
module: multiprocessing	Torch.multiprocessing.spawn can deadlock
module: multiprocessing	Error with minimal hogwild test (multiprocessing shared memory)
module: multiprocessing	Dataloader._shutdown_workers hangs
module: multiprocessing	PyTorch multiprocessing.spawn seems slow with list of tensors
module: multiprocessing	Multiprocessing: model shared between processes hangs during copy.deepcopy
module: multiprocessing	Multiprocess DataLoader with DLPack conversion sometimes corrupts memory
module: multiprocessing	Multiprocessing map gets stuck if doing inference on loaded model
module: multiprocessing	Add start_process_in_context to torch.multiprocessing
module: multiprocessing	Problem with multiprocessing, custom __getstate__ with Tensors and forkserver
module: multiprocessing	deadlock when using mp.spawn multiprocessing
module: multiprocessing	pytorch forward hangs in multiprocess environment
module: multiprocessing	Bug in ForkingPickler for multiprocessing spawn context for shared storages on Linux
module: multiprocessing	DDP/MP not yielding nontrivial speedup
module: multiprocessing	Intel OMP multiprocessing assertion failure: Assertion failure at z_Linux_util.cpp(2338)
module: multiprocessing	pinned memory requires DeviceGuard in multi-process envs
module: multiprocessing	CUDA error: initialization error (multiprocessing) with Python 3.7
module: multiprocessing	Torch getting stuck transfering model to GPU in multiple GPU setting
module: multiprocessing	torch.multiprocessing.spawn fails when join=False
module: multiprocessing	Unexpected difference torch.multiprocessing.manager.queue and torch.multiprocessing.queue
module: multiprocessing	c10:Error: could not unlink the shared memory file
module: multiprocessing	Modules without copying  in multiprocess
module: multiprocessing	Sending CUDA tensors via queue between processes, memory of Consumer process grows infinitely
module: multiprocessing	parallel_for may hang when called in main process and then on daemon process
module: multiprocessing	How to get rid of zombie processes using torch.multiprocessing.Pool?
module: multiprocessing	Memory leak in multithreading environment when loading checkpoint
module: multiprocessing	Error in python3: double free or corruption (fasttop)
module: multiprocessing	DataLoader slow down when pin_memory=False
module: multiprocessing	Multi-gpu example freeze and is not killable
module: multiprocessing	segmentation faults when using multiprocessing_context='spawn' with large number of processes
module: multiprocessing	nn.init.orthogonal_ doesn't work with multiprocessing
module: multiprocessing	Sparse tensors can't be used in DataLoader running many workers
module: multiprocessing	Deadlock with multiprocessing (using fork) and OpenMP / PyTorch should warn after OMP and fork that multithreading may be broken
module: multiprocessing	torch.multiprocessing.pool.Pool broken
module: multiprocessing	Multiple CPU processes using same GPU model for inference
module: multiprocessing	Make torch.multiprocessing.SpawnContext usable
module: multiprocessing	Use a dill-based multiprocessing library and serialization
module: multiprocessing	DataLoader num_workers > 0 causes CPU memory from parent process to be replicated in all worker processes
module: multiprocessing	Mysterious error due to num_workers: 1
module: multiprocessing	Multiprocess Deadlock when using np.transpose and torch.stack
module: multiprocessing	Sending CUDA tensor to process, and then back, does not work
module: multiprocessing	Multiprocessing Self Test Error
module: multiprocessing	[Feature request] Add torch.multiprocessing.Pipe
module: multiprocessing	PyTorch multiprocessing using single CPU core
module: multiprocessing	MultiGPU hangs Titan Xp in multiprocessing/queue.py
module: multiprocessing	Invoking MKL in multiprocessing with importing torch causes blocking
module: multiprocessing	Multiprocessing with torch.solve hangs
module: multiprocessing	"Shared memory manager connection has timed out"
module: rocm	[ROCm] Fixes for hipblasLt for mm use case.
module: rocm	PyTorch crashes when moving tensor to GPU on Python 3.10.13 pytorch 2.1.2 + rocm5.6 (Radeon RX 6650 XT)
module: rocm	Mixed precision is not working with CudnnLSTM on rocm
module: rocm	[ROCm] Add opt-in option for inductor's layout optimisation on ROCm
module: rocm	Introduce cmake variable USE_SYSTEM_ROCM
module: rocm	[ROCm] upgrade CI to 6.0
module: rocm	Float16 scaled_dot_product_attention returns NaNs on ROCm
module: rocm	DISABLED test_bitwise_cuda_dynamic_shapes_cuda_wrapper (__main__.DynamicShapesCudaWrapperCudaTests)
module: rocm	Re-add initial Flash Attention support on ROCM
module: rocm	Skip unsupported flash attention tests on ROCM
module: rocm	DISABLED test_intra_node_comm_all_reduce (__main__.CommTest)
module: rocm	DISABLED test_relu_cuda_cuda_wrapper (__main__.TestCudaWrapper)
module: rocm	DISABLED test_as_strided_cuda_dynamic_shapes_cuda_wrapper (__main__.DynamicShapesCudaWrapperCudaTests)
module: rocm	DISABLED test_tp_transform_with_uncovered_op (__main__.TensorParallelTest)
module: rocm	[ROCm] Enabling additional UTs on ROCm
module: rocm	ninja: build stopped: subcommand failed.
module: rocm	Unable to build FBGEMM_GPU against PyTorch-ROCm: roc::hipblas not found
module: rocm	DISABLED test_silu_cuda_cuda_wrapper (__main__.TestCudaWrapper)
module: rocm	[ROCm] Add minimal inductor test to rocm-test workflow
module: rocm	Switch ROCm CI jobs to use Github Actions group linux.rocm.gpu.group
module: rocm	[ROCm] enable hipsolver backend for linalg.eigh
module: rocm	DISABLED test_addmm (__main__.ComputeBoundedTests)
module: rocm	DISABLED test_matmul_triton_kernel_benchmark (__main__.TestKernelBenchmark)
module: rocm	[ROCm] TunableOp
module: rocm	Respect user-specified USE_ROCM/USE_CUDA
module: rocm	[ROCm][hipify] Resolve any ".." in the path using os.path.abspath
module: rocm	DISABLED test_vjpvmap_nn_functional_conv_transpose3d_cuda_float32 (__main__.TestOperatorsCUDA)
module: rocm	RoCm support loop unrolling for at::native::gpu_kernel_multiple_outputs
module: rocm	torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch
module: rocm	Plan for transformer module based ROCm
module: rocm	torch.compile crash on sdxl unet compile with AMD 7900XTX
module: rocm	DISABLED test_reorder_compute_for_overlap (__main__.TestComputeCommReorderingMultiProc)
module: rocm	DISABLED test_trace_while_active (__main__.NCCLTraceTest)
module: rocm	[ROCm] Skip failing tests on ROCm
module: rocm	[ROCm] Centos stream9 pytorch image support
module: rocm	Matmul failure after dtype change on mixed AMD setup
module: rocm	[ROCm] Properly set atol/rtol settings for test_Conv2d_groups tests
module: rocm	[CI] Add inductor workflow for rocm
module: rocm	Update hipify mappings and rocm_version.h path
module: rocm	DISABLED test_cat_addmm (__main__.TestMaxAutotune)
module: rocm	Memory access fault with AMD Rocm
module: rocm	Pytorch ROCM windows builds
module: rocm	multiple AMD GPUs
module: rocm	DISABLED test_autocast_flash_attention (__main__.ActivationCheckpointingViaTagsTests)
module: rocm	Installation with rocm5.6 results in error: assert len(weights) == expected_node_count AssertionError
module: rocm	DISABLED test_redundant_clone_for_layout_convert_cuda (__main__.FreezingCudaTests)
module: rocm	DISABLED test_conv_weight_layout_convert_cuda (__main__.FreezingCudaTests)
module: rocm	DISABLED test_conv_with_as_strided_cpu (__main__.FreezingCpuTests)
module: rocm	DISABLED test_conv_stride_constraints (__main__.CPUReproTests)
module: rocm	Support AMD Ryzen Unified Memory Architecture (UMA)
module: rocm	[ROCm] support caffe2 operator export
module: rocm	The generated triton MaxPool2d kernel has poor performance on amd vega20/60
module: rocm	DISABLED test_RNN_input_size_zero (__main__.TestNN)
module: rocm	DISABLED test_RNN_dropout_state (__main__.TestNN)
module: rocm	ROCm & Windows Support
module: rocm	[ROCm] Add summary warning about no write permissions for hipify
module: rocm	Pytorch + ROCm+ Windows
module: rocm	DISABLED test_profiler_cuda_sync_events (__main__.TestProfiler)
module: rocm	DISABLED test_cross_entropy_large_tensor_reduction_sum_cuda (__main__.TestNNDeviceTypeCUDA)
module: rocm	DISABLED test_conv_with_as_strided_dynamic_shapes_cuda (__main__.DynamicShapesCudaTests)
module: rocm	DISABLED test_cross_entropy_large_tensor_reduction_none_cuda (__main__.TestNNDeviceTypeCUDA)
module: rocm	DISABLED test_cross_entropy_large_tensor_reduction_mean_cuda (__main__.TestNNDeviceTypeCUDA)
module: rocm	DISABLED test_conv (quantization.jit.test_quantize_jit.TestQuantizeJit)
module: rocm	DISABLED test_conv_transpose (quantization.jit.test_quantize_jit.TestQuantizeJit)
module: rocm	DISABLED test_observer_with_ignored_function (quantization.jit.test_quantize_jit.TestQuantizeJit)
module: rocm	DISABLED test_single_linear (quantization.jit.test_quantize_jit.TestQuantizeJit)
module: rocm	DISABLED test_nested (quantization.jit.test_quantize_jit.TestQuantizeJit)
module: rocm	DISABLED test_unary_ops (__main__.TestTensorExprFuser)
module: rocm	RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch
module: rocm	DISABLED test_conv3d_64bit_indexing_cuda (__main__.TestConvolutionNNDeviceTypeCUDA)
module: rocm	PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration
module: rocm	Installing Torch on AMD Platform Leads to Huge Docker Image
module: rocm	setup.py fails to pass USE_ROCM to CAFFE2 build
module: rocm	Quickstart notebook fails to train properly with ROCm
module: rocm	On UMA systems, pytorch fails to reserve memory exceeding the initial memory size
module: rocm	Error building Pytorch from source
module: rocm	no-duplicate-decl-specifier as a invalid compile flag for CXX in GCC
module: rocm	Ubuntu 22.04 LTS issue <built-in function load_binary> returned NULL without setting an exception
module: rocm	GPU：7900xtx Pytorch2.0.0  rocBLAS error:
module: rocm	PyTorch SGEMV is using 1 single core on AMD CPUs (very slow)
module: rocm	Enable functorch testing for rocm
module: rocm	ROCm distributed flaky on test_distributed_spawn
module: rocm	DISABLED test_forward_mode_AD_linalg_det_singular_cuda_complex128 (__main__.TestFwdGradientsCUDA)
module: rocm	DISABLED test_fn_grad_linalg_det_singular_cuda_complex128 (__main__.TestBwdGradientsCUDA)
module: rocm	Segmentation fault after trying to create a tensor with float values
module: rocm	Periodic ROCM distribtued jobs are broken
module: rocm	Moving tensor to GPU by .cuda() gets stucked when AMD Secure Encripted Virtualization (SEV) is activated
module: rocm	Steam Deck Core Dump
module: rocm	Please put back missing rocm builds of Torch Vision.
module: rocm	hipErrorNoBinaryForGpu, but reversed
module: rocm	Error on installation
module: rocm	[ROCm] build instruction is haphazard missing information unclear, build does not work
module: rocm	[RFC] Module specific workflows
module: rocm	DISABLED test_complex_half_reference_testing_as_strided_scatter_cuda_complex32 (__main__.TestCommonCUDA)
module: rocm	DISABLED test_zero_model_parallel_parameters_as_bucket_view_True (__main__.TestZeroRedundancyOptimizerDistributed)
module: rocm	Potential memory leak in Adam optimizer in AMD chips (CPU)
module: rocm	DISABLED test_post_localSGD_optimizer_parity_with_hierarchical_sgd_grad_is_view (__main__.TestDistBackendWithSpawn)
module: rocm	DISABLED test_post_localSGD_optimizer_parity_with_hierarchical_sgd (__main__.TestDistBackendWithSpawn)
module: rocm	[CTA] Let's Stamp Out Flaky Tests!
module: rocm	PyTorch for ROCm on a Supported Device Throws "hipErrorNoBinaryForGpu"
module: rocm	After XNNPack update TestXNNPACKSerDes.test_linear started to fail
module: rocm	channels_last/channels_last_3d memory format not supported for some modules on ROCm that should be supported on CUDA
module: rocm	[ROCM] elementwise kernel launch
module: rocm	istft gradcheck fails on ROCm
module: rocm	test_local_optimizer_parity (__main__.TestZeroRedundancyOptimizerDistributed) is flaky on rocm
module: rocm	tools/amd_build/build_amd.py should fail if any file fails to write
module: rocm	Add support for ROCm MIOpen miopenConvolutionForwardBias and miopenConvolutionBackwardBias
module: rocm	ROCm build documentation lacks needed dependencies
module: rocm	distributed/elastic/multiprocessing/api_test is flaky on ROCm
module: rocm	"test_events_wait" is flaky on ROCm
module: rocm	ROCm miopenStatusInternalError /MIOpen/src/sqlite_db.cpp:109: open memvfs: unable to open database file
module: rocm	[ROCm] test_gather_stress_cuda is flaky
module: rocm	test_eig_with_eigvec_cuda_float64 is flaky on ROCm
module: rocm	test_stream_event_nogil: Is the test making a wrong assumption?
module: rocm	[ROCm] test failures during 4.1 upgrade
module: rocm	at::numeric_limits is misleading
module: rocm	GPU Vendor-Agnosticism via Vulkan
module: rocm	torch.igamma error and gives wrong results on float64 ROCm
module: rocm	from torch._C import default_generator ImportError: cannot import name 'default_generator'
module: rocm	ROCm 2.1: test_gamma_gpu_sample test fails
module: bootcamp	Functionalization on inplace_views should properly reflect autograd metadata
module: bootcamp	invalid_arguments.cpp is busted
module: bootcamp	[Profiler] Snapshot CudaCachingAllocator on profile begin
module: bootcamp	[Profiler] Generic Tensor summary
module: bootcamp	Improve FSDP error msg on wrong attr access
module: bootcamp	Use c10d broadcast_object in Zero
module: bootcamp	Add TORCH_SHOW_CPP_STACKTRACES when TORCH_DISTRIBUTED_DEBUG = detail
module: bootcamp	Standalone unittests for checkpoint_wrapper
module: bootcamp	Disable TracerWarnings on NVFuser opinfo tests
module: bootcamp	[RFC] Consolidated and unified state_dict and load_state_dict hooks
module: bootcamp	User raised TypeError from __torch_dispatch__ gets turned into generic "unsupported operand type" message
module: bootcamp	Teach tools.codegen.api.translate about IValues
module: bootcamp	torch.distributed.nn.functional.all_gather: Tensors must be contiguous
module: bootcamp	[feature request] Support tensor count vector argument in torch.split
module: bootcamp	Pin dependencies + expand the current linter
module: bootcamp	Clarify test dependencies (e.g., into a test-requirements.txt file)
module: bootcamp	torch.hasnan
module: bootcamp	JIT support for torch.__version__ & str comparison operations
module: bootcamp	The jit model will fail when calling the torch.autograd.functional.jacobian with multiple inputs and setting the vectorize to true.
module: bootcamp	Feature Request: CUDA torch.histogram (and histogramdd)
module: bootcamp	Add TORCHELASTIC_RESTART_COUNT env variable to DDP logging
module: bootcamp	Feature Request: Support prelu_cuda for BFloat16
module: bootcamp	Bazel target all_tests improperly reports failures on CPU-only (non-CUDA) build
module: bootcamp	[JIT][Sym Shape Analysis] Extend analysis to extract output shape logic
module: bootcamp	Exception thrown in final autograd callback (queue_callback) not caught if not on CPU thread: terminate called after throwing an instance of 'python_error'
module: bootcamp	Consolidate ProcessGroup allgather_coalesced and allgather
module: bootcamp	Consolidate ProcessGroup allreduce_coalesced and allreduce
module: bootcamp	Consolidating NCCL version parsing functions
module: bootcamp	Add tests for DDP broadcast_buffers=True
module: bootcamp	Consolidate distributed benchmark folders
module: bootcamp	[typing] new_ones has wrong signature
module: bootcamp	Add a timeout argument to RPC shutdown()
module: bootcamp	[BE] Clean up DDP Single-Program Multi-Device Vestige in reducer.cpp
module: bootcamp	Don't use try-catch to handle overload resolution handling in torch.ops
module: bootcamp	Use linecache.lazycache in torch.fx
module: bootcamp	Remove c10d::kDefaultFirstBucketBytes
module: bootcamp	Make KernelFunction::makeFromUnboxedFunctor infer KernelFunctor from input argument
module: bootcamp	torch.fx.node.normalized_arguments does not pass normalize_to_only_use_kwargs
module: bootcamp	[FX] to_folder breaks when a directly-traced nn.Sequential is dumped
module: bootcamp	[Meta] Change default branch name to main for repos in pytorch project
module: bootcamp	Dedupe code in functional optim classes
module: bootcamp	Bootcamp Task: Add prim::to_mkldnn to convert from aten's nchw to mkldnn's nChw8c directly
module: bootcamp	Broadcasting documentation diverges from implementation
module: bootcamp	Get TensorType's device in python
module: bootcamp	Make copy_ use dispatcher
module: bootcamp	Implement simple view methods for TensorAccessor/PackedTensorAccessor
module: bootcamp	[c10d] Make broadcast_object_list accept a device parameter
module: bootcamp	[docs] Strange torch.unique function signature
module: bootcamp	Generate special clear error messages for known common misuses in TorchScript
module: bootcamp	Support a dist.group_like API
module: bootcamp	unsafe_reclaim_from_nonowning is not that unsafe
module: bootcamp	[FX] Issues with names of functions in user packages
module: bootcamp	Source location range issue for dictionary
module: bootcamp	[FX] to_folder breaks with qualified type name
module: bootcamp	torch.tensor(..., device='meta') doesn't work
module: bootcamp	[TensorExpr] Provide a safe API to request the N-th loop for a given Buf.
module: bootcamp	Structured kernels have increased TensorIterator overhead.
module: bootcamp	Optimizations to TORCH_CHECK change inlining behavior.
module: bootcamp	Regression in Python arg parser performance.
module: bootcamp	[fx] forward references in annotations do not produce correct source code
module: bootcamp	[JIT] Provide a way to access a flattened optimized graph from Profiling Executor
module: bootcamp	[TensorExpr] Add IR Verifier
module: bootcamp	[JIT/Futures] Future callbacks/then() APIs should throw the correct exception
module: bootcamp	cure unnecessary NaN values that arise from ±∞ arguments
module: bootcamp	Improve AssertionError for torch.nn.functional.pad 'replicate'
module: bootcamp	[NNC] Some ops have type promotion logic which adds extra casts & does compute in different dtype than eager
module: bootcamp	memory_format kwarg doesn't work on most factory functions
module: bootcamp	[NNC] Bugs Exposed in Binary Op Testing
module: bootcamp	[NNC] Improve "UNSUPPORTED DTYPE" error messages
module: bootcamp	Keyword-only function not allowed with TorchScript
module: bootcamp	Future returned by RPC should print a warning message on destruction if it's not waited
module: bootcamp	Add distributed examples into PyTorch CI tests
module: bootcamp	NCCL watchdog thread should log warnings about long-running GPU operations instead of silently hanging
module: bootcamp	Complex Number support for distributed
module: bootcamp	Can't initialize NCCL/GLOO process group if default process group is MPI
module: bootcamp	Don't query current device on stream construction
module: bootcamp	Consider cache effects in Timer
module: bootcamp	Useless Exception traces when DataSet timing out
module: bootcamp	Switch C10_EXPORT_CAFFE2_OP_TO_C10 to new operator registration API
module: bootcamp	Reduce number of stack frames used up by dispatcher
module: bootcamp	Do not call nullptr deleter in at::fromDLPack (dlpack)
module: bootcamp	Implement map-style caching DataSet as PyTorch build-in DataSet.
module: bootcamp	[jit] Support NamedTuple in tracing
module: bootcamp	Add a function to convert SyncBatchNorm layers back to BatchNorm Layers
module: bootcamp	Encoding dimension argument for F.one_hot
module: bootcamp	RemoteModule enhancements
module: bootcamp	Print values (but not strings) when STRIP_ERROR_MESSAGES is defined
module: bootcamp	[RFC] Add a RPC context manager that collects/waits for all RPC futures created in scope
module: bootcamp	Add Distributed LR Scheduler to RPC
module: bootcamp	[JIT] torch.tensor needs a Tensor overload
module: bootcamp	When TorchScripted module has bad type annotation you get bad error message
module: bootcamp	Support Slicing of ModuleList during JIT model tracing/scripting
module: bootcamp	Guard Gloo and TensorPipe related code in RPC with #ifdef
module: bootcamp	The pytorch's graph is lack of common names for nodes
module: bootcamp	Make it an error to def() an operator multiple times
module: bootcamp	Don't unnecesarily send cleanup dist autograd context RPCs to other nodes
module: bootcamp	Improve cuda OOM message
module: bootcamp	Distributed hangs on process termination with world_size=1
module: bootcamp	Support DataParallel with PackedSequence
module: bootcamp	Add torch.version.nccl
module: bootcamp	CPU MaxPool2d is very slow
module: bootcamp	[distributed] all_gather on a List of Tensors directly
module: bootcamp	Gloo scatter gives wrong result for stride != 1
module: bootcamp	Check PyTorch version when initializing process groups
module: bootcamp	Add gatherv/allgatherv primitives to support non-equal contribution
module: bootcamp	Redundantly saving sizes of SavedVariables in autograd Function
module: bootcamp	Add support for tuple type deduction in C++ custom operators
module: bootcamp	torch.Tensor.cpu talks about the object being "on the correct device"
module: bootcamp	improved assert message in the case of "CUDA error: device-side assert triggered"
module: bootcamp	Implement numpy.random.choice equivalent
module: bootcamp	AvgPool2d doesn't test if kernel is smaller than input size
module: bootcamp	No test coverage for kwargs of AvgPool2d and AvgPool3d
module: bootcamp	torch.bmm doesn't support CUDA uint8 (byte) tensor
module: bootcamp	Sending CUDA tensor to process, and then back, does not work
module: bootcamp	[Feature request] Add torch.multiprocessing.Pipe
module: bootcamp	torch.jit.trace(network, data) fails if data is an OrderedDict
module: bootcamp	Delete obsolete THCDeviceTensor::downcastOuter / THCDeviceTensor::downcastInner functions
module: bootcamp	Exposing CuDNN benchmark strategy selection
module: bootcamp	Expose optimizer options as attributes when there's a single param group
module: complex	Convolution NN for complex numbers and more special functions
module: complex	nansum with complex support
module: complex	Error when calculating the Jacobian of torch.conj using forward-mode differentiation
module: complex	Custom FFT implementation returns unexpected results when using torch.compile
module: complex	Incorrect strides and accuracy when combining torch.compile with op(out=out) having complex number outputs, test_ops::test_out is bugged
module: complex	Inconsistent any( ) between cuda and cpu - Incorrect complex to bool conversion
module: complex	1/torch.inf produce inconsistent results
module: complex	Conversion Error to ComplexDouble on MPS
module: complex	Torch.compile Error: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'.
module: complex	vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division
module: complex	torch.view_as_real(tensor) should return nn.identity(tensor) if its not complex instead of raising an error
module: complex	DistributedDataParallel doesn't work with complex buffers
module: complex	Type conversion between float/complex
module: complex	Incompatibility with complex tensors
module: complex	logical_xx operations trigger INTERNAL ASSERT FAIL when input is complex tensor on cuda and other is on cpu
module: complex	It seems that torch.Tensor.addmv and torch.Tensor.addr will check some inputs' dtype if and only if in backward()
module: complex	Changing behavior of module.to() to better support mixed real- and complex-valued parameters
module: complex	RuntimeError: view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16
module: complex	jacfwd and jacrev are fundamentally broken for complex inputs
module: complex	func.jacrev() should be implemented as func.jacfwd().mT.contiguous()
module: complex	svd triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode
module: complex	Segfault when running torch.atan2
module: complex	Torchscript troubles with complex values. RuntimeError: isInt() INTERNAL ASSERT FAILED
module: complex	[Inductor] support complex dtypes
module: complex	Different behavior for complex numbers operations with numpy
module: complex	Add complex support for SparseAdam and LBFGS optimizers
module: complex	Complex-Valued Gaussian distributions
module: complex	Complex addition result in NaN when it shouldn't
module: complex	Cloning conjugate tensor in torch_dispatch context produces non equality.
module: complex	test_sparse_matmul_cpu_complex128 fails on my local copy
module: complex	BatchNorm for complex tensor
module: complex	DISABLED test_non_contiguous_tensors_nn_ConvTranspose1d_cuda_complex32 (__main__.TestModuleCUDA)
module: complex	torch.stft does not normalize non-rectangular windows correctly
module: complex	[complex] dropout and it's variants should support complex tensors
module: complex	torch.svd_lowrank fails for complex matrices
module: complex	[chalf] reference_testing: low quality test for fast growing ops
module: complex	FFT operators are not supported on MPS device
module: complex	complex abs strides are wrong on empty tensors and tensors with 1 dimension
module: complex	torch.stack test_conj_view and test_neg_view are failing after 77043
module: complex	Inconsistent results between Pow and Float Pow with their numpy references for complex types
module: complex	cumprod, prod will backward fail if dtype argument is different than the dtype of input tensor
module: complex	addr, baddmm, dist, l1_loss will backward fail when input tensors have different dtypes
module: complex	torch.addmv backward fails
module: complex	[jiterator] perf regression when jiterating few ops for complex dtype
module: complex	at::real and at::imag as methods
module: complex	[jiterator] Jiterate Complex Ops
module: complex	ComplexHalf Coverage Tracker
module: complex	storage does support complex32 tensor
module: complex	Add support for complex mean to the normal operator
module: complex	Many APIs INTERNAL ASSERT FAILED when promoting complex32 dtype
module: complex	torch.nn.functional.{instance, batch}_norm trigger INTERNAL ASSERT FAILED when input is empty tensor with complex32
module: complex	NCCL Backend does not support ComplexFloat data type
module: complex	Feature request: Add complex support to torch.nanmean
module: complex	Discussion of TorchQuantum and QuantumNAS
module: complex	Feature request: [STFT] Add warning message if signal length is not a multiple of hop_length in torch.stft
module: complex	out= variant for conj unary operator
module: complex	Casting real parameter to complex during forward produces warning on backward
module: complex	Feature request: Grid sample on complex tensors with float grid
module: complex	Feature request: Complex Number Support for Special Functions
module: complex	torch.stft - fill_cuda not implemented for ComplexHalf
module: complex	Failing test_neg_view_nn_functional_embedding_cuda_float64
module: complex	torch.nn.functional.l1_loss fails gradgradcheck for complex inputs
module: complex	[numpy] Add iscomplexobj and isrealobj
module: complex	Complex recurrent layers produce NaN as grad
module: complex	Sign in slogdet is set to requires_grad = False even when using complex numbers.
module: complex	Conjugate fallback dispatch key should be per-backend
module: complex	Support matrix operations between complex and real tensors
module: complex	Incorrect dtype cast for binary ops w/ mixed ComplexFloat + Double operands
module: complex	Some ops throw runtime errors when called with complex tensors that require_grad but work when requires_grad=False
module: complex	"_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'ComplexFloat'
module: complex	complex128 autograd failures on PPC
module: complex	torch.view_as_complex() does not work when storage_offset is odd
module: complex	Missing complex autograd support for some operators
module: complex	Failure in complex CUDA numerics tests for sigmoid
module: complex	RuntimeError: "max_cuda" not implemented for 'ComplexFloat'
module: complex	DataParallel (broadcast_coalesced) with complex tensors yield real views
module: complex	torch.sqrt for negative values should either return complex tensors or clearly throw a domain error/warning
module: complex	torch.pow returns incorrect value for 0^0j
module: complex	Conjugate gradient Descent, and Linear operator are not present in pytorch.
module: complex	Add complex support for torch.unique
module: complex	Inconsistent out= behaviour on advanced indexing operations.
module: complex	Add autograd tests to verify correctness for R -> C cases
module: complex	Reciprocals of complex tensors with infinities are different from NumPy.
module: complex	Implementation of many complex functions is fast but inaccurate in libc++
module: complex	Inconsistent result between NumPy and PyTorch for 0^(z) when Re(z) < 0
module: complex	Complex->Real cast is a warning, calling real or imag on non-complex tensors is an Error.
module: complex	Errors when coercing complex numbers of various sizes
module: complex	Cannot print 32-bit complex tensors
module: complex	Unknown failure for test_reference_numerics_sinc_cpu_complex64
module: complex	[complex] torch.abs: does not match numpy
module: complex	[complex] torch.{exp}: does not match numpy
module: complex	Inconsistent complex results with NumPy when computing non-positive power of 0
module: complex	Activation functions for complex tensors
module: complex	Loss functions for complex tensors
module: complex	Complex Number support for torch.nn modules
module: complex	Complex backward returns NaN values
module: complex	Complex Number support for distributed
module: complex	Complex and real results do not agree when computing reciprocal or pow(-1) of 0
module: complex	ComplexHelper.h contains non-inline functions
module: complex	cblas_gemv is not being used for gemv on complex on CPU
module: complex	MacOS CPU torch.tan and torch.tanh do not compute some values properly
module: complex	For the same complex dtype and same value, comparing a PyTorch tensor with a NumPy array results in False
module: complex	torch.abs(complex) is divergent from NumPy on vectorized NaN values
module: complex	torch.tan(complex) on CUDA doesn't handle nonfinite values properly
module: complex	Inconsistent behavior between numpy.exp and torch.exp on CPU for complex numbers
module: complex	Current implementation of c10::complex does not support being used in shared memory
module: complex	Problems implementing complex support for acosh, asinh, tanh
module: complex	The MacOS compiler is generating illegal instruction for the division of c10::complex
module: complex	Implement torch.erf for complex dtypes
module: complex	equal_nan keyword not implemented for complex torch.isclose
module: complex	Wrong results for multiplication of non-finite complex numbers with real numbers
module: complex	Comparison ops for Complex Tensors
module: complex	New dtype ComplexPolarFloat (phasor)
module: complex	Method/constructor which takes as input angle and magnitude and returns a complex tensor
module: complex	documentation for adding a new type via C++ extensions
module: serialization	In BufferAdapter, file size in miscalculated if current cursor position is not 0
module: serialization	Make "torch.load" multi threaded process
module: serialization	Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'>
module: serialization	Add a function to torch.nested to create nested tensors from a buffer and sizes
module: serialization	KeyError during model export while using "newer" data types
module: serialization	torch::serialize::OutputArchive::save_to crash if save on C:\
module: serialization	Heap-buffer-overflow during tensor unpickling
module: serialization	torch._dynamo.export produces object that is not pickleable
module: serialization	torch.save() fails if path contains multibyte characters
module: serialization	Issue with loading similar checkpoints in a distributed fashion
module: serialization	raise RuntimeError faster when loading an object with a torch CUDA tensor on a CPU-only machine
module: serialization	There is a memory leak in torch.load
module: serialization	torch.utils._content_store will deduplicate storage with identical contents; may be problematic for mutation
module: serialization	Unable to load MultiStepLR with torch.load(weights_only=True)
module: serialization	how can i load seperate pytorch_model.bin?
module: serialization	consider bumping DEFAULT_PROTOCOL
module: serialization	PyTorchFileWriter should drop the GIL while writing files
module: serialization	Bitwise-perfect method for (de)serializing tensors in base64
module: serialization	torch.load(..., map_location="cuda:0") allocates memory on both CPU and GPU
module: serialization	Support for saving multiple storages/tensors that view same data as different dtypes
module: serialization	torch.load() cannot load data saved at non-zero position in a file (failed finding central directory)
module: serialization	torch.save throws ValueError: ctypes objects containing pointers cannot be pickled
module: serialization	Saving and loading from physical storage
module: serialization	when distribute training  load pretrain model error
module: serialization	Redundant info are saved when using torch.save to save part of torch.tensor
module: serialization	ValueError during yaml.dump(dtype)
module: serialization	torch.multiprocessing.spawn raise PicklingError inside a decorator
module: serialization	[RFC] Allow device override during Tensor unpickling without torch.load
module: serialization	Pytorch can't process special unicode
module: serialization	RuntimeError: [enforce fail at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\caffe2\serialize\inline_container.cc:300]
module: serialization	large model, low memory: need torch.load that loads one submodule at a time
module: serialization	torch.package unpickling transforms: ModuleNotFoundError: No module named 'torch._C._linalg'; 'torch._C' is not a package
module: serialization	Create a nested directory while saving objects using torch.save
module: serialization	_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training
module: serialization	loading large model not finished after 16 hours
module: serialization	Non-persistent modules
module: serialization	Plain Tensor serialization does not save the content of __dict__.
module: serialization	pytorch serializes entire tensor when you try to pickle a slice
module: serialization	It' so strange!!!
module: serialization	Rename C++ Serialization APIs
module: serialization	RFC: multi-part torch.load/torch.save to support huge models and/or low CPU memory
module: serialization	AttributeError: module 'torchvision.models' has no attribute 'load_model'
module: serialization	Uncleared memory use after torch.load()
module: serialization	Torch.save with zip serialization causes memory bloat
module: serialization	Serialising torch.bool generates a warning about np.bool being deprecated.
module: serialization	Serialization map_location silently ignores xla/mlc/meta (any serialization mechanism that skips storage)
module: serialization	Refactor serialization tests to use device parametrization
module: serialization	Calling torch.save() twice writes a corrupted file
module: serialization	jit.load error when changed Folder Name
module: serialization	torch.load non backwards compatible on Transformer between 1.8.1 and 1.9.0
module: serialization	torch.load with dill is unable to unserialize from buffer
module: serialization	Save checkpoint error
module: serialization	Torch.save doesn't make new file
module: serialization	Unknown builtin op: torchvision::nms in LibTorch
module: serialization	I meet an error assert key in deserialized_objects when I torch.load(pthname),and pth file is trained on multi gpu
module: serialization	torch.load is very slow with gzip.open
module: serialization	torch.load with Exception
module: serialization	Model loaded from full model state checkpoint does not run on multiple GPUs (device mismatch)
module: serialization	[NIT] Unpickler persistent_load monkey patch should fall back to original persistent_load
module: serialization	pickle is a security issue
module: serialization	torch.load(..., weights_only=True) currently raises a Deprecation warning + [proposal] weights_only=True should become default for safe legacy-loading pickles
module: serialization	Make TorchBind object appear in state_dict
module: serialization	ModuleNotFoundError: No module named '__torch__'
module: serialization	cudnn cannot be pickled by cloudpickle
module: serialization	[bug][libtorch] torch::load seeks to the start of the input stream before reading
module: serialization	torch.save() fails when attempting to save to mounted drives
module: serialization	[FR] torch.load should support loading directly to pinned/shared memory
module: serialization	torch.save writes file even when pickle fails
module: serialization	LibTorch cannot load PyTorch exported model
module: serialization	Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2
module: serialization	torch.save has a maximum size regardless of RAM
module: serialization	Tensor subclasses lose type when pickling
module: serialization	Add support for reading the whole file in from_file
module: serialization	Multiple torch.load in one file
module: serialization	Request: Add PyTorch version to state dicts
module: serialization	Make torch.Generator picklable
module: serialization	torch.load(.., map_location='cpu') fails when unserializing cuda tensors on a cpu-only device serialized with pickle
module: serialization	Add support for user defined types in serialization in libtorch
module: serialization	Proper testing of nn.Module loading backward compatibility
module: serialization	KeyError: 'track_running_stats' in batchnorm.extra_repr
module: serialization	Option to allow loading state dict with mismatching shapes.
module: serialization	torch while loading weighs found runtime error on storage has wrong size: expected 4254413747647032608 got 1024
module: serialization	Adding model metadata in TorchScript model file
module: serialization	torch.save incompatible with lzma file
module: serialization	Saved model behaves differently on same data
module: serialization	Multiprocess DataLoader with DLPack conversion sometimes corrupts memory
module: serialization	Models saved in C++ LibTorch with torch::save, cannot be loaded in python using torch.load
module: serialization	Torch serialization does not restore tensors properly when custom __reduce__ is defined
module: serialization	Problem with multiprocessing, custom __getstate__ with Tensors and forkserver
module: serialization	Serialization inconsistency with pickling tensors breaks caching
module: serialization	'torch.load' report 'bad pickle data'
module: serialization	save model definition file
module: serialization	Sending sparse tensors over RPC not yet supported
module: serialization	torch.save/load shows raw path on the pickle_module arg
module: serialization	Make torch.save serialize a zip file
module: serialization	load_state_dict on CPU first
module: serialization	Serialization does not work for quantized modules
module: serialization	Use a ScriptModule on GPU that was saved from CPU
module: serialization	Allow incompatible shapes in load_state_dict(strict=False)
module: serialization	torch.{save,load} data corruption when serializing a Module with __{get,set}state__
module: serialization	Add support for serializing Mkldnn Tensor
module: serialization	torch.save also saves docstrings into pickle for some reason
module: serialization	Statically make __setstate__ set all attributes/parameters
module: serialization	failed to load model which is saved as text format(pickle_protocol=0) instead of binary format
module: serialization	UnpicklingError when trying to load multiple objects from a file
module: serialization	The documentation to Module._version isn't visible.
module: serialization	Should be a way to unpickle an object with a torch cuda tensor on a CPU-only machine when using plain "pickle"
module: serialization	[BUG]: unstable happend in saving model.
module: serialization	batch_sampler/test_worker_seed intermittently fails with address already in use on OS X
module: serialization	batchnorm2d  track_running_stats
module: serialization	Well documented, safe method to deserialize model parameters from untrusted sources
module: serialization	Saving model with runtime code changes
module: serialization	Feature Request: Distributed send arbitrary objects
module: convolution	[discussion] Route pointwise Conv1d/Conv2d to matmul?
module: convolution	Optimize cudnn_convolution_out to Reduce Unnecessary Memory Allocation and Copy
module: convolution	[Bug] Big difference between the output of Conv float precision and double precision
module: convolution	Runtime Errors with convolution_backward_out When Handling Optional Bias Gradient
module: convolution	DISABLED test_noncontiguous_samples_nn_functional_conv3d_cuda_complex64 (__main__.TestCommonCUDA)
module: convolution	Contradictory Error Message for stride Argument in torch.conv_transpose3d()
module: convolution	torch.compile operation benchmark result is poor
module: convolution	Check for output_padding <= stride/dilation in ConvTranspose1d
module: convolution	torch.compile uses more memory when using less parameters
module: convolution	Can group convolution support other grouping methods?
module: convolution	Performance bugs exists in multiple convolution operations(e.g., Convtranspose2d) when useing the groups argument
module: convolution	cuDNN doesn't support convolutions with more than INT_MAX elements and native kernel uses too much memory
module: convolution	aten::cudnn_convolution chooses different conv implementation given the same inputs.
module: convolution	Unable to find an engine to execute when using pip to install but not with conda
module: convolution	Conv2d error on M1 mac, RuntimeError: NNPACK SpatialConvolution_updateOutput failed
module: convolution	Conv2d is not deterministic when input tensor has different strides
module: convolution	We don't have an op for vulkan_prepack::conv2d_clamp_prepack but it isn't a special case.
module: convolution	Conv1d: NNPACK SpatialConvolution_updateOutput failed when batchsize or padding is too large
module: convolution	When using libtorch v1.10.2, calling at::slow_conv_dilated3d directly returns wrong results on cpu backend
module: convolution	Need "valid" and "same" padding mode for convTranspose2d
module: convolution	test_conv_backend tests OOMing in 10.2 slow_gradcheck CI
module: convolution	scripted fft Convolutions are faster than nn.Conv1d with large kernels
module: convolution	Heap corruption in slow_conv_transpose3d
module: convolution	Floating point exception in slow_conv3d
module: convolution	Sporadic convolution error with dilation=0
module: convolution	cannot convert to channels last format for conv2d conv3d hybrid model
module: convolution	Process hangs after calling conv2d() in pytorch 1.11.0 with CUDA 11.3
module: convolution	Initial integration of ZenDNN as backend into PyTorch
module: convolution	RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
module: convolution	Forward AD convolution fails for the empty backend
module: convolution	conv3d has numerical issue where same input produces output that are not bit-wise identical
module: convolution	Efficient very large (e.g., 31x31) depth-wise convolution
module: convolution	Group convolution slower than manually running separate convolutions in CUDA streams
module: convolution	Conv3D consumes lots of memory on Mac with Apple Silicon
module: convolution	Floating point exception in _nnpack_spatial_convolution
module: convolution	N-dimensional Convolutions
module: convolution	Can't forward pass conv2d with kernel_size=1, and padding=1
module: convolution	Training grouped Conv2D is slow
module: convolution	conv3d padding=same gradgradcheck fails on CUDA
module: convolution	Optimization: convolution_backward doesn't always need to call .contiguous on certain inputs
module: convolution	Conv2d kernel performance regression on CPU since PyTorch 1.9
module: convolution	Forward results vary depending on batch size on A100 machine
module: convolution	Significantly difference in execution time when convolution is run as nn.Conv2d and as nn.Sequential
module: convolution	Is there a bug in transposed convolution？
module: convolution	[BUG] ConvTranspose with out_channels=1
module: convolution	nn.conv1d padding='same'
module: convolution	[Feature Request] Any plan to add 'Sparse Convolution' as default nn module?
module: convolution	F.conv2d: confusing error message when using uint8 input instead of float32
module: convolution	Add Convolution support for lazy tensor
module: convolution	[feature request] torch.nn.Conv3d on tensors having more than 5 dimensions
module: convolution	Can I set the algorithm of torch.nn.Conv to CUDNN_CONVOLUTION FWD_ALGO_GEMM by myself?
module: convolution	Unknown type name '__torch__.torch.classes.metal.Conv2dOpContext'
module: convolution	Quantized conv2d with dilation and groups much slower than float32
module: convolution	NaN values on torch.nn.functional.conv2d (aarch64)
module: convolution	[cuDNN v8] Improve cuDNN convolution v8 API error reporting
module: convolution	[cuDNN v8] Extend current cuDNN convolution v8 API binding to support conv-bias-activation fusion
module: convolution	[cuDNN v8] Extend current cuDNN convolution v8 API binding to support cuDNN benchmark
module: convolution	[cuDNN v8] Extend current cuDNN v8 API binding to support convolution backward and transposed convolution forward
module: convolution	Add the features like Keras
module: convolution	Circular padding in Convolution layers should not only be wrap for once.
module: convolution	Reflect padding_mode should be supported for Conv3d
module: convolution	Silent incorrect running with zero padding for Conv1d
module: convolution	Error message regarding Padding of Conv2d needs improving
module: convolution	16 bit accumulation for Convolution
module: convolution	Out variants for Convolution ops
module: convolution	RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
module: convolution	Exceedingly different F.conv2d outputs on cuda/cpu
module: convolution	[proposal] Way to export/display the convolution algorithm found with cudnn.benchmark = True + allow to override algo choice with conv/matmul module/function algo/hint arguments
module: convolution	Conv3D error : CUDNN_STATUS_INTERNAL_ERROR
module: convolution	Weird behavior in Conv2d padding when changed to 'reflect'
module: convolution	Calling emptyCache in Conv_v7.cpp causes performance degradation
module: convolution	Provide a mechanism to limit the workspace size of cudnn convolution
module: convolution	CUDA error: illegal memory access Conv3d
module: convolution	cudnn convolution modifies the input Tensor metadata inplace when it tries to .resize_() it
module: convolution	conv_transpose3d returns different result when the input and kernel are mkldnn tensors
module: convolution	Is nn.Conv2d equivalent with Unfold + Matrix Multiplication + Fold ?
module: convolution	ConvTranspose1d groups=channels is very slow!!!
module: convolution	Padding mode for ConvTransposeNd
module: convolution	RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.
module: convolution	Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D
module: convolution	ConvTranspose1d layer behaviour under different channel numbers
module: convolution	Conv3d with specific kernel size outputs inconsistent results between FP16 and FP32 in V100 GPU
module: convolution	Wrong conv2d output on GPU when kernel has many zeros
module: convolution	Performance bug with convolutions with weights and inputs of similar spatial size
module: convolution	addition of attention based techniques to pytorch
module: convolution	Spurious negative output in convolution of positive tensors
module: convolution	Conv2d: Inconsistent results on Raspberry Pi 3B
module: convolution	[FR] F.pad support syntax sugars for specifying the padding amount
module: convolution	nn.Conv(n)d constructor doesn't check for the number of kernel dimensions
module: convolution	conv2d Memory usage is too large； pytorch 1.1.0
module: convolution	NNPACK condition should be changed (ARM processors)
module: convolution	Conv2D 2x~20x slower than Tensorflow when channel count is small
module: convolution	Fan out calculation broken for group (depthwise) convolution
module: convolution	model use dilated conv backward in v1.1.0 is ~3x slower than in v0.4.1 on 1080Ti
module: convolution	[c++] torch::conv2d() expected output_padding to be a single integer value or a list of 3 values
module: convolution	Unify tensor shape formatting in shape checks
module: convolution	Double backward 3 times slower for conv2d with padding = 1
module: convolution	Eigen Tensor library for convolutions on CPU
module: convolution	Know which function is used by conv and force to use a function
module: convolution	Training CNNs with deconvolution
module: convolution	Batched Conv2d for sequence data
module: convolution	Slow convolution with large kernels, should be using FFT
module: convolution	FP32 depthwise convolution is slow in GPU
module: convolution	Conv2d layers should accept 4-tuple for padding argument
module: convolution	depthwise convolution are slow on cpu
module: convolution	[feature request] Singular values and spectral norm for convolutional layers
module: convolution	Non-Zero Padding in Convolution Module
module: convolution	Specify out= argument to convolution
module: convolution	[feature request] convtbc with group convolution
module: convolution	OOM Exception when using torch.nn.grad.conv2d_weight (apparently because CuDNN backwards is not used)
module: convolution	Conv3D can be optimized for cases when kernel is spatial (probably)
module: convolution	CUDNN_STATUS_INTERNAL_ERROR when training with conv3d
module: convolution	[Feature Request] Implement "same" padding for convolution operations?
module: c10d	gather_object does not work with ThreadPoolExecutor
module: c10d	Timeout of the coalesced operation cannot be detected by the watchdog thread
module: c10d	Native c10d_functional collectives on inductor + CUDAGraphTrees generate wrong results
module: c10d	P0: Improve failure trace back when crashed to identify the cause of a crash and the ranks that the crash, output the real traceback at last.
module: c10d	P0: Logging Granularity checks fixes across torch.distributed + torchelastic launcher
module: c10d	P0: Integrate distributed logger with TORCH_LOGS, make sure upper level library can't override distributed LOG level.
module: c10d	[PTD] Logging Improvements Main Task
module: c10d	NCCL p2p ops hung after communicator aborts
module: c10d	[c10d] fix functional collective reduce op naming convention
module: c10d	NCCL ISend is not asynchronous
module: c10d	torch compile error with SyncBatchNorm
module: c10d	Refcount problem for torch.distributed.Store objects defined in Python
module: c10d	distributed.batch_isend_irecv() crash when send/recv refers to itself
module: c10d	Improve collectives fingerprinting
module: c10d	Desync debugger encounters traceMap error
module: c10d	[PTD] dist.barrier() unreliable when using collectives from multiple threads.
module: c10d	Support datatype argument for torch.distributed.all_gather() (And the whole distributed module)
module: c10d	[Distributed] Destruction order fiasco in ProcessGroupNCCL workCleanupLoop()
module: c10d	Add gloo support for all_to_all
module: c10d	ProcessGroupNCCL watchdog can't catch NCCL comm initialization issues
module: c10d	Distributed collective ops fail in inference_mode for CPU-only
module: c10d	[Feature] Dispatching PyTorch Distributed Collectives
module: c10d	Gloo DDP SocketTimeout error on Windows
module: c10d	Multiple GPUs get  "errno: 98 - Address already in use"
module: c10d	c10d all_gather aborts with Signal 8 (SIGFPE) when tensor.numel() == 0
module: c10d	Uneven and/or Dynamically sized collectives
module: c10d	RuntimeError: Interrupted system call when doing distributed training
module: c10d	Improvements to ProcessGroupGloo monitored_barrier
module: c10d	RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Timeout waiting for key: default_pg/0/0 after 1800000 ms
module: c10d	Message exchange failure when perform alltoallv (cpus)
module: c10d	[c10d] Async object-based collectives
module: c10d	[BE] Update ProcessGroupWrapper tests to test other collective message
module: c10d	Distributed Store get doesn't work well with add
module: c10d	memory leaking when doing all_to_all_single communication
module: c10d	Add TORCH_SHOW_CPP_STACKTRACES when TORCH_DISTRIBUTED_DEBUG = detail
module: c10d	Can't pickle model torch._C._distributed_c10d.ProcessGroupNCCL' object
module: c10d	Pytorch return TCPStore( RuntimeError: Connection reset by peer)
module: c10d	Object-base collectives create tensors at unexpected devices
module: c10d	Multi-GPU distributed training reports errors
module: c10d	Handle noncontiguous inputs in distributed backend layer
module: c10d	Incorrect results for torch.distributed.gather for tensor created from permuted NumPy array
module: c10d	RuntimeError: Unconvertible NCCL type Short when sending torch.cuda.ShortTensor.
module: c10d	My_rank in the implementation of torch.distributed.scatter_object_list should be global
module: c10d	Supports for dist.send/dist.recv sending and recving torch.shorttensor
module: c10d	[c10d] gather/scatter inconsistency behavior on non-dst/src rank
module: c10d	processes hang when executing cross-machine asynchronous P2P communication on NCCL backend
module: c10d	TypeError: cannot pickle 'torch._C._distributed_c10d._ProcessGroupGloo' object
module: c10d	test_broadcast_coalesced_nccl fails on A100 GPUs
module: c10d	torchrun: Hostname/endpoint mismatch not handled
module: c10d	Investigate why tensor shapes are not populated when printing WorkNCCL for broadcast
module: c10d	torch.distributed.nn.functional.all_gather: Tensors must be contiguous
module: c10d	make c++ logger preamble meaningful
module: c10d	torch.distributed hangs at barrier()
module: c10d	ProcessGroupWrapper: Additional Improvements
module: c10d	Custom ProcessGroup Destructor Not Correctly Called in PT 1.10
module: c10d	torch.distributed.new_group() should consolidate the type of rank list
module: c10d	halt or exit function implementation
module: c10d	[c10d] destruction of Store objects
module: c10d	[RFC] Cross-Process Performance Analysis: Straggler Detection
module: c10d	Slowdown in torch.distributed.new_group when scaling to large clusters.
module: c10d	Distributed broadcast fails with simple GPU tensor on Windows + GLOO
module: c10d	RuntimeError: Tensor must be CUDA and dense when calling all_gather_object even though there is no tensor in the object.
module: c10d	[c10d] have a way to determine two ranks on the same hosts or not
module: c10d	RuntimeError: Operation not supported
module: c10d	[DDP] Verify ignored parameter names in debug mode during init
module: c10d	[distributed] Restructure our distributed collective tests
module: c10d	ProcessGroupNCCLTest.testSequenceNumInit is failing on 4xlarge
module: c10d	Consolidate ProcessGroup allgather_coalesced and allgather
module: c10d	Consolidate ProcessGroup allreduce_coalesced and allreduce
module: c10d	Consolidating NCCL version parsing functions
module: c10d	[Poll] Support higher-order gradient computation in DDP
module: c10d	[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend
module: c10d	Seeing unit test failures for ProcessGroupShareTensorTest
module: c10d	dist.destroy_process_group() don't release master_port when there're at least 2 process groups
module: c10d	ProcessGroupGloo CUDA Comm Synchronization Looks Wrong
module: c10d	c10d and discontiguous tensors with mismatch strides
module: c10d	Add Future return from default process group collective communications
module: c10d	TCPStoreTest.testWatchKeyCallback Timed Out
module: c10d	[c10d] Work objects should have a general operator<<
module: c10d	Why Pytroch.distributed does not expose NCCL cuda stream?
module: c10d	ProcessGroupGloo creation crashes when world_size > 150
module: c10d	Support a dist.group_like API
module: c10d	Add distributed testing for CUDA aware MPI
module: c10d	Remove single device constraint from ProcessGroupNCCL profiling
module: c10d	Support scatter/gather for NCCL backend
module: c10d	Add comprehensive subgroup testing to torch.distributed
module: c10d	torch.distributed with NCCL can hang if the first operation is a barrier
module: c10d	Enable timed barrier in c10d
module: c10d	Multiple ProcessGroup C++ Tests are flaky
module: c10d	Bug in profiler on CI machines when profiling NCCL distributed calls
module: c10d	[RFC] CUDA-aware future for distributed
module: c10d	Document torch.distributed.destroy_process_group()
module: c10d	send_object/recv_object APIs for c10d
module: c10d	ProcessGroup::Work API changes
module: c10d	NCCL watchdog thread should log warnings about long-running GPU operations instead of silently hanging
module: c10d	Complex Number support for distributed
module: c10d	Pytorch with CUDA aware OpenMPI for Infiniband not working with HCOLL and MXM
module: c10d	ProcessGroupGlooTest.test_gather_stress is flaky
module: c10d	ProcessGroupGlooTest.test_scatter_stress_cuda is flaky
module: c10d	[distributed] Synchronization on CUDA side with MPI backend
module: macos	[RFC] macOS x86 builds / test deprecation
module: macos	Can't build PyTorch 2.1 from source by GCC 13.2 on M1 MacOS
module: macos	DISABLED test_resnet18_cpu (__main__.BenchmarkFusionCpuTest)
module: macos	Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors
module: macos	DISABLED test_cat_nhwc (__main__.TestQuantizedOps)
module: macos	DISABLED test_vmapjvpall_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU)
module: macos	Build failure with Xcode 15 linker
module: macos	DISABLED test_jvp_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU)
module: macos	M2 Failing to build example-app in c++
module: macos	DISABLED test_multilayer_var_cpu (__main__.CpuTests)
module: macos	torch.dot gives wrong result on Macos
module: macos	DISABLED test_learnable_forward_per_channel_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps)
module: macos	Conv1d step-by-step numerical error
module: macos	Get errors after compiling and running PyTorch MINIMAL EXAMPLE for c++ Mac M1 with make
module: macos	F.conv1d and F.conv2d propagate nan's incorrectly when minibatch > 15
module: macos	row.device().is_cpu() INTERNAL ASSERT FAILED at "csrc/cpu/diag_cpu.cpp":7
module: macos	Error: no matching constructor for initialization of 'at::OptionalIntArrayRef'
module: macos	[bug] Internal assert failed when using pyro
module: macos	slow torch import on macos
module: macos	Adding sparse addmv and triangular_solve support on CPU - Mac OS - Apple Silicon M2
module: macos	Pytorch 2.0 installation tutorial does not work under Macbook
module: macos	Add arm64 builds for libtorch on MacOS with mps support
module: macos	Dynamo + MacOS: fatal error: 'omp.h' file not found
module: macos	Compiling libtorch from Source on Mac Beyond v1.11.0
module: macos	error: no member named 'residual_with_sum_zero_point' in 'ideep::attr_t
module: macos	Build from Source Issues on MacOS Ventura 13.2
module: macos	Cannot cast float64 to float32
module: macos	Conv2d error on M1 mac, RuntimeError: NNPACK SpatialConvolution_updateOutput failed
module: macos	Segmentation fault: 11 when running "import torch" on Mac OS X
module: macos	Importing torch 1.12.0 breaks subprocess module
module: macos	Python3 Depletes 2021 M1 Mac Memory Running Training Ops For Model's M, L and X
module: macos	Build from source failed on MacOS 10.6 with CUDA 10.1
module: macos	I have the same issue as @samgelman on my MacOS.
module: macos	cannot import name 'ProcessGroup' from 'torch.distributed'
module: macos	torch failure to open libcuda.so.1 on macOS
module: macos	Conda install from pytorch-nightly channel does not install the expected version on macOS
module: macos	Mismatch in clang toolchain lead to binary incompatibilities on M1 between torch and torchvision
module: macos	If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac
module: macos	Mac M1 Build Failure on DEBUG=1
module: macos	.lldbinit for lldb debuger
module: macos	TYPEIGNORE lint run locally disagrees with CI
module: macos	Compilation error on M1 Mac
module: macos	Conv3D consumes lots of memory on Mac with Apple Silicon
module: macos	Clang Compilation Error: more than one constructor applies to convert from "ptrdiff_t" to "c10::Scalar"
module: macos	Cannot install latest pytorch into Docker on Apple M1
module: macos	Can't forward pass conv2d with kernel_size=1, and padding=1
module: macos	M1 Pro Apple Silicon chip support.
module: macos	Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel(R) Math Kernel Library.
module: macos	DISABLED test_ind_worker_queue (__main__.TestIndividualWorkerQueue)
module: macos	ImportError: cannot import name 'ProcessGroup' from 'torch.distributed'
module: macos	MacOS CI result is not in /README.md
module: macos	Feature request: FFT operations on Metal
module: macos	Crashed with terminating with uncaught exception of type std::__1::system_error: condition_variable wait failed: Invalid argument
module: macos	universal binaries for libtorch on mac (x86_64+arm)
module: macos	libtorch on Apple m1
module: macos	NNAPI delegate library cannot be built on MacOS
module: macos	Unknown type name '__torch__.torch.classes.metal.Conv2dOpContext'
module: macos	Singular build setting CLANG_CXX_LANGUAGE_STANDARD has different values
module: macos	bundled libiomp5 causing segfaults in other libraries that use libomp
module: macos	Can't perform any operation on Vulkan device - macOS M1
module: macos	Calling benchmark.Timer with default num_threads=1 disables parallelism permanently
module: macos	Segmentation fault with ITIMER_REAL
module: macos	Unable to pip install PyTorch on M1 Mac - Errors
module: macos	qsize not implemented error
module: macos	macOS error building 1.7.1 from source on Catalina 10.15.7
module: macos	dyld: Symbol not found: _PyBaseObject_Type   Referenced from
module: macos	/usr/local/include/google/protobuf/stubs/pbconfig.h:3:37: note: expanded from macro 'GOOGLE_PROTOBUF_HASH_MAP_H'
module: macos	Segmentation Fault when importing torch on macOS Big Sur
module: macos	PyTorch 1.7.1 on (macOS/python 3.9/conda) links libtorch_global_deps.dylib with libomp.dylib instead of libiomp5.dylib
module: macos	About 2 minor bug fixes on CUDA macOSX 10.13.6
module: macos	MacOS CPU torch.tan and torch.tanh do not compute some values properly
module: macos	libtorch 1.5 crashes when used on macs when using torch::max without AVX support
module: macos	[Macos][CircleCI] Test failed in test_dataloader.py
module: macos	Installation from source fails on macOS (No CUDA)
module: macos	MacOS Error: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '12']' returned non-zero exit status 1.
module: macos	Illegal instruction: 4 - OSX 10.13.6 install from source
module: macos	iOS libtorch superpoint model bug
module: macos	[mac] Failure to import torch
module: macos	Installer not setting rpath for MAGMA (OS X w/ GPU)
module: macos	Tests for pytorch_macos_10_13_cuda9_2_cudnn7_py3_build fail
module: macos	TestAutograd.test_deep_reentrant fails with SIGBUS on macOS
module: macos	Pytorch master can not build with computer capability 3.0 under Mac OS X 10.13.16 with Nvidia GT 750m
module: macos	Bogus "Your compiler (clang++) is not compatible" message
module: macos	Undefined symbols for architecture x86_64: "testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith(void const*)" on Mac OS X
module: macos	torch.set_flush_denormal not working on some (old) OSX machines
module: macos	linked error of Pytorch 1.0 release
module: macos	Only one thread is used on macOS (super slow on CPU)
module: macos	[feature request] Include libomp support (macOS)
module: macos	MultiGPU hangs Titan Xp in multiprocessing/queue.py
module: loss	torch.nn.functional.cross_entropy different loss when providing one_hot_target and class weights
module: loss	No checks when running torch.nn.functional.ctc_loss with bogus inputs
module: loss	nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access
module: loss	Backward pass with sparse parameters results in error "Sparse division requires a scalar or zero-dim dense tensor divisor"
module: loss	torch.nn.CrossEntropyLoss: class weighting changes label_smoothing
module: loss	New Loss Function Add In Pytorch
module: loss	Discussion and Design  for Masked Loss Functions which can be used with PackedSequence training (but not exclusively)
module: loss	Add SSIM as Loss Function
module: loss	InfoNCE loss for contrastive learning
module: loss	Add oscillating activation functions to PyTorch.
module: loss	Adding label smoothing option to nn.BCELoss  and nn.BCEWithLogitsLoss?
module: loss	nn.CrossEntropy/nn.NLLLoss : Request for option to specify invalid ignore_index for perf. optimization
module: loss	torch.nn.MultiLabelMarginLoss has different performance on CPU and GPU.
module: loss	RuntimeError: CUDA error: device-side assert triggered
module: loss	torch.nn.CTCLoss Trigger out-of-bound Read under Compute Sanitizer
module: loss	binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor
module: loss	Group losses in a common namespace
module: loss	torch.nn.CTCLoss Trigger heap-buffer-overflow under AddressSanitizer
module: loss	nn.CrossEntropyLoss overflow with FP16 and minibatch
module: loss	Gradient value calculation error in MultiLabelMarginLoss
module: loss	Incorrect CPU implementation of CTCLoss backward step
module: loss	Could be clearer that Cross Entropy takes logits as input
module: loss	Missing corner case handling in ATen ctc_loss implementation
module: loss	F.binary_cross_entropy_with_logits unexpected behaviour
module: loss	soft_margin_loss gives wrong gradient when target with dtype uint8
module: loss	softmarginloss should use log1p and has an incorrect out= behaviour.
module: loss	CrossEntropyLoss computes SoftMax always across the second dimension
module: loss	Adding Polyloss to torch
module: loss	torch.nn.HuberLoss backwards unexpectedly fail
module: loss	Bug in label smoothing with ignored samples
module: loss	Cuda lacks checking of "out of bound"
module: loss	Some loss functions support dtype broadcast but some do not
module: loss	F.cross_entropy do not have a deterministic implementation,  adding deterministic support for this operation.
module: loss	Docs for torch.nn.MSELoss are confusing
module: loss	torch.nn.functional.ctc_loss with invalid input produce NaN or infinity gradient, while the batch entries are fine
module: loss	Port MarginRankingLoss to TensorIterator
module: loss	Feature Request: Bayesian Personalized Ranking Loss
module: loss	Delta not defined in the docs of HingeEmbeddingLoss
module: loss	addition of loss function RMSE in the torch.nn
module: loss	[feature request] Allow broadcasting for F.huber_loss and potentially other losses
module: loss	Bad Input to torch.nn.NLLLoss causes CUDA Error
module: loss	How can I re-weight a sample based on both class weights and instance weights?
module: loss	F.nll_loss with 16-bit CUDA tensors and reduction=mean produces NaNs
module: loss	The class weights implementation is incorrect
module: loss	ctc loss bug
module: loss	Add a BlendLoss class in torch.nn and a blend_loss function in torch.nn.functional to allow blending of a list of loss functions
module: loss	need  sampled softmax loss  function to train model to get user vector and item vector with a large dictionary
module: loss	Warn users when cross entropy is called after softmax
module: loss	Inconsistent behaviour with weight argument in CrossEntropyLoss and BCEWithLogitsLoss
module: loss	Add additive angular margin loss
module: loss	CTCLoss gradient is incorrect
module: loss	torch.nn.functional.ctc_loss crash(segfault)
module: loss	ignore_index for nn_mse_loss
module: loss	out variant of many loss functions are not consistent with non-out variant when reduction is not none
module: loss	nll_loss2d: t >= 0 && t < n_classes assertion is not checked when using GPU tensors and reduction='none'
module: loss	[Feature Request] SuperLoss (NeurIPS 2020)
module: loss	BCEWithLogitsLoss gives out nan with -inf logits
module: loss	[FR] loss reduction argument accepts None
module: loss	Reproducibility breaks down with weighted Cross Entropy loss
module: loss	Support for More Loss Functions
module: loss	BCEWithLogitsLoss() not equal to BCELoss() with sigmoid()
module: loss	Feature Request: Sampled softmax loss
module: loss	Typecasting issue in MSELoss
module: loss	Addition of Siamese loss or contrastive loss function
module: loss	[FR] general nll_loss and cross_entropy along arbitrary dimension
module: loss	CTCLoss cuda backend large batch handling takes up to 1.8x more memory
module: loss	Higher dimension support for MultiLableSoftMarginLoss
module: loss	BCEWithLogitsLoss expects wrong shape of weight (#classes instead of batch size)
module: loss	ctc_loss computes different losses and gradients on batched utterances vs. individual utterances
module: loss	"CrossEntropyLoss" should mention in its name that it takes softmax for target
module: loss	Feature Request: deterministic CUDA torch.nn.CTCLoss
module: loss	Feature Request: Earth Mover's Distance Loss
module: loss	CategoricalCrossEntropy Loss runs with wrong tag
module: loss	CrossEntropyLoss, ignore_index does not prevent back-prop if the logits are -inf
module: loss	[feature request] Runtime warning for inappropriate labels (among others)
module: loss	Unintuitive reduction of mini-batch loss for NLLLoss
module: loss	[doc] many losses still mention size_average in formula
module: loss	Accumulate into accreal instead of real for CPU loss functions
module: loss	[feature request] SSIM-based cost function as part of the standard set of loss functions
module: loss	CrossEntropyLoss mishandles weights
module: loss	Bugs: Score Function approach in REINFORCE for PONG
module: loss	BCELoss - weight parameter shape incorrect
module: loss	[feature request] add pairwise ranking loss
module: loss	Discrepancy in BCEWithLogitsLoss and ClassNLLLoss
module: loss	Unhelpful CrossEntropyLoss dimension error message
module: loss	Dice Loss PR
module: loss	BCELoss doesn't accept LongTensor targets
module: loss	Feature Request: NegativeSampling and HierarchicalSoftmax loss functions
module: windows	Operating on CUDA tensors on multiprocessing queue hangs
module: windows	Pytorch build from source fails
module: windows	Internal CI for libTorch
module: windows	Build LibTorch for Windows ARM64
module: windows	Create a reproducible build for LibTorch x64 on VS2022
module: windows	Parameters between models don't copy in the C++ Pytroch Frontend under windows
module: windows	exit code -1073740791 (0xC0000409) when torch.package.PackageExporter
module: windows	[pt2] Make error message clearer for torch.compile when running on windows
module: windows	/nodefaultlib:vcomp doesn't seem to be set when compiling with Intel OpenMP on Windows
module: windows	Tensors Can't be  Overwritten in Visual Studio Windows
module: windows	Error loading "AppData\Local\Temp\_MEI136882\torch\lib\cufft64_10.dll" or one of its dependencies.
module: windows	Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'>
module: windows	pyTorch 2.1 3x slower than 2.0
module: windows	Cannot build static windows libraries
module: windows	Failed to import transformer.
module: windows	torch::serialize::OutputArchive::save_to crash if save on C:\
module: windows	Perf-Drop (factor=2) Ubuntu-vs-Windows on same PC (dual-boot)
module: windows	Slow performance when running torch.jit traced model with Flash Attention using libtorch on Windows
module: windows	Pytorch ROCM windows builds
module: windows	torchrun fails to run on Windows 11
module: windows	Can't run Test/Inductor test: test_compiled_optimizers.py
module: windows	Readily available python wheels for windows ARM
module: windows	torch.save throws an error when the path uses mixed separators on Windows
module: windows	Support Delay Loading of c10.dll in when using libtorch as a thirdparty library.
module: windows	LibTorch 2.0.1 scripting in Debug mode on Windows
module: windows	Network does not return any thing, not even None and breaks loops
module: windows	Python Crashes When Importing Torch With C API
module: windows	[Pytorch 2.0] torch::nn::Dropout output is incorrect on Windows
module: windows	Fix sparse windows on CPU with MKL
module: windows	[RFC] Add third-party malloc library to improve pytorch memory performance on Windows
module: windows	CrossEntropyLoss output difference on Windows vs. Linux
module: windows	pytorch-nightly not have torch/version.py.tpl:cuda specified
module: windows	Fault and vauge error when invoking nvcc: The system cannot find the file specified
module: windows	Pytorch compile failure on Windows with CUDA 12.1 because of lacking NVTX component
module: windows	[Discussion] Investigate possibilities for Windows Arm64 BLAS and LAPACK
module: windows	Sparse is not available on Windows
module: windows	No GPU found, using CPU during preprocessing Error processing dataset with NsfHifiGAN
module: windows	Investigate/add Windows Arm64 support for cpuinfo
module: windows	Enable CUPTI
module: windows	torchdim can not be compiled for Python-3.11 on Windows
module: windows	[CI]  PyTorch Windows Test AMIs contains CUDA-11.3 installation
module: windows	Estimate effort needed to bring PyTorch to Windows Arm64
module: windows	Update quantization to make source files complient with /Zc:lambda
module: windows	Cost & performance estimation for Windows Arm64 compilation
module: windows	Investigate CUDA enabled build-time difference between MSVC and GCC+WSL
module: windows	Cross-compiled libtorch Windows Arm64 binaries
module: windows	PyTorch 2.0 not working on Windows
module: windows	Enable NCCL for PyTorch on Windows
module: windows	Libtorch's CPU inference is much slower on Windows than on Linux
module: windows	Get https://github.com/pytorch/benchmark working
module: windows	Investigate possibilities of automation for build pipeline
module: windows	Performance issue on Windows with a "benchmark" comparing to Linux and WLS
module: windows	built from source windows static library with multiple "unresolved external symbol"
module: windows	path\tp\torch\torch.h(14,1): fatal error C1001: Internal compiler error.
module: windows	_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training
module: windows	Windows multi machine multi card training, card initialization in GLOO communication.
module: windows	Libtorch C++ model forward  crashed on windows10, CUDA 11.2, Qt ,RTX 3070, but libtorch C++ works with cpu successfully
module: windows	Migrate current Windows CI scripts off of batch
module: windows	PyTorch Profiler is not working with CUDA
module: windows	Bad performance of stock model on Windows compared to Linux
module: windows	We should perhaps prevent linking against LibTorch DLLs with the wrong configuration (debug vs release)
module: windows	High CPU using torch.stack/torch.cat on Windows
module: windows	in WINDOWS, CUDA Out of Memory error but CUDA memory is almost empty
module: windows	FusedKernelCPU failed to delete generated dll files on Windows
module: windows	Cudnn header files should be copied into build package as well
module: windows	[Discussion] Use the unicode variant of the Windows API
module: windows	How to build libtorch static libraries on Windows?
module: windows	[RFC] Add Windows support to torch.distributed package
module: windows	Try Address Sanitizer in MSVC builds
module: windows	CUDA sources are not cached with sccache
module: windows	[DISCUSSION] Better user experience for debugging on Windows
module: windows	Support multiple-build-type generators for CMake
module: windows	Why Keras behave better and faster than Pytorch under the same network configuration?
module: windows	The performance of multiplication of two matrices is different between window and linux
module: windows	[Proposal] Pin Windows SDK and MSVC compiler versions in LibTorch
module: windows	Pytorch is slower on windows than on linux
module: windows	Adding a method called T in native_functions causes undefined behavior on Windows
module: windows	In-source build causes repeating filename annotations (Windows doesn't support out-of-source build)
module: windows	why my personal compiled libtorch is so slow?  2~3 times slower than caffe
module: windows	Multiple CPU processes using same GPU model for inference
module: windows	Unicode support for the MS Windows platform
module: windows	Windows CPU version much slower than Unix versions
module: random	[FakeTensor] FakeTensorMode dispatch won't change generator state
module: random	torch.multinomial - Unexpected (incorrect) results when replacement=True in version 2.1.1+cpu
module: random	Add the possibility to pass a Generator to gumbel_softmax
module: random	cuda rng state for 2.0.1 cannot be used for 2.1.0
module: random	PYTORCH_TEST_WITH_INDUCTOR=1 python test/test_ops.py -k test_out_{warnings_, *}{_refs_, *}randn_cuda_float32 fails on main
module: random	NotImplementedError: Could not run 'aten::multinomial' with arguments from the 'Meta' backend.
module: random	LSTM built-in dropout not reproducible on GPU
module: random	Torch randperm with device mps does not sample exactly uniformly from all possible permutations
module: random	[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method normal of type object at ***: got an unexpected keyword argument 'device'
module: random	Request for adding support for torch.rand_like, torch.randn_like, torch.randint_like with torch.Generator
module: random	torch.nn.init functions with generator argument
module: random	torch.randn signature is missing generator
module: random	Document and promise reproducibility torch.randn / torch.rand / torch.randint family behavior on CPU devices
module: random	Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities
module: random	[Inductor] support complex dtypes
module: random	RuntimeError: philox_cuda_state for an unexpected CUDA generator used during capture
module: random	[threaded pg] All threads share one Random Number Generator
module: random	Allow low and high to be tensors in torch.randint
module: random	torch.randn and torch.normal sometimes produce NaN on mps device
module: random	Add a device keyword argument to torch.manual_seed
module: random	Batched Random Number Generators
module: random	torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed)
module: random	Poisson sampling on GPU fails for high rates
module: random	Randomness should be consistent across devices with use_deterministic_algorithms
module: random	Complex-Valued Gaussian distributions
module: random	torch.randint should accept high=2**63
module: random	Float and double tensors randomly initialized with the same seed get different values for size >= 16
module: random	Random functions should infer device from user-specified Generator
module: random	torch.randperm uses too much cpu, but not efficient.
module: random	Random Generator for Dropout
module: random	Improve handling of generators
module: random	Having rrelu functional + module take a generator object to match native functions entry
module: random	Segmentation fault in _sobol_engine_scramble_
module: random	Segmentation fault in _sobol_engine_initialize_state_
module: random	Segmentation fault in _sobol_engine_ff_
module: random	[docs] Tensor.uniform_ docs are not clear about whether from/to boundary values are included in sampling or not
module: random	Random Shuffle along Axis
module: random	cannot pickle 'torch._C.Generator' object for torch.Generator
module: random	[discussion] Have default size/shape equal to a scalar
module: random	alias to generate tensor with random uniform distribution.
module: random	[BUG] Inconsistent initialization on different machines (tensor.uniform_())
module: random	Sobol state API
module: random	Setting a random seed or passing a generator to Modules for deterministic sampling.
module: random	[feature request] torch.Generator constructor to accept seed directly
module: random	a problem happened in torch.randperm
module: random	torch.Generator ignores the device argument?
module: random	setting  all seed still get different result.
module: random	Implement random SeedSequence
module: random	Half Normal Log_Prob not defined for 0
module: random	hope to support something like  "torch::manual_seed_for_mulit_thread"
module: random	torch.Tensor.random_ is divergent from NumPy's np.random.random
module: random	Multinomial without replacement produces samples that have zero probability
module: random	Move randperm() to DistributionTemplates
module: random	[proposal] batch mode for randperm
module: random	torch.random.randperm stuck in multiprocess
module: random	torch.random.fork tries to initialize cuda even when no cuda devices are available
module: random	ppc64le: test cpp_extensions/rng_extension.cpp failure (without altivec override)
module: random	Simplify checks that generator has next normal sample cache methods in normal_distribution
module: random	Do not modify global random state
module: random	Unify CPU/CUDA exponential transformation formula
module: random	Investigate using log1p instead of log in transformation functions(TransformationHelper.h)
module: random	Investigate exponential distribution improvements
module: random	Investigate using -cospi(u) / sinpi(u) instead of tan(pi * (u - 0.5)) in transformation::cauchy
module: random	Move bernoulli_() to DistributionTemplates
module: random	Generator C++ API should match Python API
module: random	Tensor.random_ is not implemented for bfloat16 on CPU(but implemented on CUDA)
module: random	torch.rand() not having same values on using torch.manual_seed(0)
module: random	Sobol engine generates out-of-bounds samples after drawing too many samples
module: random	Tensor.random_ is not implemented for bool on CUDA(but implemented on CPU)
module: random	Sobol point implementation
module: random	[FR] add generator= kwarg support for torch.randn and torch.rand
module: random	torch.multinominal ignores elements from cumulative distribution
module: random	RNG for torch.randn_like
module: random	[FR] Dropout modules/functions should take in generator=
module: random	Generator objects should not always use the same seed
module: random	[FR] torch.(Generator|random).seed allows specifying the seed value
module: random	CPU torch.exponential_ function may generate 0 which can cause downstream NaN
module: random	CPU random number generator is slow
module: random	Improve multithreaded random number generation (RNG)
module: random	torch.bernoulli's parameter generator not documented
module: random	Very poor Uniform() sampling near floating 0.0
module: random	Better user experience for using Generator object
module: arm	Pytorch with GPU support compile error on Jetson Xavier RX
module: arm	python -c "import torch;print(torch.nn.GELU()(torch.rand(2)))" crashes on aarch64
module: arm	Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors
module: arm	pytorch consuming all cpu cores 100% on ARM
module: arm	Inconsistencies when casting to integral types
module: arm	Enable SLEEF on ARM
module: arm	Conv1d step-by-step numerical error
module: arm	Get errors after compiling and running PyTorch MINIMAL EXAMPLE for c++ Mac M1 with make
module: arm	F.conv1d and F.conv2d propagate nan's incorrectly when minibatch > 15
module: arm	Illegal instruction in ARM64 (ver 2.0.0)
module: arm	Cross compile Pytorch for ARM in Bazel
module: arm	Adding sparse addmv and triangular_solve support on CPU - Mac OS - Apple Silicon M2
module: arm	Add arm64 builds for libtorch on MacOS with mps support
module: arm	Investigate/add Windows Arm64 support for cpuinfo
module: arm	Add Support for RockChip NPUs (RKNN(2))
module: arm	Cross-compiled libtorch Windows Arm64 binaries
module: arm	PyTorch memory leak reference cycle in for loop, Mac M1
module: arm	onednn(mkldnn) backend support for quantized operators
module: arm	Compare oneDNN and OpenBLAS backend of PyTorch on arm64 architecture
module: arm	Illegal hardware instruction following Real Time Inference on Raspberry Pi 4 tutorial
module: arm	Conv2d error on M1 mac, RuntimeError: NNPACK SpatialConvolution_updateOutput failed
module: arm	torch/csrc/utils/python_arg_parser.h:424:94: error: format ‘%ld’ expects argument of type ‘long int’, but argument 7 has type ‘int’
module: arm	Error: unknown architecture armv7-a;' and Error: selected processor does not support command' in ARM mode
module: arm	Pytorch built for Jetson errors if CUDA is not found
module: arm	Figure out the future of Metal backend given the existence of MPS
module: arm	Add torch nightly builds pipeline for aarch64 linux
module: arm	Incorrect results for mean or sum kernels on aarch64 when building with gcc-7
module: arm	[Mac M1] torch.mm sometimes produces incorrect results
module: arm	Mismatch in clang toolchain lead to binary incompatibilities on M1 between torch and torchvision
module: arm	If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac
module: arm	Mac M1 Build Failure on DEBUG=1
module: arm	Performance bad on ARM AArch64 for PyTorch C++
module: arm	Compilation error on M1 Mac
module: arm	Cannot install latest pytorch into Docker on Apple M1
module: arm	Can't forward pass conv2d with kernel_size=1, and padding=1
module: arm	M1 Pro Apple Silicon chip support.
module: arm	Core dumped with large matmul on aarch64
module: arm	Tensor precision manifests differently between CPU and GPU.
module: arm	CIFAR10 doesn't work on M1 MacBook
module: arm	[libtorch]incorrect sigmoid result on arm chip(rk3326)
module: arm	PyTorch inference on tensors of a particular size cause Illegal Instruction (core dumped) on Jetson Nano
module: arm	Get zero when using torch.matmul and torch.dot with 1-D tensor in torch-1.9.0-cp39-none-macosx_11_0_arm64.whl
module: arm	universal binaries for libtorch on mac (x86_64+arm)
module: arm	Implement set_flush_denormal  for ARM
module: arm	Enable fft  support for mobile builds
module: arm	libtorch on Apple m1
module: arm	Incorrect exponential calculation on Jetson devices with float32 dtype
module: arm	complie error when i use python setup.py build on arm computer
module: arm	Can't perform any operation on Vulkan device - macOS M1
module: arm	NaN values on torch.nn.functional.conv2d (aarch64)
module: arm	pytorch/manylinux-cuda102 support for aarch64
module: arm	pytorch build failure on arm64
module: arm	Possibly alignment issues on NEON vectorized ops, Jetson platforms
module: arm	Can't install pytorch =1.7 with python 3.8.5 on Raspberry Pi
module: arm	Unable to pip install PyTorch on M1 Mac - Errors
module: arm	torch.nn.functional.log_softmax on Arm CPU gives partly NaN results
module: arm	TorchVision and TorchAudio wheels for AArch64 absent from https://download.pytorch.org/whl/torch_stable.html
module: arm	link failed when using custom build pytorch on Android
module: arm	Could you please increase the wheel compiled Linux (aarch64) GPU, thank you very much!
module: arm	from_numpy function rounds float values on Jetson NX
module: arm	Why isn't there a pip-installable PyTorch for Raspberry Pi?
module: arm	arm64-v8a not compiling due to libpytorch_jni.so
module: arm	build from source fail in jeston tx2 Python3.8
module: arm	linker error when trying to use Metal backend in PyTorch Mobile
module: arm	After updating to Xcode 12 and LibTorch to 1.7.0. Facing issue when running unit test.
module: arm	Support for oneDNN / MKL-DNN on AArch64
module: arm	Reciprocal and reciprocal square root instructions are too inaccurate on ARM64
module: arm	ModuleNotFoundError when installing PyTorch via pip on aarch64 environment
module: arm	How to use libtorch on Jetson TX2
module: arm	CUDA cannot be found
module: arm	Eigen Tensor library for convolutions on CPU
module: arm	arm64 port for PyTorch, libtorch
module: tensorboard	Tensorboard list of tensors as input
module: tensorboard	torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch
module: tensorboard	Pytorch profiler with Tensorboard example not working
module: tensorboard	add_image_with_boxes method from torch.utils.tensorboard.writer.SummaryWriter is broken
module: tensorboard	Appending new logs to existing tbevent files when using tensorboard
module: tensorboard	SummaryWriter.add_embedding not working for RGBA images
module: tensorboard	[Utils][tensorboard]Enhancement: Include 'max_outputs' parameter in torch.utils.tensorboard.summary's 'image' method
module: tensorboard	SummaryWriter background thread holds the GIL for too long
module: tensorboard	Tensorboard graph tracing with torch fx API
module: tensorboard	torch.profiler.tensorboard_trace_handler Generates an incorrect JSON file
module: tensorboard	numpy v1.24 does not work with writer.add_histogram
module: tensorboard	Only the first logged trace in a given log dir is visible in tensorboard.
module: tensorboard	Torch SummaryWriter import fails with torch 2.0 with an error on numpy.object
module: tensorboard	AttributeError: module 'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'MakeDirs'
module: tensorboard	Session of Google Colab crashes when torch.utils::SummaryWriter is called after importing torchaudio
module: tensorboard	Let torch.utils.tensorboard support multiprocessing
module: tensorboard	tensorboard SummaryWriter.add_graph fails when model uses empty tuples
module: tensorboard	Hide or fuse TupleConstruct / TupleUnpack from tensorboard graph
module: tensorboard	TensorBoard frontend fails to display embeddings when add_embedding() writes large label_img
module: tensorboard	SummaryWriter reports encoding error
module: tensorboard	Protocol buffers library mismatch.
module: tensorboard	the abnormal display of bottleneck of resnet in tensorboard
module: tensorboard	PyTorch Profiler for distributed time count
module: tensorboard	Incubation of Graphical Representation in TF2 of HParams
module: tensorboard	[Feature Request] add graph to hparams for tensorboard
module: tensorboard	possible SummaryWriter memory leak
module: tensorboard	Support for stereo audio data in from torch.utils.tensorboard.SummaryWriter
module: tensorboard	SummaryWriter add_image() docs do not state that it is expecting images in a certain format
module: tensorboard	Proposal for export TensorBoard tracing files
module: tensorboard	TensorBoard SummaryWriter to remote storage unreasonably slow
module: tensorboard	Allow add_embedding to have dict for metadata
module: tensorboard	SummaryWriter deletes data automatically when there are too many
module: tensorboard	torch.utils.tensorboard.SummaryWriter.add_embedding fails for some label_img sizes
module: tensorboard	pytorch not uploading data to tensorboard page
module: tensorboard	Tensorboard makes logger handlers (except the 1st one) DISAPPEAR !
module: tensorboard	Combining add_scalar with add_hparams with different frequencey
module: tensorboard	Tensorboard summary_iterator
module: tensorboard	Issue while writing scalars o tensorboard using writer.add_scalars(....)
module: tensorboard	Can not get pytorch working with tensorboard
module: tensorboard	TensorBoard Summaries Written with SummaryWriter.add_scalars() are not Purged
module: tensorboard	Not able to launch tensorboard using pytorch
module: tensorboard	Missing explanation in torch.utils.tensorboard.add_histogram()
module: tensorboard	Segfault = docker + tensorboard + pytorch
module: tensorboard	Draw quantized tensors in tensorboard
module: tensorboard	Tensorboard，graph in pytorch 1.4 is more complicated than pytorch 1.1？
module: tensorboard	runtime error while using default arguments in add_graph() function
module: tensorboard	tensorboard projector mode with custom metadata_header with only one label name
module: tensorboard	torch Summary writer does not display torchvision.io.read_video output
module: tensorboard	Questions about "torch.utils.tensorboard.add_graph": Could I use it to see network graph's compute time and memory?
module: tensorboard	The pytorch's graph is lack of common names for nodes
module: tensorboard	Why do we not use TorchScript to build graph for tensorboard
module: tensorboard	Support alpha channel in tensorboard.add_figure
module: tensorboard	tensorboard add_graph 's "operator_export_type"
module: tensorboard	Some module has incorrect scope in complex naming situations in tensorboard graph export
module: tensorboard	Some module info is missing in nested graph for tensorboard
module: tensorboard	[Tensorboard] Problem with subfolders from SummaryWriter
module: tensorboard	Bug in add_param - functionality for tensorboard (scatter matrix view)
module: tensorboard	[Tensorboard] nothing appears in the projector tab
module: tensorboard	[TensorBoard] Graph with objects other than torch.nn.Module can not be visualized.
module: tensorboard	Tensorboard GPU Problems
module: tensorboard	Tensorboard add image with boxes, labels, and confidence scores.
module: tensorboard	Tensorboard logging image WITH LABEL
module: tensorboard	Importing tensorboard jams CUDA device selection
module: tensorboard	[RFC] TensorBoard extensions and improvements for PyTorch
module: tensorboard	tensorboard add_graph error
module: tensorboard	[Tensorboard] Write summaries to S3 or GCS bucket
module: tensorboard	Tensorboard: Add disable flag for debugging
module: tensorboard	torch.utils.tensorboard.SummaryWriter fails to flush at program exit
module: tensorboard	SummaryWriter doesn't read comment if log_dir precised
module: tensorboard	Wrong device in graph - Tensorboard SummaryWriter
module: tensorboard	RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48
module: rnn	Mixed precision is not working with CudnnLSTM on rocm
module: rnn	pack_padded_sequence/pad_packed_sequence support in dynamo
module: rnn	RNN Documentation is Confusing / Wrong
module: rnn	parameterizations.orthogonal does not work as intended with nn.GRU or nn.LSTM
module: rnn	Question about GRU(RNN/LSTM) outputs shape
module: rnn	torch.nn.utils.rnn.unpad_sequence modifies arguments in-place
module: rnn	Memory Corruption in torch.lstm caused by edge cases
module: rnn	DISABLED test_pickle_nn_RNN_eval_mode_cuda_float64 (__main__.TestModuleCUDA)
module: rnn	Faster pad_sequence and tensor_split function with CUDA kernel, are they possible?
module: rnn	Quantizable LSTM has different behavior than LSTM in bidirectional setting
module: rnn	pack_sequence() always fail after set_default_tensor_type to CUDA
module: rnn	pack_padded_sequence not compatible with deterministic mode it calls torch.scatter
module: rnn	Placing LSTM model on bfloat16 on GPU causes error
module: rnn	Crash on backwards step when using batch_first=True for LSTMs on MPS (1.14 nightly build)
module: rnn	RFC: Add flag for RNN decomposition to all RNN modules
module: rnn	pad_sequence and pack_sequence should support length zero tensors
module: rnn	Time Gated Lstm
module: rnn	recurrent neural network module
module: rnn	Introduction of nn.LazyRNNs
module: rnn	Misleading Error Message from nn.RNN when Passing Incorrect Data Type
module: rnn	Inconsistencies in LSTM outputs when processing sequence stepwise on CPU
module: rnn	pad_packed_sequence is not exactly the inverse of pack_padded_sequence
module: rnn	Parametrization silently disables RNN weight updates
module: rnn	Anomaly detection: Error detected in CudnnRnnBackward0
module: rnn	'LSTM' object has no attribute '_flat_weights_names'
module: rnn	lstm's input h0 and c0 bug
module: rnn	Segmentation fault when using CUDA with RNN
module: rnn	Potentially misleading note in documentation for PackedSequence
module: rnn	Libtorch segfault in packed GRU evaluation with cuda batch_sizes
module: rnn	Allow variable intermediate hidden dimensions for stacked RNN/LSTM layers
module: rnn	[DOC] Incorrect image for the confusion matrix
module: rnn	cuDNN error upon the backward method with a sliced tensor output from nn.GRU.
module: rnn	Unexpected slow dropout in stacked RNN/LSTM/GRU
module: rnn	GRU and LSTM fail for seq_len = 0
module: rnn	CARU: A Content-Adaptive Recurrent Unit for the Transition of Hidden State in NLP
module: rnn	Ambiguous RuntimeError raised when device of input PackedSequence does not match with nn.LSTM's device
module: rnn	how to use torch.utils.checkpoint + gru with variable length sequence?
module: rnn	LSTM::permute_hidden breaks Liskov substitution principle
module: rnn	CuDNN RNN bindings are basically all deprecated in cudnn 8
module: rnn	An error occurred deploying CRNN using libtorch
module: rnn	LSTMs leak memory in CPU PyTorch 1.5.1, 1.6, and 1.7 on Linux
module: rnn	Providing CUDA tensor to model on CPU causes a crash
module: rnn	Variational Dropout In RNN
module: rnn	LSTMCell consumes x1.5 more memory on CUDA on pytorch >=1.3 comparing to pytorch 1.2
module: rnn	Inaccurate batched GRU results on CPU
module: rnn	Add option in LSTM layer to access all cell states of all time steps
module: rnn	I have  reconstructed LSTM model and tested by mnist data but the loss is not changed (loss=2.3)
module: rnn	Slow (20-50x) RNN tutorial/example when torch is installed using pip comp. to conda installation
module: rnn	Access data_ptr in RNN.cpp
module: rnn	Handling of packed_sequence by activation functions and linear layers
module: rnn	"To compact weights again call flatten_parameters()" is printed every step for every GPU
module: rnn	update docs that sorting is not needed in
module: rnn	LSTM forget bias must be initialized properly
module: rnn	Error with setting tensors to use cpu in packed_padded_sequence when CUDA tensor is set as default
module: rnn	MultiGPU for gru
module: rnn	pack_padded_sequence throws IndexError when only kwargs are specified
module: rnn	[Feature request]:  add LayerNormLSTMCell
module: rnn	[feature request] - Allow sequences lengths to be 0 in PackSequence
module: rnn	RNN gradients in eval mode in pytorch 0.4
module: rnn	[feature request] Support for 0-length sequences in packed_sequences
module: rnn	[Feature Request] Additional torch.nn.LSTM functionality
module: rnn	Different behavior of LSTM and LSTMCell implementation
module: rnn	GRU is implementation of GRU v1 draft rather than final GRU paper algo
module: rnn	[feature request] norm argument for RNNCells
module: rnn	[feature request] Type-1 Multi-layer bidirectional RNN
module: rnn	Very slow on CPU
module: rnn	[Proposal] Consistent batch_first effect for RNN modules
module: rnn	Feature Request: ReLU on LSTMs and GRUs
module: rnn	Feature request: reverse_padded_sequence
module: regression	import torch results in cuInit call
module: regression	Performance regression using latest nightlies and HF transformers
module: regression	python -c "import torch;print(torch.nn.GELU()(torch.rand(2)))" crashes on aarch64
module: regression	Bug in element-wise multiplication of torch.sparse_csr_tensors on GPU - 0's in result considered significant - PyTorch 2.1.1
module: regression	[ONNX] ONNX export fails when combining tracing and scripting in the presence of symbolic functions
module: regression	torch.div on empty tensors causes segmentation fault
module: regression	[Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage
module: regression	AttributeError: 'MultiheadAttention' object has no attribute 'requires_grad'
module: regression	DISABLED test_jvp_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU)
module: regression	RuntimeError: Expected packed scalar Tensor to be of dimension 1. Got 0 instead.
module: regression	ValueError: only one element tensors can be converted to Python scalars
module: regression	Torch compile generates incorrect graph on Llama model
module: regression	PyTorch profile issues summary
module: regression	[regression] Not getting CUDA error: device-side assert triggered on main for CUDA_KERNEL_ASSERT2
module: regression	Regression in text encoding
module: regression	PyTorch 2.0.x CUDA error: operation not supported when Tensor.to a different device
module: regression	Observed regress in DataLoader spawn from PyTorch1.13 to PyTorch2.0
module: regression	[Pytorch 2.0] torch::nn::Dropout output is incorrect on Windows
module: regression	Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118)
module: regression	profiler.export_stacks doesn't return stack trace unless experimental_config is provided
module: regression	[regression] torch.norm with out dtype bfloat16 cause runtime error
module: regression	Group Norm crashes on Apple M1/MPS devices for versions 2.0+
module: regression	Power VSX vectorization support disabled
module: regression	mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (6) is not less than length of dimension[0] (6)'
module: regression	Investigate queue disparity between windows.4xlarge and linux.4xlarge
module: regression	Abort Caused by Virtual Function
module: regression	CUDA error: initialization error
module: regression	[PT][1.13] torch .numpy() fn broke for some scenario
module: regression	Add smoke-tests for CPP extensions compilations
module: regression	Can't import torch --> OSError related to libcublasLt.so.11
module: regression	Performance tests mnist_hogwild-cpu_memory CPU memory increase by 30%
module: regression	[ONNX] CSE pass in export pollutes Scope information
module: regression	torch.multinomial on MPS crashes with Error: total bytes of NDArray > 2**32'
module: regression	MPSNDArray.mm:782: failed assertion; bufer is not large enough Mac M1 MPS
module: regression	Cannot index into a tensor using indices from another device - regression from 1.12
module: regression	linux-bionic-cuda10.2-py3.9-gcc7  multigpu test are broken
module: regression	upsample_bilinear2d() received an invalid combination of arguments
module: regression	torch._weight_norm with specified dim returns wrong output
module: regression	torch.package can not be used to serialize resnet18 from TorchVision-0.12
module: regression	torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12
module: regression	CMake Error: File /opt/pytorch/build_variables.bzl does not exist.
module: regression	static builds are broken by MKL_DNN
module: regression	importing open3d before pytorch causes matmul to produce a segfault
module: regression	After XNNPack update TestXNNPACKSerDes.test_linear started to fail
module: regression	Large performance difference of loss.backward() between torch-1.9.0 and torch-1.8.0
module: regression	Regression in multi-node training speed with Transformers + PyTorch
module: regression	Calling .backward() inside of an LBFGS closure function throws an exception in Libtorch v1.6.0+
module: regression	Breakes with -OO flag in script
module: regression	Conv2d kernel performance regression on CPU since PyTorch 1.9
module: regression	JIT fuser throws compilation error (1.10 regression)
module: regression	Segmentation fault when using C++/pybind11 module without also importing torch
module: regression	Pytorch V1.1 runs OK, but Pytorch v1.5 to 1.9 run wrong
module: regression	Profiler does not contain NCCL kernel
module: regression	torch.arange() issue
module: regression	apex internal assert failed
module: regression	Segmentation fault encountered when using nn.MultiheadAttention with v1.7.1
module: regression	Pytorch 1.5+: Kernel dies / Segmentation Fault with torch.cuda.mermory_allocated() and torch.cuda.mermory_reserved()
module: regression	Training slows down and memory usage increases when upgrading from PyTorch 1.6 to 1.7
module: regression	DataLoader with cv2 and some numpy/cv2 import order causes workers to not work
module: regression	Multiple torch.load in one file
module: regression	DataParallel with Torch 1.5
module: regression	Memory Leak with Docker GPU
module: regression	Pybind11 cpp extensions broken with pytorch v1.5.0
module: regression	CUDA sources are not cached with sccache
module: regression	libtorch_global_deps.so not found.
module: reductions	Allow reductions to write into pinned memory
module: reductions	torch.sparse.mm() with reduce operator for GPU support and COO matrices
module: reductions	Functions for Calculating Skewness and Kurtosis
module: reductions	Tensor.all() fails on MPS for tensors with more than 4 dimensions
module: reductions	[numpy] mean & nanmean should support int dtypes
module: reductions	bf16 strided tensor wrong calculation
module: reductions	torch.sum promotes integral tensors to int64.
module: reductions	[Feature request] Exclusive prefix sum, torch.cumsum(input, dim=0, exclusive=True)
module: reductions	torch.sum, prod, cumsum, cumprod, sparse.sum INTERNAL ASSERT FAIL
module: reductions	ArgMax for Multi Dimensional Tensor
module: reductions	torch.{max,min} have strange error message when input.numel()==0
module: reductions	Feature request: Add complex support to torch.nanmean
module: reductions	argmin/argmax incorrect doc for the first form
module: reductions	Reductions on zero-size dims: 1) by accepting a custom default value, 2) if tensor has another non-reduced zero-size dim
module: reductions	Overflow error in torch.linalg uncaught
module: reductions	Either support torch.mean on BoolTensors or fix the error message
module: reductions	torch median / nanmedian w/ nans speed
module: reductions	Should we be upcasting integral types to int64 in torch.sum and torch.prod?
module: reductions	Reducing over empty dimensions for reductions without identity
module: reductions	Many reduction operators do not support reducing over multiple dimensions
module: reductions	F.nll_loss with 16-bit CUDA tensors and reduction=mean produces NaNs
module: reductions	Replace unbiased parameter in torch.(std|var|std_mean|var_mean) with correction=0
module: reductions	Deprecate torch.(min|max|median|mode) to only return values and not indices
module: reductions	Some reduction operators have double signatures
module: reductions	Implement missing torch.nan* operators
module: reductions	Reduce with any(), all(), median() over multiple dimensions
module: reductions	Reductions tracking issue
module: reductions	max-sum operation
module: reductions	torch.(min|max)(..., dim=...) diverges from array API specification
module: reductions	[perf] 10x improvement when doing x.sum(-1) manually
module: reductions	Change make_reduction to reflect input resizing.
module: reductions	Support multi-dim reductions for torch.prod, torch.all, torch.any
module: reductions	bool_tensor.sum(dtype=torch.int32) creates int32-copy of the original int8 tensor
module: reductions	Add start_dim and end_dim functionality for common reduction operations.
module: reductions	Have the possibility to reduce a tensor with median on more than one specified dimension
module: reductions	test_reductions ignoring some tests
module: reductions	Reset mask for torch.cumsum?
module: reductions	Massive Performance bottlenecks in some of the Reduce operations.
module: reductions	torch.median slower than torch.sort on cpu
module: reductions	out variant of many loss functions are not consistent with non-out variant when reduction is not none
module: reductions	torch.any and torch.all map uint8 -> uint8 but should map uint8 -> bool
module: reductions	torch.var and torch.std are not compatible with np.var and np.std
module: reductions	torch.sum(tensor(2.), dim=0) (and probably other reduction functions) doesn't make sense
module: reductions	torch.mode when input has nans
module: reductions	Support torch.mean for BoolTensors and other integer tensor inputs (without manual upcasting and hopefully without hidden upcasting)
module: reductions	Vec256<int64_t> does not handle LONG_MAX on minimum
module: reductions	General reduction mode selection for in-place and out-variants for wider range (hopefully all) of ops
module: reductions	Generalized CPU vector reductions
module: reductions	Dimension reducing variants of bitwise operations (bitwise_or, bitwise_and, bitwise_xor)
module: reductions	Support bool input tensors for argmax / argmin / sort / topk and other functions
module: reductions	torch batchwise max with indices
module: reductions	[feature request] Out-variant and dtype argument for torch.argmax / torch.argmin / torch.argsort (and friends)
module: reductions	Allow range in dim argument of reducing operations such as sum
module: reductions	logsumexp with subtraction
module: reductions	logsumexp: two little-impact perf suggestions
module: reductions	[doc] Tensor.mean: dtype kwarg is not documented
module: reductions	torch.sum(tensor, dim=()) is different from np.sum(arr, axis=())
module: reductions	torch.mean(x, dims=[]) has incorrect gradient in 1.2
module: reductions	torch.Tensor.mean erroneously documented as sometimes returning a tuple
module: reductions	tensor.var_mean variant for existing torch.var_mean (and same for std_mean)
module: reductions	Can't torch.sum(tensor, dim) where dim >= 64
module: reductions	Make operators like logsumexp and cumsum operate over dimension 0 by default (or at least for 1D arrays)
module: reductions	[FR] torch.dist along a dimension
module: dispatch	There seems to be redundant calling of findOp in findSchemaOrThrow method of Dispatcher.cpp
module: dispatch	Operator with only Tensor[][] args unsupported by dispatcher
module: dispatch	dispatcher cannot determine dispatch key on tuple input
module: dispatch	overloads can perhaps be more performant?
module: dispatch	assert_is_valid_input_type is too weak
module: dispatch	torchgen/gen_backend_stubs.py compatibility with DispatchStubs
module: dispatch	Add default device function interface for device-aware apis
module: dispatch	Segfault when using torch.ops.* to access de-registered op
module: dispatch	Python Dispatcher registrations beyond BackendSelect do nothing
module: dispatch	string interning for dispatcher operator names
module: dispatch	PT Dispatcher confusing error message "There were no tensor arguments to this function"
module: dispatch	Python dispatch for PyOps needs to respect tensor subclasses
module: dispatch	[Discussion] Add custom device
module: dispatch	Inconsistent naming convention for end of enum in DispatchKey
module: dispatch	Dispatcher debug/logging mode
module: dispatch	Python operator registration API for subclasses
module: dispatch	[bug] Device dispatcher can choose CPU path for CUDA tensors.
module: dispatch	Expose docs from the yaml for each torch.Tag in Python
module: dispatch	test_python_dispatch fails on DEBUG=1
module: dispatch	Many dispatch keys do not print to string correctly
module: dispatch	NewOperatorRegistrationTest.testImplNoDefGetsCaught failed.
module: dispatch	register_dispatch_key should provide a way to tell if a native function has a no-op meta kernel
module: dispatch	Give a better error message when REGISTER_DISPATCH is used in improper context
module: dispatch	Incorrect reference to previous kernel in the warning displayed when overriding a previously registered kernel for the same operator and the same dispatch
module: dispatch	Add a __torch_pre_dispatch__ that will live at the top of the dispatcher
module: dispatch	Supporting Pytorch for Custom Compiler Backend
module: dispatch	Conjugate fallback dispatch key should be per-backend
module: dispatch	Make KernelFunction::makeFromUnboxedFunctor infer KernelFunctor from input argument
module: dispatch	Create a boxed fallback / template recipe for Autograd that forwards, but errors on backwards
module: dispatch	Dispatcher doesn't handle ops with an empty list of tensors (e.g. torch.cat())
module: dispatch	Add a few tests to make sure new dispatch keys for backends are added properly.
module: dispatch	Simplified API for custom inplace & view kernel
module: dispatch	Delay errors of inference tensor to backward pass?
module: dispatch	Renaming Autograd dispatch keys to ADCreateGraph
module: dispatch	JIT prim ops v.s. dispatcher
module: dispatch	Dispatcher TLS to bypass loads of dispatch key from tensor arguments
module: dispatch	Generate delegates for all non-structured kernels
module: dispatch	Split VariableTypeManual.cpp
module: dispatch	cdist skip for manual backward is kind of janky
module: dispatch	Dispatch table for linalg_norm is fishy
module: dispatch	Dispatcher documentation needs update
module: dispatch	Improve error message for "Could not run 'aten::record_stream' with arguments from the 'CPU' backend. "
module: dispatch	Unknown DispatchKey when indexing tensor with named dimensions
module: dispatch	Eliminate redundant device guards in generic dispatch key kernel wrappers
module: dispatch	Staged backend boxed fallback (per-operator precomputation / precompute)
module: dispatch	Simple functions shouldn't go through dispatcher
module: dispatch	Use macro to define DispatchKey::toString method
module: dispatch	Inference performance regression caused by hacky_wrapper_for_legacy_signatures
module: dispatch	Avoid dynamic isCustomClassRegistered() checks in kernel call paths
module: dispatch	prelu_backward, hardshrink_backward shouldn't be a method
module: dispatch	Can't registered boxed kernel for operator that doesn't support boxing
module: dispatch	Make operator registrations truly commutative using priority
module: dispatch	Deprecate and remove RegisterOperators
module: dispatch	Rename Dispatcher::findSchema to Dispatcher::findOperator
module: dispatch	Operation Registration Error
module: dispatch	"Tried to register multiple operators with the same name and the same overload name" error is confusing
module: dispatch	Natively Declarable Fast-path Functions
module: dispatch	Fold DispatchStub into c10 dispatcher
module: dispatch	TestTorch.test_c10_layer_norm fails if you run it on a build of PyTorch with BUILD_CAFFE2_OPS=0
module: dispatch	[c10] c10 dispatch doesn't support tracing of scalars
module: dispatch	Dispatch Tracing/Debugging
module: dispatch	Optional modifiers (e.g., Tensor?) are not checked for non-dispatched native functions
module: nccl	inconsistency for enableTiming / FlightRecorder / duration_ms
module: nccl	Undefined reference to ncclCommSplit
module: nccl	is PGNCCL abortCommsFromMap working correctly?
module: nccl	NCCL watchdog thread terminated with exception
module: nccl	PyTorch is shipped with different versions on NCCL
module: nccl	Need latest NCCL support to reduce GPU HBM consumption
module: nccl	about nccl not work
module: nccl	Fine-tuning HuggingFace wav2vec 2.0 with torch.compile
module: nccl	Support datatype argument for torch.distributed.all_gather() (And the whole distributed module)
module: nccl	tag parameter is ignored from NCCL P2P isend/irecv pair
module: nccl	torch.distributed.all_reduce allocates excess GPU memory when using NCCL backend
module: nccl	several questions about pytorch DDP
module: nccl	RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Timeout waiting for key: default_pg/0/0 after 1800000 ms
module: nccl	Cudnn batch norm kernel (batchnorm_bwtr_nhwc_semiPersist) gets blocked by overlapping NCCL all_reduce calls
module: nccl	ProcessGroupNCCL is relying on UB to support bool data type
module: nccl	RuntimeError: Connection reset by peer when backened by NCCL
module: nccl	processes hang when executing cross-machine asynchronous P2P communication on NCCL backend
module: nccl	Add NCCL and MPI version printing to torch.utils.collect_env
module: nccl	NCCL Backend does not support ComplexFloat data type
module: nccl	Inconsistent multi-node latency with NCCL and OpenMPI
module: nccl	NCCL Network is unreachable / Connection refused when initializing DDP
module: nccl	wait() does not block the default stream for NCCL's asynchronous P2P operations
module: nccl	Failed to create Gloo new group after initialized with NCCL
module: nccl	c10::CUDAError
module: nccl	init_process_group hangs for multi-node, Pytorch > v1.3.1 and file init_method
module: nccl	Make streams used for NCCL operations configurable
module: nccl	nccl comm init needs a global barrier.
module: nccl	Backend capability chart for nccl GPU send/recv may be stale
module: nccl	getNcclVersion() in NCCLUtils.cpp handles "2.10.3" incorrectly
module: nccl	Seeing unit test failures for ProcessGroupShareTensorTest
module: nccl	NCCL multi-gpu test intermittently failing after NCCL version upgrade
module: nccl	"uncorrectable NVLink error" making the tests fail
module: nccl	Remove single device constraint from ProcessGroupNCCL profiling
module: nccl	all reduce hangs and does not throw exception when CUDA_VISIBLE_DEVICES is not set properly using the NCCL backend
module: nccl	Initialize NCCL backend with MPI
module: nccl	THCCachingAllocator::cuda_free_mutex has no effect
module: nccl	"distributed" NCCL tests fail when having more than 3 GPUs
module: nccl	Can't initialize NCCL/GLOO process group if default process group is MPI
module: nccl	Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.
module: nccl	NCCL WARN Your program may be hanging, this may be caused by a collective mismatch around rank 2. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs
module: nccl	RuntimeError: ProcessGroupNCCL does not support recv (torch-nightly)
module: nccl	[FR] NCCL and bool type
module: nccl	RuntimeError: broken pipe from NCCL
module: nccl	Bump up NCCL to 2.7.3
module: nccl	[distributed] calling nccl reduce with inconsistent dst hangs
module: nccl	RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8
module: nccl	[doc] document cuda.nccl
module: nccl	[FR] Support SyncBatchNorm in DataParallel
module: nccl	torch.cuda.nccl.init_rank does not handle "uid" properly, causing runtime error
module: nccl	NCCL fails to find cuda include dir
module: nccl	RuntimeError: NCCL error in ProcessGroupNCCL.cpp:290, unhandled system error
module: nccl	Libtorch build error when setting both USE_GLOO and USE_SYSTEM_NCCL to ON
module: nccl	TORCH_CUDA_API export failure on torch::cuda::nccl::detail::throw_nccl_error(ncclResult_t)
module: nccl	Using Tensor.to(device) after distributed all_reduce intermittently causes deadlock with NCCL
module: nccl	Add torch.version.nccl
module: nccl	Problem installing from source on CentOS 6.5
module: nccl	NCCL backend fails when calling broadcast from different threads
module: nccl	distributed data parallel, gloo backend works, but nccl deadlock
module: nccl	Properly release NCCL resources
module: nccl	Consider disallowing Variables that require grad in NCCL/comm functions
module: determinism	Raise warning if determinism setting mismatches in compiled backward
module: determinism	torch.compile doesnt respect use_determistic_algorithms during the backward()
module: determinism	yolov5_train
module: determinism	max_pool3d_with_indices_backward_cuda and avg_pool3d_backward_cuda does not have a deterministic implementation
module: determinism	Severe performance regression on deterministic algorithm in torch 2.0
module: determinism	Bits types cannot be used under deterministic mode
module: determinism	adaptive_avg_pool2d_backward_cuda is not deterministic
module: determinism	upsample_bilinear2d_backward_out_cuda is nondeterministic
module: determinism	Determinism by using datapipes shuffle
module: determinism	max_pool3d_with_indices_backward_cuda does not have a deterministic implementation
module: determinism	Conv1d step-by-step numerical error
module: determinism	Can't reproduce/non-deterministic results with CUDA
module: determinism	Tool for identifying where in eager model an operation is nondeterministic
module: determinism	Add a deterministic version of reflection_pad2d_backward_cuda
module: determinism	Non-deterministic results when training a model on GPU with MPS backend
module: determinism	[libtorh]Consistency problem of gpu computing
module: determinism	torch.compile breaks reproducibility
module: determinism	Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities
module: determinism	Feature Request: deterministic CUDA cumsum
module: determinism	pack_padded_sequence not compatible with deterministic mode it calls torch.scatter
module: determinism	Add nondeterministic alert to torch.Tensor.scatter()
module: determinism	Feature Request: Deterministic Algorithm for MaxPool3d
module: determinism	Update use_deterministic_algorithms documentation and tests to include nn.functional counterparts for all nn modules
module: determinism	Feature Request: deterministic adaptive_avg_pool2d_backward_cuda
module: determinism	Randomness should be consistent across devices with use_deterministic_algorithms
module: determinism	How to turn off determinism just for specific operations, e.g. upsampling through bilinear interpolation?
module: determinism	Tensor operation hangs when used with multiprocessing
module: determinism	Inconsistent computation of gradient in MaxUnPooling
module: determinism	with torch.backends.cudnn.flags(deterministic=True) doesn't give an exception for ctc_loss backward on CUDA
module: determinism	max_unpool2d is not deterministic
module: determinism	torch.unique() nondeterministic behavior on nan inputs (on GPU)
module: determinism	conv3d has numerical issue where same input produces output that are not bit-wise identical
module: determinism	Large cumulative sums appear to be nondeterministic.
module: determinism	C++ Context::setDeterministicAlgorithms default 2nd arg not defined in header
module: determinism	Toggling deterministic mode for individual autograd backward functions
module: determinism	Feature Request: Deterministic MaxPool3d and AvgPool3d
module: determinism	torch.linalg.lstsq is nondeterministic
module: determinism	conv3d padding=same gradgradcheck fails on CUDA
module: determinism	Add nondeterministic alert to torch.scatter_
module: determinism	Deterministic implementation for upsample_bilinear2d_backward_cuda
module: determinism	Deterministic implementation for grid_sampler_2d_backward_cuda
module: determinism	Setting a random seed or passing a generator to Modules for deterministic sampling.
module: determinism	[Feature Request] Deterministic implementation for AdaptiveMaxpool1d.
module: determinism	Use PYTHONHASHSEED during pytorch build to avoid nondeterminism
module: determinism	Why the same training code running on different GPUs (V100 vs P100) brings a significiant difference in outputs and thus train loss / model accuracy?
module: determinism	setting  all seed still get different result.
module: determinism	Tolerance for non-determinism operators in gradcheck
module: determinism	OpInfo mechanism to test for nondeterminism
module: determinism	Add branch predictor hints to prefer Context::deterministicAlgorithms() == false
module: determinism	deterministic implementation for adaptive_avg_pool2d_backward_cuda
module: determinism	Reproducibility breaks down with weighted Cross Entropy loss
module: determinism	Crossentropy inconsistent results depending on tensor order
module: determinism	Doc update regarding predictability of experiments using Seeds and Workers
module: determinism	Stochasticity for DistributedDataParallel on CPU but not on GPU
module: determinism	nn.LSTM gives nondeterministic results with dropout and multiple layers, OR cuDNN version mismatch
module: determinism	DataParallel gives different gradients when using LSTMs
module: determinism	Make topk sort stable
module: determinism	Deterministic mode for scatter_add operation
module: determinism	Feature Request: deterministic CUDA torch.nn.CTCLoss
module: determinism	Gather backward is faster than integer indexing on GPU
module: pooling	incorrect output shape for nn.AvgPool2d with ceil_mode=True
module: pooling	max_pool2d_with_indices_backward_out_cuda: remove useless  code gradInput.zero_();
module: pooling	AdaptiveMaxPool documentation is not detailed
module: pooling	max_pool1d, max_pool2d, max_pool3d Integers for cpu and cuda
module: pooling	max_pool3d_with_indices_backward_cuda does not have a deterministic implementation
module: pooling	Unrelated error messages with torch.nn.AdaptiveAvgPool3d
module: pooling	Overflow when running torch.nn.AdaptiveMaxPool2d
module: pooling	AdaptiveAvgPool1d failed in the lower version
module: pooling	AdaptiveAvgPool1d throws different exceptions when using the gpu
module: pooling	MaxPool1D output shapes can be negative when ceil_mode=True
module: pooling	AvgPool2D output shapes are inconsistent when ceil_mode=True
module: pooling	max_unpool3d will trigger an assertion fail under compute sanitizer
module: pooling	Feature Request: Deterministic Algorithm for MaxPool3d
module: pooling	Revisit OpInfo samples for nn.functional.max_poolNd
module: pooling	max_unpool gives wrong gradient when indices has duplicate
module: pooling	max_unpool2d is not deterministic
module: pooling	max_pool1d can succeed when padding is negative for tensor requiring grad
module: pooling	max_pool1d(): RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory
module: pooling	max_pool1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1
module: pooling	Segmentation fault in max_pool1d
module: pooling	Segmentation fault in fractional_max_pool3d
module: pooling	Segmentation fault in fractional_max_pool2d
module: pooling	max_unpool2d returns a tensor with negative dimension
module: pooling	Feature Request: Deterministic MaxPool3d and AvgPool3d
module: pooling	input.dim() == 4 INTERNAL ASSERT FAILED mkldnn/Pooling.cpp:201
module: pooling	nn.functional.max_unpool(2|3)d: failing shape check for correct inputs (with dilation > 1) with specified output_size
module: pooling	nn.functional.max_unpool{n}d: shape checks fail with output_size=(C, ...).
module: pooling	bug when loss backward, related to AdaptiveAvgPool1d
module: pooling	Incosistency with args for nn.functional.max_poolNd vs nn.MaxPoolNd functions
module: pooling	F.max_pool1d docs are really bad
module: pooling	[Feature Request] Deterministic implementation for AdaptiveMaxpool1d.
module: pooling	MaxPool2D Returns Wrong Shape With Ceil_Mode
module: pooling	Caught an integer overflow or wraparound in torch.nn.MaxPool1d
module: pooling	[docs] FractionalMaxPool2d : _random_samples arg is present in signature but undocumented.
module: pooling	fractional_max_pool{2, 3}d inconsistent between CUDA and CPU
module: pooling	max_pool2d CPU forward performance is poor
module: pooling	The pooling code is ignoring sliding windows that start within the right padding
module: pooling	deterministic implementation for adaptive_avg_pool2d_backward_cuda
module: pooling	Pooling code does not allow sliding window starting in right padded region
module: pooling	Median / quantile / mode / rank / percentile pooling
module: pooling	max_pool2d always compute indices even when it's not required
module: pooling	Refactor the adaptive avg pool code
module: pooling	Different max_pool2d cpp signatures due to indices.
module: pooling	max_unpool2d and max_unpool3d cpp signature should be similar
module: pooling	[feature request]Support dilation parameter for unfold2d_* function (slow cpu maxpool2d #28733)
module: pooling	F.max_pool*d/F.min_pool*d should support integer dtypes and bool tensors
module: pooling	Dedicated inverse AdaptiveMaxPool1d operation (e.g., AdaptiveMaxUnpool1d)
module: pooling	CPU MaxPool2d is very slow
module: pooling	fractional_max_pool2d_with_indices silently ignores output_ratio if output_size is provided
module: pooling	[Feature Request] Flattened indices option for max pooling
module: pooling	MaxPool with n-dimensional tensors
module: pooling	Global Second Order Pooling
module: pooling	AvgPool2d doesn't test if kernel is smaller than input size
module: pooling	No test coverage for kwargs of AvgPool2d and AvgPool3d
module: pooling	Missing dilation from several pooling modules (AvgPool)
module: pooling	[feature request] More options for Fractional Max Pooling
module: pooling	[Bug] Dilated max-pooling fails due to padding check
module: pooling	avg_pool functions hold input for backward
module: nestedtensor	Implement KV cached attention with NestedTensor
module: nestedtensor	at::_efficient_attention_forward takes optional max_seq_len_k
module: nestedtensor	Nested tensors fail on Conv2D
module: nestedtensor	Dynamic shapes not properly supported for nested tensor / tensor subclasses
module: nestedtensor	Add a function to torch.nested to create nested tensors from a buffer and sizes
module: nestedtensor	[Tracker] Move nested tensors to beta
module: nestedtensor	Backward pass for Nested Tensors using flash attention in sdpa fails
module: nestedtensor	Clean way to distinguish python subclass NT vs. C++ NT
module: nestedtensor	Torch Nested Issue With Backward Pass In Transpose
module: nestedtensor	Certain torch functions are not handled by torch func wrapper
module: nestedtensor	Allow slicing of Nested Tensors along constant dimensions
module: nestedtensor	Padded tensor subclass
module: nestedtensor	Nested Tensor with PyG dataset custom class
module: nestedtensor	Calling pin_memory() fails for nested tensor
module: nestedtensor	[discussion] "TensorList" as first-class abstraction (including python frontend) and as key for dispatch for merging torch._foreach_* into regular torch.* functions
module: nestedtensor	More Nested Tensor Functionality (layer_norm, cross_entropy / log_softmax&nll_loss)
module: nestedtensor	Multi-output derivative formulas can save unnecessary tensors
module: nestedtensor	TransformerEncoder truncates output when some token positions are masked by src_key_padding_mask across batch
module: nestedtensor	Let Nested Tensor Metadata be cached on GPU
module: nestedtensor	test_layer_norm_backward and test_layer_norm_backward_5d run OOM in slow gradcheck
module: nestedtensor	Enrich shape operations with nested tensors
module: nestedtensor	EmbeddingBag to support mini-batches with offsets
module: nestedtensor	Update map_nt to take into account size and strides
module: nestedtensor	Clean up nt impl duplicates where one can
module: nestedtensor	Support arbitrary masks for _nested_tensor_from_mask in nn.TransformerEncoder
module: nestedtensor	[discussion, idea] Batched, vectorized base64 decoding / encoding + maybe RLE decoding / encoding
module: nestedtensor	binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor
module: nestedtensor	Zero-copy way to make flat tensor into a nested_tensor given a shape
module: nestedtensor	Add eq, to, masked_select, index_select, narrow to nested tensors
module: nestedtensor	Broadcasting add for nested_tensor
module: nestedtensor	NestedTensor 2.0 issue tracking
module: nestedtensor	Ensure ops account for offsets and strides
module: nestedtensor	[Nested Tensor] Enable Nestedtensor to work with OpInfos
module: nestedtensor	General NestedTensor op coverage tracking issue
module: nestedtensor	Tracking nested tensor functions with backward kernels registered in derivatives.yaml
module: nestedtensor	[Nested Tensor] Move nested tensor specific ops to nested namespace
module: nestedtensor	[Nested Tensor] view + inplace for autograd.
module: nestedtensor	[Nested Tensor] Update TestCase.AssertEqual
module: nestedtensor	Slice operation on "ragged" dimension in NestedTensor
module: nestedtensor	Move nested-tensor tutorial from prototype
module: nestedtensor	Use NestedTensor in RNN models
module: nestedtensor	nn.InstanceNorm and nn.GroupNorm are affected by padding, so they need to masking
module: nestedtensor	Implement shape/size functions for nestedtensor
module: nestedtensor	2-dimensional arange
module: nestedtensor	Implement NestedTensor size function
module: nestedtensor	Add Autograd Support for Nested Tensor
module: nestedtensor	Getting NotImplementedError when trying to implement E2E support for prim::is_nested Op in torch-mlir.
module: nestedtensor	Add nesting of nested Tensor
module: nestedtensor	[feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion
module: nestedtensor	Potentially misleading note in documentation for PackedSequence
module: nestedtensor	Silent failing of batch_sampler when the data points are lists of tensors.
module: nestedtensor	First element in data passed to torch.*Tensor constructors cannot be a tensor
module: nestedtensor	[FYI] NestedTensor Project Progress
module: nestedtensor	Pad PackedSequences to original batch length
module: nestedtensor	[feature request] Support tensors of different sizes as batch elements in DataLoader
module: mkldnn	[discussion] Route pointwise Conv1d/Conv2d to matmul?
module: mkldnn	[Intel GPU] Enable oneDNN GEMM Primitive
module: mkldnn	Intel GPU oneDNN upstreaming for library compilation
module: mkldnn	python -c "import torch;print(torch.nn.GELU()(torch.rand(2)))" crashes on aarch64
module: mkldnn	Unsupported operator error: aten::to_mkldnn export to ONNX not supported
module: mkldnn	[fbcode] s,\bnp\.bool\b,bool,
module: mkldnn	Add caffe2 ideep/onednn tests to OSS CI
module: mkldnn	reseed all Generators in Dataloader's _worker_loop() -- via GC
module: mkldnn	oneDNN kernel fails to compile
module: mkldnn	PyTorch can not be compiled with MKLDNN if system compiler is clang
module: mkldnn	PyTorch installs the file mkldnn.cmake that looks for the package MKLDNN that doesn't exist
module: mkldnn	mkldnn matmul kernel may be slower than openblas kernel for very small tensor shapes
module: mkldnn	Build Error: no matching function for call to ‘dnnl::graph::stream::stream(<brace-enclosed initializer list>)’
module: mkldnn	Build Error: OpenMP library could not be found.  Proceeding might lead to highly sub-optimal performance.
module: mkldnn	WIP: feat: LARS optimizer
module: mkldnn	Adding a linear layer leads to failure of optimize_for_mobile
module: mkldnn	Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight
module: mkldnn	Build failure using GCC 11.2.0
module: mkldnn	Unable to build and use libtorch function via pybind11: undefined symbol error upon import
module: mkldnn	input.dim() == 4 INTERNAL ASSERT FAILED mkldnn/Pooling.cpp:201
module: mkldnn	comile error
module: mkldnn	SIGILL, Illegal Instruction from libtorch_cpu.so when callling backward() function in a VM.
module: mkldnn	Conv2d kernel performance regression on CPU since PyTorch 1.9
module: mkldnn	Magma : Intel MKL Errors
module: mkldnn	Memory Leak in MKL OpenMP on AVX2 machine
module: mkldnn	Compilation error with gcc11 and libstdc++11 (thread_id != operator removed)
module: mkldnn	PyTorch(with mkldnn) in inference mode using non-optimal prop_kind for convolution
module: mkldnn	Incorrect trace with MKLDNN (adding scalar)
module: mkldnn	Unable to add a scalar to an MKLDNN tensor
module: mkldnn	[Mkldnn] has_bf16 check only works on Linux for tests
module: mkldnn	Support for mkldnn + ddp
module: mkldnn	conv_transpose3d returns different result when the input and kernel are mkldnn tensors
module: mkldnn	Support for oneDNN / MKL-DNN on AArch64
module: mkldnn	libtorch 1.5 macos crash when loading on some mac
module: mkldnn	Compilation errors
module: mkldnn	DNNL's backward pass much slower when using nn.grad.conv2d_input and nn.grad.conv2d_weight
module: mkldnn	[1.4.1] Intel MPI libs not found.
module: mkldnn	MKLDNN_conv2d 2X slower than the native TH implementation
module: mkldnn	Enable OpaqueTensor to possess Storage then allow it to view from CPUTensor
module: mkldnn	[feature request]Support dilation parameter for unfold2d_* function (slow cpu maxpool2d #28733)
module: mkldnn	PyTorch build with Ideep support?
module: mkldnn	Some questions Concerning Intel's DNNL(MKLDNN) support in Pytorch (adding support for Intel Processors GPUs) by transitioning to DNNL
module: mkldnn	MKLDNN doesnt work and is slower than normal cpu mode
module: mkldnn	torch runtime error when manual link libmkldnn.so
module: mkldnn	TSAN failure related to mkldnn
module: mkldnn	MKLDNN+AMD BLIS path for PyTorch
module: mkldnn	Accelerate PyTorch just-in-time compilation using MKL-DNN
module: mkldnn	Add support for serializing Mkldnn Tensor
module: mkldnn	(LLD 8.0.0) ld: error: can't create dynamic relocation R_X86_64_DTPOFF32 against symbol: ideep::utils::computation_cache
module: mkldnn	[RFC] Adding MKL-DNN Int8 functions to PyTorch/Aten/JIT backend
module: mkldnn	[RFC] Memory format (aka layout aka NHWC) support
module: mkldnn	[CMake] Linking against Intel OpenMP
module: mkldnn	Invoking MKL in multiprocessing with importing torch causes blocking
module: mkldnn	Feature Request: CPU performance optimization with MKL-DNN
module: typing	Merge all 3 mypy lint jobs
module: typing	Enable mostly-complete import-following for MYPYINDUCTOR
module: typing	Tensor.new_empty type annotation does not accept SymInt
module: typing	Overly strict type hints for torch.utils.data.random_split
module: typing	Incorrect type hint for torch.library.Library.define
module: typing	Re-enable test_typing
module: typing	Support Sequence type in JIT
module: typing	Mis-annotated return for F._no_grad_embedding_renorm_ (also JIT related)
module: typing	Type misalignments in nn.functional (also JIT related)
module: typing	Generate complete annotations for torch._C._nn
module: typing	Merge type stubs for torch.nn.functional
module: typing	Typing missing on arithmetic ops on Tensor
module: typing	Inplace binary ops on tensor subclasses can cause mypy error
module: typing	Increased / more verbose type aliases for improved readability of user defined content
module: typing	Sequence annotation in type hints is wrong
module: typing	Wrong type for get_lr inside lr_scheduler.pyi
module: typing	Look into test coverage for UntypedStorage
module: typing	Broken mypy check in test_type_hints.py::TestTypeHints::test_doc_examples
module: typing	DISABLED test_doc_examples (__main__.TestTypeHints)
module: typing	Wrong output type hint for F.one_hot
module: typing	Better type annotations for torch.Tensor subclasses
module: typing	torch.Tag doesn't have accurate mypy info
module: typing	torch.concat type hints fail for keyword argument
module: typing	Tensor.backward type hints clarification
module: typing	three typing inconsistencies on Tensor methods
module: typing	Add typing support to ModuleList and ModuleDict
module: typing	Pyre type checking fails
module: typing	mypy typing strategy for Tensor-likes (__torch_function__)
module: typing	Change the type hint for nn.Module.__call__ to be friendly to overrides.
module: typing	Pytorch Typing, for Tensor type annotations
module: typing	torch.utils.data.Sampler is not recognized as a collections.abc.Sized
module: typing	torch.nn.Linear wrong type annotation for bias
module: typing	quicklint mypy step fails with assertion
module: typing	[typing] new_ones has wrong signature
module: typing	[typing] ModuleDict ctor has wrong signature
module: typing	_foreach_maximum returns "incompatible type" mypy error unexpectedly
module: typing	Add better python operators annotations for IDE type checking
module: typing	ModuleList not recognised as reversible by mypy
module: typing	Inconsistent function name between stub and implementation in torch.optim.swa_utils
module: typing	Enable torch.nn.modules.pooling typechecks during CI
module: typing	A few years after #701 and PyTorch is still using implicit __all__ imports.
module: typing	Fix the way imports are done to be more correct for static type checkers
module: typing	Module __call__ typing
module: typing	Type hints from _VariableFunctions and elsewhere clash
module: typing	Enable torch.optim typechecks during CI
module: typing	LSTM::permute_hidden breaks Liskov substitution principle
module: typing	Enable torch.quantization.fuse_modules typechecks during CI
module: typing	Enable torch.testing._internal typechecks during CI
module: typing	[typing] overly restrictive List[int]
module: typing	Force JIT to do type inference even when mypy annotated
module: typing	Missing OneCycleLR and MultiplicativeLR in lr_scheduler.pyi
module: typing	Override __call__ instead of forward
module: typing	RuntimeError: arg_types.size() == param_names.size() - (moduleSelf_ ? 1 : 0) INTERNAL ASSERT FAILED
module: half	arctan2 fp16 error when optimising
module: half	[RFC] Add GradScaler on CPU
module: half	[ONNX][dynamo] Failed to export cumsum with dtype=float16
module: half	fp16 parity issue with traced code on GPU
module: half	NAN appears in the backward results of masked.cumprod on macos
module: half	[inductor] Triton matmul templates should use reduced_precision_reduction flags
module: half	Fix permuted sum precision issue for lower precision on CPU
module: half	conv2d wrong results on 3090/3090ti
module: half	Add half specializations for load of sum
module: half	There is a big precision error between A100 and 3090 when using torch.matmul with fp16 precision
module: half	pca_lowrank and svd_lowrank broken under automatic mixed precision.
module: half	torch.fmod produces inconsistent results in eager and compile mode
module: half	[RFC] CPU float16 performance optimization on eager mode.
module: half	[RFC] quantile should work for float16/half on the GPU
module: half	quantile fails for float16/half inputs
module: half	A segment fault can be triggered in fbgemm_pack_gemm_matrix_fp16
module: half	amp with bf16: backward happens in f16 when using @torch.cuda.amp.custom_bwd
module: half	RuntimeError: "slow_conv2d_cpu" not implemented for 'Half'
module: half	relu-gru mse is 0.022 much greater than 0.003 with half dtype.
module: half	addcdiv_ (in and out of place) not implemented for torch.float16 and cpu
module: half	[chalf] reference_testing: low quality test for fast growing ops
module: half	Sampling Issue With Distributions
module: half	ComplexHalf Coverage Tracker
module: half	storage does support complex32 tensor
module: half	torch.nn.functional.{instance, batch}_norm trigger INTERNAL ASSERT FAILED when input is empty tensor with complex32
module: half	Accuracy problem of torch.batch_norm_gather_stats_with_counts when running_mean is half tensor
module: half	Inconsistent behavior of cosine_similarity between fp16 and fp32 inputs
module: half	torch.stft - fill_cuda not implemented for ComplexHalf
module: half	layer_norm needs to be done in fp32 for fp16 inputs
module: half	No support for torch.trace on CPU for float16 tensors
module: half	A modest proposal: delete arithmetic overloads from c10::Half
module: half	Use fast gpuAtomicAdd for FP16 data type in CUDA kernels
module: half	[bug] torch.topk sometimes supports float16 and sometimes doesn't
module: half	Half precision support for torch.sparse.mm
module: half	Some float16 inputs to CPU matmul are supported, others aren't
module: half	torch.arctanh not implemented for torch.float16
module: half	More AVX2 vectorization support for half (float16)
module: half	Accept objects with __float__ wherever regular floats are accepted
module: half	RuntimeError: "log2" "_vml_cpu" not implemented for 'Half'
module: half	Half Normal Log_Prob not defined for 0
module: half	"LayerNormKernelImpl" not implemented for 'Half' - CPU
module: half	for CNN in fp16 execution time depends on input scale
module: half	Errors when coercing complex numbers of various sizes
module: half	Cannot print 32-bit complex tensors
module: half	RuntimeError: "threshold_cpu" not implemented for 'Half'
module: half	nn.functional.interpolate backward in fp16 is extremely slow
module: half	RuntimeError: "triangular_solve_cuda" not implemented for 'Half'
module: half	[discussion] Generic solutions for too-small-epsilon in FP16 training
module: half	torch.nn.functional.normalize epsilon too small for half precision
module: half	Half precision cdist
module: half	Default adam epsilon to 1e-7 when on fp16
module: half	Group Norm Error When using FP16
module: half	Backward pass over torch.nn.functional.pad is extremely slow with half tensors
module: ux	torch.set_printoptions overwrites settings of its own previous calls
module: ux	ArgMax for Multi Dimensional Tensor
module: ux	Ability to assign to tensor.require_grad might lead to bugs
module: ux	Inplace and out variants for positive operator in PyTorch
module: ux	[feature request] making pytorch less noisy
module: ux	softmin and softmax operators support different input dtypes based on whether the dtype kwarg is passed
module: ux	[ux] F.binary_cross_entropy (and maybe other losses) to auto-cast bool category mask to float
module: ux	[feature request] Sticky/force regime of train/eval modes
module: ux	[ux] Print tensor's dtype by default
module: ux	A sieve operation for separating values of a tensor into two tensors based on a condition.
module: ux	Proposal: allow using axis, axes, dim and dims interchangeably
module: ux	Naming convention for variants of operations that never short circuit
module: ux	[docs] Tensor.bernoulli_ formatting is hard to read and UX inconsistent with the function variant
module: ux	Resolved: Only add type promotion support to unary pwise, binary pwise, and reduction operations
module: ux	Sparse-sparse matrix multiplication only works with torch.sparse.mm()
module: ux	Make searchsorted and bucketize API consistent
module: ux	[ux] Proposal to have t() === transpose(-1, -2), since batches are very frequent
module: ux	Complex->Real cast is a warning, calling real or imag on non-complex tensors is an Error.
module: ux	torch.tril_indices is incompatible with np.tril_indices
module: ux	Multinomial without replacement produces samples that have zero probability
module: ux	rfc: automating the switching of inputs to the device of the params
module: ux	Inconsistent type of property stride of Conv1d and MaxPool1d
module: ux	[feature request] Show warning if optimizer.zero_grad() was not called before optimizer.step()
module: ux	Allow all torch.nn modules to accept arbitrary batch dimensions
module: ux	Support deleting a parameter/buffer by name
module: ux	Mixing Numpy's arrays and PyTorch tensors
module: ux	PyTorch NaN behavior and API design
module: ux	Better support for operators that return (named) tuples of tensors
module: ux	Unite/unify tensor.unfold with F.unfold and make them more performant (zero-copy or little-copy with stride tricks, as in NumPy)
module: ux	renorm dim argument is extremely confusing
module: ux	out-variant for tensor.bitwise_and (exists for torch.bitwise_and) + bitwise_friends
module: ux	Naming inconsistency: padding_mode vs pad_mode + F.conv* docs ops miss padding_mode arg at all
module: ux	Deprecate type() and type_as() call
module: ux	torch.ceil, torch.floor should accept a dtype argument
module: ux	torch.multinomial is misnamed.
module: ux	User-friendly handling of types and devices
module: ux	Allow range in dim argument of reducing operations such as sum
module: ux	More dynamic PyTorch APIs
module: ux	Integrate torch.xxx and Tensor.xxx
module: ux	torch.masked_fill missing out argument
module: ux	Convert manually bound cuda cpu byte float operators to native_functions
module: ux	Stop binding in-place methods to torch.*
module: ux	Consider changing the behavior of Tensor.__contains__(Tensor) to make more sense
module: ux	make torch.utils._download_url_from_file public and add a docstring
module: ux	torch.fill_() exists and modifies the input tensor: Expected or bug?
module: ux	Mysterious Tensor Indexing Problem
module: ux	Deprecate torch.add(tensor, value, other)
module: ux	torch.cuda.is_available()  returns misleading value
module: ux	Should torch.arange take a layout parameter?
module: ux	[pytorch] Make dtype second positional argument of tensor factory methods
module: ux	Naming inconsistencies
module: vision	Static Linking C++, Op not available at runtime
module: vision	Possible memory leak when using Torch and Torchvision in conjunction with XGBoost
module: vision	tracing torchvision detection model results in an error
module: vision	[discussion, idea] Batched, vectorized base64 decoding / encoding + maybe RLE decoding / encoding
module: vision	Deep copy models with create_feature_extractor produces different parameters
module: vision	torch.package can not be used to serialize resnet18 from TorchVision-0.12
module: vision	[discussion] Consolidation of audio-visual I/O in a new package
module: vision	torchvision.models.mobilenetv3 can't save pre-trained model to custom dir?
module: vision	Cannot use socks5h proxy because of urllib: urlopen error Remote end closed connection without response
module: vision	RuntimeError: cuDNN error: CUDNN_STATUS_VERSION_MISMATCH for torchvision models
module: vision	Domain Transformation APIs for LibTorch and LibTorch-Lite
module: vision	API torch.ops.image.read_file reports RuntimeError - No such operator image::read_file
module: vision	NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend.
module: vision	Error while running FINETUNING TORCHVISION MODELS
module: vision	[Docker] Incompatible torchvision for 1.9.0-cuda10.2-cudnn7-runtime tag.
module: vision	Unknown builtin op: torchvision::nms in LibTorch
module: vision	TorchVision and TorchAudio wheels for AArch64 absent from https://download.pytorch.org/whl/torch_stable.html
module: vision	GoogleNet pretrained/scratch bug?
module: vision	IndexError: Input _features.0.weight is undefined!
module: vision	Add CutMix transform to torchvision.transforms
module: vision	slow data loading in VisionDataset  - need to allow batch loading.
module: vision	EMNIST looks different to MNIST
module: vision	RuntimeError: rois.device().is_cpu() ASSERT FAILED at /vision/torchvision/csrc/cpu/ROIAlign_cpu.cpp:386
module: vision	How do I derive weights for CrossEntropy Loss on my custom dataset?
module: vision	NaN Loss for FasterRCNN on Multiclass Object Detection on Custom Dataset COCO
module: vision	Torchvision error TypeError: _resolve_type_from_object()
module: vision	Can you add NMS,RoIAlign,RoIPool for libtorch?
module: vision	Resnet Model always predicting same label
module: vision	Allow grid_sample to accept pixel units (absolute coordinates)
module: vision	Unsupported ONNX op (Upsample 3D, bicubic) contrary to documentation
module: vision	PyTorch freezes on second call to scripted densenet model from torchvision
module: vision	torchvision
module: vision	What is the significance of torchvision._is_tracing()?
module: vision	Mnasnet0_5 first layer shape incorrect
module: vision	Dedicated inverse AdaptiveMaxPool1d operation (e.g., AdaptiveMaxUnpool1d)
module: vision	F.interpolate returns unexpected result when dealing with output size 1
module: vision	Scripting torchvision.models.detection.maskrcnn_resnet50_fpn
module: vision	Missing bin and include when building with torchvision on CentOS
module: vision	[BUG Report]Integrate libtorch to ffmpeg but memory leak happened!
module: vision	PyTorch 1.2 'module' object has no attribute 'BFloat16StorageBase'
module: vision	Training CNNs with deconvolution
module: vision	Not obvious how to install torchvision with PyTorch source build
module: vision	downsampling with grid_sample doesn't match interpolate
module: vision	torch.from_PIL() Request ?
module: vision	Deformable Convolution
module: vision	size mismatch when trying to reconstruct predifined network
module: vision	Feature request: SSIM/MS-SSIM
module: vision	[Feature Request] Extract glimpses from a batch of images (as in tf.image.extract_glimpse)
module: vision	support grid_sample with batch=1 but supprting batch affine parameters
module: deprecation	deprecate integral and boolean dtype support torch.logit and torch.special.logit
module: deprecation	Drop deprecated behavior from NumPy-style T
module: deprecation	nn.Softmax should not allow default/implicit/unset dim constructor argument
module: deprecation	Deprecation warning from SequentialLR
module: deprecation	Remove deprecated torch.qr
module: deprecation	Remove deprecated torch.cholesky
module: deprecation	Remove deprecated torch.chain_matmul
module: deprecation	RFC: Deprecate Bottleneck
module: deprecation	[POLL][RFC] DataParallel Deprecation
module: deprecation	Deprecation: Remove nn.functional.sigmoid
module: deprecation	Deprecate torch.cross with optional dim
module: deprecation	[nn] Passing dtype to _stacklevel argument in log_softmax silently works
module: deprecation	Replace unbiased parameter in torch.(std|var|std_mean|var_mean) with correction=0
module: deprecation	Deprecate torch.(min|max|median|mode) to only return values and not indices
module: deprecation	Some reduction operators have double signatures
module: deprecation	requires grad get lost during transform.
module: deprecation	Ensure warnings relate to user code with stacklevel
module: deprecation	backward compatibility - need a way to find out when a certain API was added/modified/etc.
module: deprecation	Remove support for multiple ellipses in slicing
module: deprecation	torch.(min|max)(..., dim=...) diverges from array API specification
module: deprecation	torch.size() diverges from array API specification
module: deprecation	Delete max_pool2d_with_indices_backward.grad_input?
module: deprecation	Finish deprecating torch.range
module: deprecation	Deprecations tracking issue
module: deprecation	Update linspace and logspace to throw an error when steps is not provided
module: deprecation	Deprecate torch.stft returning real-valued tensors and torch.istft accepting real-valued inputs
module: deprecation	Make searchsorted and bucketize API consistent
module: deprecation	Deprecate and remove torch.set_default_tensor_type
module: deprecation	torch.tril_indices is incompatible with np.tril_indices
module: deprecation	torch.Tensor.random_ is divergent from NumPy's np.random.random
module: deprecation	torch.meshgrid is divergent from np.meshgrid
module: deprecation	torch.transpose is divergent from np.transpose
module: deprecation	torch.equal is divergent from np.equal
module: deprecation	torch.cross is divergent from np.cross
module: deprecation	torch.Tensor.repeat is divergent from np.repeat
module: deprecation	torch.split is divergent from np.split
module: deprecation	torch.var and torch.std are not compatible with np.var and np.std
module: deprecation	Better support for operators that return (named) tuples of tensors
module: deprecation	Deprecate spmm and dsmm functions
module: deprecation	CuDNN RNN bindings are basically all deprecated in cudnn 8
module: deprecation	Deprecate type() and type_as() call
module: deprecation	SGD optimizer with deprecation warning
module: deprecation	torch.sum(tensor, dim=()) is different from np.sum(arr, axis=())
module: deprecation	torch.fill_() exists and modifies the input tensor: Expected or bug?
module: deprecation	Deprecate torch.add(tensor, value, other)
module: deprecation	[doc] many losses still mention size_average in formula
module: deprecation	Deprecate torch.Tensor
module: deprecation	torch.Tensor.new() disappeared in 0.4 doc
module: deprecation	Deprecate inplace argument in torch.nn.functional
module: testing	Early testing stop logic for CUDA error looks wrong for instantiated_test with pytest
module: testing	Add OpInfo for _weight_norm_interface.
module: testing	misusing percision value in test_cuda function in torch/testing/_internal/common_nn.py.
module: testing	type promotion test for torch.div variants is broken
module: testing	opinfo split is confusing
module: testing	Use expect tests for error inputs
module: testing	Command to reproduce error is incorrect
module: testing	[OpInfo] index.Tensor
module: testing	torch/testing/_comparison.py: If you are a user and see this message during normal operation please file an issue
module: testing	Undeterministic behavior in testing in dynamo.
module: testing	Unexpected Behavior when using torch.isclose()
module: testing	tools PYTHONPATH trick in run_test.py does not work reliably
module: testing	[BE] Dedup the functorch skipOps mechanism and the common_method_invocations one
module: testing	torch.testing.assert_close: allow check to fail on part on the input
module: testing	Proposal: Disable GC in test suite; GC after every test case
module: testing	Make torch.testing functions overrideable with torch_function?
module: testing	test_jit_fuser_te SIGIOT's frequently during dynamo testing
module: testing	Public API definition is not compatible with torch.testing
module: testing	Per-sample input xfail / test generation
module: testing	test/test_ops.py is segfaulting on master build with DEBUG assets
module: testing	Unit test with --subprocess command doesn't respect the -k filter flag and runs all available sub tests
module: testing	Replace same with TestCase assertEqual
module: testing	test_warp_softmax_64bit_indexing_cuda_float16 takes ~147GB of CPU memory and is very slow
module: testing	[Nested Tensor] Update TestCase.AssertEqual
module: testing	TestCommon.test_dtypes error message is confusing
module: testing	UnaryUfuncInfo Sample Generation Ignores sample_kwarg function
module: testing	unittest.subTest and way to selectively mark subTests as expected failures
module: testing	Schema information for torch.* operations
module: testing	in-place variants should get their own OpInfos
module: testing	Split up common_methods_invocations.py?
module: testing	Revisit OpInfo samples for nn.functional.max_poolNd
module: testing	fatal_signal_asan_no_sig_test in current master hang.
module: testing	Extend tag testing for aliases
module: testing	Test approximation and numerical stability of numerical operators
module: testing	Add Tensor compare support for MPS backend
module: testing	Some easy way to add xfails to OpInfos
module: testing	Feature Request: torch.testing.make_scalar (make_tensor for scalars)
module: testing	One should be able to query if a DecorateInfo is a xfail or skip
module: testing	instantiate_device_type_tests is misnamed
module: testing	Make OpInfo repr more useful
module: testing	How to get tolerance override in OpInfo-based test?
module: testing	TestCase.assertEqual has equal_nan default to True
module: testing	make_tensor tracking issue
module: testing	Tests should be runnable without run_test.py
module: testing	Cuda memory leak check is somehow unstable with repeat_test_for_types
module: testing	Find a good namespace home for torch._assert_async
module: testing	torch.equal is divergent from np.equal
module: infra	A100 runners down: apt-get install nvidia-docker2, Could not get lock /var/lib/dpkg/lock-frontend
module: infra	Please verify 1.14.1 ONNX release candidate on TestPyPI
module: infra	Command to reproduce error is incorrect
module: infra	add github check that diffs generated code
module: infra	Proposal: Disable GC in test suite; GC after every test case
module: infra	Add arm64 builds for libtorch on MacOS with mps support
module: infra	Make this ridiculously long error message more user friendly
module: infra	Hitting rate limits for pytorchbot token
module: infra	Migrate master to main: https://github.com/pytorch/tutorials
module: infra	Migrate master to main: https://github.com/pytorch/ignite
module: infra	Migrate master to main: https://github.com/pytorch/ELF
module: infra	Migrate master to main: https://github.com/pytorch/captum
module: infra	Migrate master to main: https://github.com/pytorch/glow
module: infra	Migrate master to main: https://github.com/pytorch/serve
module: infra	Migrate master to main: https://github.com/pytorch/xla
module: infra	Migrate master to main: https://github.com/pytorch/QNNPACK
module: infra	Migrate master to main: https://github.com/pytorch/tnt
module: infra	Migrate master to main: https://github.com/pytorch/hub
module: infra	Migrate master to main: https://github.com/pytorch/extension-cpp
module: infra	Migrate master to main: https://github.com/pytorch/android-demo-app
module: infra	Migrate master to main: https://github.com/pytorch/translate
module: infra	Migrate master to main: https://github.com/pytorch/elastic
module: infra	Migrate master to main: https://github.com/pytorch/tvm
module: infra	Migrate master to main: https://github.com/pytorch/ios-demo-app
module: infra	Migrate master to main: https://github.com/pytorch/accimage
module: infra	Migrate master to main: https://github.com/pytorch/extension-ffi
module: infra	Migrate master to main: https://github.com/pytorch/nestedtensor
module: infra	Migrate master to main: https://github.com/pytorch/cppdocs
module: infra	Migrate master to main: https://github.com/pytorch/workshops
module: infra	Migrate master to main: https://github.com/pytorch/extension-script
module: infra	Migrate master to main: https://github.com/pytorch/java-demo
module: infra	Migrate master to main: https://github.com/pytorch/csprng
module: infra	Migrate master to main: https://github.com/pytorch/pytorch_sphinx_theme
module: infra	Migrate master to main: https://github.com/pytorch/rfcs
module: infra	Migrate master to main: https://github.com/pytorch/add-annotations-github-action
module: infra	Migrate master to main: https://github.com/pytorch/ossci-job-dsl
module: infra	Migrate master to main: https://github.com/pytorch/pytorch-integration-testing
module: infra	Migrate master to main: https://github.com/pytorch/pytorch-ci-dockerfiles
module: infra	Migrate master to main: https://github.com/pytorch/labeler-github-action
module: infra	Migrate master to main: https://github.com/pytorch/pytorch
module: infra	Build release binaries with USE_GLOG=ON by default
module: infra	TestProfilerCUDA. test_mem_leak failing for CUDA 11.5 on Linux
module: infra	Add a GitHub actions workflow for Macos
module: infra	Use pytorch-probot for PyTorch-specific stuff + MOTD in Dr. CI comment
module: infra	[Meta] Change default branch name to main for repos in pytorch project
module: infra	Automate bumping of clang-tidy docker image tag on CI
module: infra	Too many labels in the repo
module: android	ModuleNotFoundError: No module named 'torchgen.code_template'
module: android	No pytorch_android 2.0.x builds
module: android	Pytorch mobile crashes on Android when loading a custom model
module: android	Are PyTorch Android nightly builds getting automatically published
module: android	pytorch-android-lite use its own libfbjni.so, which is not compatible with any other version at all..
module: android	[Android Studio] DefaultCPUAllocator: not enough memory: you tried to allocate 280166432 bytes
module: android	isObject()INTERNAL ASSERT FAILED at "../aten/src/ATen/core/ivalue_inl.h":115, please report a bug to PyTorch. Expected Object but got Tensor
module: android	Forward method slows down at some point if it is executed repeatedly on Android with NNAPI
module: android	Memory leak issue with pytorch_java_only 1.9.0 and libtorch 1.9.0+cpu
module: android	pytorch mobile: some quant models on android cause memory leak
module: android	Add nnapi serialization for module components in Wav2Vec2Model
module: android	It' so strange!!!
module: android	When compiling TAG 1.9.0 or master for android，An error occurred and the compilation failed！ABIS_LIST=arm64-v8a
module: android	issues flutter in android studio
module: android	Alternate Model Loading - Android
module: android	script/build_android nn::Module support
module: android	Enable fft  support for mobile builds
module: android	pytorch_mobile custom build Module.forward null reference.
module: android	RuntimeError: xnn_status_success == run_status INTERNAL ASSERT FAILED at "/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp":158
module: android	Vulkan: error -5 when trying to use it with Nvidia card
module: android	[pytorch android] use Vulkan backend crash
module: android	Pytorch Mobile on Android, LAPACK library not found in compilation
module: android	[Android] Upgrading to 1.9.0 causes NNAPI model loading to fail
module: android	Custom build of c++ libtorch for Android program or dynamic so lib does not reduce program or lib size
module: android	Inference ran on new thread leak memory on Android
module: android	Pixel shuffle support for PyTorch Mobile NN-API
module: android	In Android, the memory used by the tensor or model cannot be recycled. Is there any way to solve it?
module: android	Mobile Android: Could not run 'aten::quantize_per_tensor' with arguments from the 'Vulkan' backend
module: android	Error while converting model for NNAPI "Unsupported node kind ('quantized::batch_norm2d')"
module: android	RuntimeError after optimize_for_mobile
module: android	[Pytorch Mobile] Error running build_pytorch_android.sh
module: android	SIGSEGV at at::is_vulkan_available() invocation on Android
module: android	Build errors with USE_VULKAN=ON when cross-compiling for Android
module: android	Wrong link in readme for the android nightly version
module: android	link failed when using custom build pytorch on Android
module: android	arm64-v8a not compiling due to libpytorch_jni.so
module: android	[PyTorch Mobile] Can't use Vulkan backend
module: android	PyTorch mobile perf recipes, fails when building android
module: android	Android: allow preallocation of output buffers
module: android	./scripts/build_android.sh need support nn::module nn::Functional nn::Linear
module: android	nn::Sequential Link error in Android version
module: android	will you support SRU on mobile?
module: android	Is it possible to run an object detection android app by using Pytorch Mobile?
module: android	Torchscript used to work, but now it fails with VariableTensorId error
module: android	run ./android/run_tests.sh --warning-mode all   ,  show error.
module: android	C++ inference (CPU-only) stalls in Android, and crashes on Mac/Linux
module: multithreading	Race condition on shutdown involving PThreadPool and autograd
module: multithreading	Dynamo's eval_frame.c is not thread/subinterpreter safe
module: multithreading	Regression in text encoding
module: multithreading	PyTorch 2.0.0 encountered CUDA error: an illegal memory access was encountered
module: multithreading	PyTorch's packaged libgomp causes significant performance penalties on CPU when used together with other Python packages
module: multithreading	Improve checkpoint thread-safety
module: multithreading	Questions and Possible Features: Pytorch RPC 'future.wait()' will not release GIL which will block other thread's execution when using multithreading.
module: multithreading	torch.inverse multi-threading RuntimeError: lazy wrapper should be called at most once
module: multithreading	torch.randperm uses too much cpu, but not efficient.
module: multithreading	[bug] NATIVE and OMP parallel_for implementations are inconsistent.
module: multithreading	[feature request] no-param sort to exploit parallelization
module: multithreading	Failure to set number of threads on AWS Lambda
module: multithreading	Allow to shutdown persistent workers
module: multithreading	ThreadLocalState::setThreadLocalState is not setting the "enabled" flag of SavedTensorDefaultHooks
module: multithreading	CPU parallelization across batch has random faulty behavior on backward
module: multithreading	Add more explanation on multithreaded graph building of Autograd
module: multithreading	Exception in thread when using dataloader
module: multithreading	Crashed with terminating with uncaught exception of type std::__1::system_error: condition_variable wait failed: Invalid argument
module: multithreading	Overflow error in torch.linalg uncaught
module: multithreading	at::parallel_for created max_threads for inputs larger than GRAIN_SIZE
module: multithreading	Base parallelism on CPU count available to process rather than system total
module: multithreading	profiler crashes the program when trying to stop on different threads without context manager
module: multithreading	Memory Leak in MKL OpenMP on AVX2 machine
module: multithreading	Memory leak in multi-thread inference
module: multithreading	OpenCV causes backpropagation to get stuck
module: multithreading	Calling benchmark.Timer with default num_threads=1 disables parallelism permanently
module: multithreading	No tensor operations allowed inside at::parallel_for
module: multithreading	Detect OpenMP Loop and this application may hang warnings
module: multithreading	torch.eye(d) is slow and hogs cpu for d >= 182
module: multithreading	DataLoader with cv2 and some numpy/cv2 import order causes workers to not work
module: multithreading	Setting threads number to the number of default by torch.set_num_threads is faster than not setting it
module: multithreading	[DISCUSSION] RPC server-side ThreadLocalState
module: multithreading	Expose internal::GRAIN_SIZE through Python API.
module: multithreading	Creating Torch tensors slows OpenCV video reading a lot
module: multithreading	Pytorch openmp thread number tuning option for CPU trainning
module: multithreading	Unsafe use of at::parallel_for in current codebase
module: multithreading	Unified management of thread local variables
module: multithreading	Memory leak in multithreading environment when loading checkpoint
module: multithreading	Avoid non-POD data in thread_local
module: multithreading	Improve multithreaded random number generation (RNG)
module: multithreading	Unify Caffe2 and PyTorch OpenMP initialization
module: multithreading	Performance issue master (a25b79531)
module: multithreading	Give clearer guidance about multithreading in PyTorch, and how to disable it
module: multithreading	Pytorch is slow when only using CPU, and cannot utilize multicore of CPU
module: multithreading	Only one thread is used on macOS (super slow on CPU)
module: interpolation	interpolate with antialias=True on CUDA doesn't work if the difference of spatial size is large
module: interpolation	Incorrect gradient calculation for upsample nearest on CUDA
module: interpolation	grid_sample with relative grid
module: interpolation	How to perform unstructured interpolation
module: interpolation	torch.nn.functional.interpolate fails on some degenerate shapes, but passes on others
module: interpolation	Interpolation artifacts when using nn.interpolate, trilinear mode for 3D label images
module: interpolation	F.interpolate uses incorrect size when align_corners=True
module: interpolation	Behavior of torch.nn.functional.interpolate with unchanged output size and recompute_scale_factor=False
module: interpolation	Feature request: Grid sample on complex tensors with float grid
module: interpolation	Feature Request: Add constant padding_mode for grid_sample
module: interpolation	Functional grid_sample: example of padding_mode="reflection" description error
module: interpolation	Torch.nn.functional.interpolate() function memory release problem
module: interpolation	grid_sample should not require grid to be the same dimension as input.
module: interpolation	Make interpolation output size compatible with opencv, scikit-image and scipy for floating scale factor
module: interpolation	Function Request: scipy.ndimage.map_coordinates
module: interpolation	Feature Request: torch.functional.interpolate to quietly ignore "align_corners" when mode is set to "nearest"
module: interpolation	bilinear interpolate is very slow under mixed precision training mode.
module: interpolation	torch.lerp to support argument type promotion / broadcasting similar to torch.where
module: interpolation	torch.lerp and torch._foreach_lerp should support uint8 inputs (for vision), int16 (for audio), int32/int64 (for generality)  without (up)casting to float for  and/or dtype argument
module: interpolation	[docs] torch.lerp has unmatching signature and explained parameter names
module: interpolation	[feature request] F.interpolate_as
module: interpolation	Interpolation tracking issue
module: interpolation	Function request: scipy.interpolate.InterpolatedUnivariateSpline
module: interpolation	Function request: scipy.interpolate.griddata
module: interpolation	Function request: scipy.interpolate.RegularGridInterpolator
module: interpolation	Function request: scipy.interpolate.RectBivariateSpline
module: interpolation	Function Request: scipy.ndimage.zoom
module: interpolation	Function Request: scipy.interpolate.interp1d
module: interpolation	Function Request: np.interp
module: interpolation	Add parameter "half_pixel_center =False" to the Bilinear function
module: interpolation	nn.functional.interpolate backward in fp16 is extremely slow
module: interpolation	RuntimeError: derivative for grid_sampler_2d_backward is not implemented
module: interpolation	F.interpolate returns unexpected result when dealing with output size 1
module: interpolation	Add new interpolation modes to grid_sample
module: interpolation	Recommendations for Grid Sample/Affine Grid/Displacement Fields/Optical Flow
module: interpolation	Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results
module: interpolation	downsampling with grid_sample doesn't match interpolate
module: interpolation	F.grid_sample doesn't respect padding_mode when height of inputs is 1
module: interpolation	Bilinear interpolation behavior inconsistent with TF, CoreML and Caffe
module: interpolation	Reduce code duplication in interpolate and make it more generic
module: interpolation	[feature request] Different interpolation algos for 'grid_sample' function
module: interpolation	support grid_sample with batch=1 but supprting batch affine parameters
module: codegen	overloads can perhaps be more performant?
module: codegen	torchgen/gen_backend_stubs.py compatibility with DispatchStubs
module: codegen	Allow overriding __repr__ to call dataclass_repr (infinite recursion right now)
module: codegen	add github check that diffs generated code
module: codegen	Despite having aten::diag_embed.out, torch.diag_embed doesn't support out= argument
module: codegen	The autogenerated out variants via autogen: do not check that the dtype of the out kwarg via canCast.
module: codegen	Autogenerated out functions are missing at::cpu:: and co bindings
module: codegen	torch.Tensor.to.dtype_layout overload is not available in Python
module: codegen	torchgen.model.FunctionSchema.parse fails with following ops' schema
module: codegen	[bug] Device dispatcher can choose CPU path for CUDA tensors.
module: codegen	The codegen unconditionaly generate code even when it is not going to be used
module: codegen	Move _SKIP_PYTHON_BINDINGS to native_functions.yaml
module: codegen	Teach tools.codegen.api.translate about IValues
module: codegen	Figure out what to do with functions that take both Tensor and TensorOptions
module: codegen	ufunc codegen support for dtypes that are supported on CUDA but not CPU
module: codegen	Include Declarations.yaml in Libtorch distributions
module: codegen	Invalid handling of out args with type Tensor[] in native_functions.yaml
module: codegen	Codegen issues with Tensor(a!)? arguments in native_functions.yaml schemas
module: codegen	native functions should not be allowed to take in a grad argument
module: codegen	[python codegen] correctly plumb TensorOptions defaults through the python binding layer.
module: codegen	tools/autograd/derivatives.yaml doesn't support methods on optional tensor
module: codegen	Unnecessary python bindings and documentation for internal functions/ops
module: codegen	Support adding new keyword-only parameters without breaking FC
module: codegen	backward compatibility - need a way to find out when a certain API was added/modified/etc.
module: codegen	Issue: support auto generation of device check for sparse tensors
module: codegen	Use PYTHONHASHSEED during pytorch build to avoid nondeterminism
module: codegen	[codegen] generated inplace/out= wrappers don't have input checks
module: codegen	Python API binding code generation does not need to pack TensorOptions for xxx_like generators
module: codegen	TensorList upfront checks parameter types in argument parsing and throws an error unexpectedly
module: codegen	Let backends specify a schema version when registering kernels
module: codegen	[codegen] Change public C++ to accept out/mut arguments by const&
module: codegen	[codegen] Make it easier to codegen call to API
module: codegen	Scalar/Tensor arg type for op schemas
module: codegen	A few years after #701 and PyTorch is still using implicit __all__ imports.
module: codegen	Record shaping assertions and use them for tracing / scripting optimization and codegen
module: codegen	Support undispatched ops in codegen
module: codegen	[tools.codegen] Remove byte-for-byte compatibility code
module: codegen	out kwargs are sometimes inconsistent with returned named tuple field name (and in some cases, cannot be made consistent)
module: codegen	TensorOptions extensibility has rusted shut
module: codegen	Include expanded TensorOptions version of op in at:: namespace
module: codegen	Generated __init__.pyi contains invalid default values
module: codegen	Make the generator tools data model more explicit
module: LrScheduler	EPOCH_DEPRECATION_WARNING in ChainedScheduler.step
module: LrScheduler	Using ChainedScheduler with ReduceLROnPlateau leads to unexpected keyword argument error
module: LrScheduler	backwards compatibility about class _LRScheduler
module: LrScheduler	ReduceLROnPlateau increases learning rate exponentially, causing training to diverge
module: LrScheduler	OneCycleLR's state_dict includes a full reference to the optimizer
module: LrScheduler	[discussion] Integrate widely used utilities from fvcore into the core repo
module: LrScheduler	ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups.
module: LrScheduler	ExponentialLR unexpectedly calls step() when init argument last_epoch is larger than -1
module: LrScheduler	Multiple Learning Rate Scheduler for Specific Parameters Groups
module: LrScheduler	Wrong type for get_lr inside lr_scheduler.pyi
module: LrScheduler	Remove lr_scheduler.print_lr
module: LrScheduler	CosineAnnealingWarmRestarts but restarts are becoming more frequent
module: LrScheduler	Add plots of LRSchedulers to doc to make it easier to read
module: LrScheduler	Potential bug in torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
module: LrScheduler	Expand Learning rate scheduling to any optimization hyperparameter
module: LrScheduler	SequentialLR does not work correctly with multiple ConstantLR
module: LrScheduler	CosineAnnealingWarmRestarts does not update parameters added with add_param_group
module: LrScheduler	LambdaLR changes the learning rate in an undesired way
module: LrScheduler	Any plan to add Noam scheduling?
module: LrScheduler	Deprecation warning from SequentialLR
module: LrScheduler	Problematic ASGD Optimizer
module: LrScheduler	Adam optimizer doesn't work with CyclicLR scheduler but works with OneCycleLR.
module: LrScheduler	SequentialLR scheduler incorrect initialization
module: LrScheduler	Add pct_end parameter to OneCycleLR
module: LrScheduler	UserWarning: Seems like optimizer.step() has been overridden after learning rate scheduler initialization. Please, make sure to call optimizer.step() before lr_scheduler.step(). See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate   warnings.warn("Seems like optimizer.step() has been overridden after learning rate scheduler
module: LrScheduler	CosineAnnealingWarmRestarts should use integer epoch
module: LrScheduler	torch.optim.lr_scheduler.SequentialLR.get_last_lr() does not work
module: LrScheduler	last_epoch parameter of CyclicLR and OneCycleLR is not the number of epochs
module: LrScheduler	Support for arbitrary schedulers in SequentialLR
module: LrScheduler	SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments
module: LrScheduler	SequentialLR object has no attribute '_last_lr'
module: LrScheduler	[feature request] Provide functional form of scheduler formulas (and reconsider older decisions of not doing it)
module: LrScheduler	Some lr-schedulers docs seems to have typos/missing information
module: LrScheduler	The SequentialLR scheduler uses a deprecated pattern
module: LrScheduler	Allow LRScheduler to take in param_groups directly without an optimizer
module: LrScheduler	Add an LRScheduler interface for torch schedulers.
module: LrScheduler	SequentialLR have a question and why it use step(epoch)
module: LrScheduler	lr_scheduler.py  /  list index out of range
module: LrScheduler	Unexpected behaviour when resuming from checkpoint using CosineAnnealingLR
module: LrScheduler	MultiStepLR with different gammas for each parameter group
module: LrScheduler	add gamma to CosineAnnealingWarmRestarts so max lr can decrease in cycles
module: LrScheduler	Learning rate scheduler list index out of range
module: cublas	Potential cublas handle leaking
module: cublas	[cuBLAS] add int_mm_cuda shape check warnings
module: cublas	Severe performance regression on deterministic algorithm in torch 2.0
module: cublas	[feature request] [discussion] Include basic ctypes bindings for cudart/cublasLt/cublas/nvrtc/cudnn with stock PyTorch
module: cublas	CUBLAS_STATUS_NOT_SUPPORTED
module: cublas	CUBLAS_WORKSPACE_CONFIG can not be parsed
module: cublas	torch.svd fails on large matrices
module: cublas	Matrix multiplication performance regression in case of an additional dimension of size 1
module: cublas	2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs
module: cublas	[BUG] Poor torch.bmm performance on H100
module: cublas	CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv
module: cublas	CUDA error CUBLAS_STATUS_NOT_INITIALIZED
module: cublas	Stochastic Illegal Memory Access error mid-epoch on AWS p4d instances
module: cublas	bmm operator in bfloat16 has low TFLOPS for some tensor shapes with CUDA 11.6
module: cublas	torch.linalg.lstsq raises CUBLAS_STATUS_EXECUTION_FAILED for large B in CUDA tensors
module: cublas	importing open3d before pytorch causes matmul to produce a segfault
module: cublas	Review and refactor  the way libcublas static libraries are linked
module: cublas	RuntimeError: cublas runtime error
module: cublas	Ability to enabling/disabling cuDNN and cuBLAS API logging in PyTorch API directly
module: cublas	BFloat16 CUDA GEMM ops unsupported on Nvidia P100 (SM_60) on CUDA 11.3
module: cublas	Matrix multiplication broken on PyTorch 1.8.1 with CUDA 11.1 and Nvidia GTX 1080 Ti
module: cublas	simple matrix multiplication yields wrong result on Ampere (3080)
module: cublas	RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle)
module: cublas	torch.bmm incorrect with pytorch 1.7.1 and cuda 11
module: cublas	Run time error
module: cublas	Support for CUDA matrix multiplication on long (and other integer) tensors
module: cublas	CUDA error: an illegal memory access was encountered (laugh_kernel at ...../cuda/CUDALoops.cuh:112)
module: cublas	RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:27 for spectral norm
module: cublas	When I use cuda(), wg = th.matmul(extra_obs, extra_obs.transpose(-2, -1)) take a mistake
module: cublas	Significant speed difference between P100 and V100
module: cublas	Computing dot product of columns sliced from large matrix causes illegal memory access in CUDA
module: cublas	Overlapping strides not supported by cublas
module: cublas	RuntimeError: cublas runtime error
module: cublas	RuntimeError: cublas runtime error
module: cublas	CUDA large matrix-vector product (torch.mv) causes illegal memory access
module: POWER	Question  on XNNPACK support for PowerPC
module: POWER	Compile pytorch for ppc64 redhat8
module: POWER	Wrongly returns nan for vectorized complex numbers division on PPC/ZArch
module: POWER	Wrong vector shift results on PowerPC
module: POWER	test_memory_timeline fails on PPC due to extra temopraries
module: POWER	Power VSX vectorization support disabled
module: POWER	PR #88607 breaks build for POWER9 CPU
module: POWER	torch.jit.save() generates different contents in a file among different endian machines
module: POWER	ENORMOUS OVERHEAD from mp.get_context('spawn')
module: POWER	Install PyTorch from source on power machine
module: POWER	compilation error with PyTorch v1.11 for CPU on ppc64le
module: POWER	ppc64le build fail: invalid conversion from Bfloat16 in functional_base.h:180
module: POWER	Error /usr/local/lib/libopenblas.so: error adding symbols: File in wrong format while building pytorch for ppc64le
module: POWER	Pytorch build issues on PowerPC
module: POWER	complex128 autograd failures on PPC
module: POWER	test_grid_sample (from TestNN) fails on POWER
module: POWER	test_clamp fails on ppc64le
module: POWER	Compilation errors on power-pc
module: POWER	RuntimeError: test_autograd failed! Build success, test failed on IBM POWER9
module: POWER	Installing pytorch from source on Power9 (PPC64LE) + CUDA 10.2 + RHLE7
module: POWER	Compilation on Power fails with clang due to vec_xl
module: POWER	ASAN build breaks when using third_party/protobuf
module: POWER	ASAN build broken when using USE_ASAN=1
module: POWER	test_bottleneck_cuda fails on Power
module: POWER	Test failure in test_shared_allgather_nccl: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:537
module: POWER	test_backward_deadlock fails with "Directory not empty"
module: POWER	test_autograd failures on Power
module: POWER	Failure to build on Power9 due to FXDIV_SOURCE_DIR not being set
module: POWER	ppc64le: test cpp_extensions/rng_extension.cpp failure (without altivec override)
module: POWER	TestTorchDeviceTypeCPU.test_float_to_int_conversion_finite_cpu_uint8 is broken on PowerPC
module: POWER	Seg fault with test_rnn_retain_variables on ppc64le
module: POWER	Installing pytorch from source on labs.cognitiveclass.ai
module: POWER	[Installation]: Support conda/pip install with ppc64le(power8)
module: POWER	Have ppc64le docker images?
module: POWER	Autograd test failure on ppc64le
module: data	pin_memory() for a custom Python object
module: data	New feature: Balanced Sampler
module: data	Add _worker_end_fn_t to the DataLoader
module: data	Dtype hard-coded in DataLoader (for python floats).
module: data	RuntimeError: DataLoader worker (pid 11011) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit
module: data	"file_descriptor" multiprocessing sharing strategy works incorrectly in dataloading
module: data	Documenting __getitems__ for slicing support in torch.utils.data
module: data	Documenting IterableDataset's needing StopIteration for finite data
module: data	Dataloader extremely slow on in-memory datasets
module: data	Dataset  with Queue issue
module: data	Passing dict in datapipe/dataset will have memory leak problem
module: data	Observed regress in DataLoader spawn from PyTorch1.13 to PyTorch2.0
module: data	Issue with ShufflerIterDataPipe in torch 1.13.1
module: data	jit.fork stalls multiprocessing dataloader
module: data	Open file leak when dataloader is using persistent_workers and pin_memory AND you create multiple dataloaders.
module: data	Cannot import traverse_dps from torch.data.utils.graph
module: data	[feature request] Add support for a custom DatasetFetcher in DataLoader
module: data	Add balance flag to random_split
module: data	torch.utils.data.Dataset combined with pycuda issue
module: data	libtorch: DistributedRandomSampler uses the same random order in every epoch
module: data	Initialize DataLoader workers in parallel
module: data	How to implement bucket_by_sequence_length with IterableDataset and DataLoader
module: data	torch.utils.data.random_split example broken in 1.10
module: data	Domain Transformation APIs for LibTorch and LibTorch-Lite
module: data	Removal of BufferedShuffleDataset
module: data	[DataPipe] Add Test Cases to Ensure Correct Behaviors When IterDataPipes Reset
module: data	Ability to explicitly close/dispose a DataLoader
module: data	Support for stereo audio data in from torch.utils.tensorboard.SummaryWriter
module: data	Analytical metric package
module: data	Can we add try-except for list/slice indices even when auto_collation is True?
module: data	torch.utils.data.DistributedSampler allow uneven inputs
module: data	[types] torch.utils.data.{Dataset, Sampler} are not Sized
module: data	A random split function that return the datasets following specific target (label) distribution.
module: data	Generalized Data Class
module: data	[Enhancement] Increase user-friendliness of dataset.random_split
module: bfloat16	bfloat16 on 'grid_sampler_3d_cuda' not implemented
module: bfloat16	RuntimeError: "grid_sampler_2d_cuda" not implemented for 'BFloat16'
module: bfloat16	Implmenet kthvalue for bfloat16 on CUDA
module: bfloat16	Fix permuted sum precision issue for lower precision on CPU
module: bfloat16	Extreme slowdown of torch.mm for certain sizes and strides with bfloat16
module: bfloat16	torch.embedding: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype.
module: bfloat16	Add support for bfloat16 in torch.from_numpy()
module: bfloat16	torch.concat fails with float16 input in autocast(device_type=cpu) context
module: bfloat16	RuntimeError: view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16
module: bfloat16	cpu log1p for bfloat16 gives wrong result.
module: bfloat16	Compare oneDNN and OpenBLAS backend of PyTorch on arm64 architecture
module: bfloat16	Bfloat16 tensor .numpy() support
module: bfloat16	Can not access to "sbgemm" routine with user-defined OpenBLAS
module: bfloat16	prod_cpu not implemented for 'BFloat16'
module: bfloat16	Placing LSTM model on bfloat16 on GPU causes error
module: bfloat16	amp with bf16: backward happens in f16 when using @torch.cuda.amp.custom_bwd
module: bfloat16	Autocast with BF16 on CPU slows down model more than 2X
module: bfloat16	bfloat16 matmul gives incorrect result on CPU (without mkldnn)
module: bfloat16	bf16 strided tensor wrong calculation
module: bfloat16	bmm_sparse_cuda kernel for bfloat16
module: bfloat16	Implement histc for bfloat16 on CPU
module: bfloat16	Placing model on bfloat16 on CPU make it freeze/hang
module: bfloat16	torch.cuda.is_bf16_supported() seem to not work properly
module: bfloat16	Feature Request: Support prelu_cuda for BFloat16
module: bfloat16	Feature request: bfloat16 support for CUDA matmuls
module: bfloat16	ppc64le build fail: invalid conversion from Bfloat16 in functional_base.h:180
module: bfloat16	BFloat16 CUDA GEMM ops unsupported on Nvidia P100 (SM_60) on CUDA 11.3
module: bfloat16	[RFC] Extend Autocast to CPU/CUDA with BF16 data type
module: bfloat16	[testing] test_reference_numerics_extremal_clamp_cpu_bfloat16 fails on ci build with GCC5.4 and Python 3.6
module: bfloat16	Use rlibm for faster and more accurate floating point operations
module: bfloat16	Implement operator<< for bfloat16
module: bfloat16	There should be gating around BFloat16
module: bfloat16	BFloat16 numeric limits should contain more info
module: bfloat16	documentation for adding a new type via C++ extensions
module: docker	Document PyTorch docker release policies in RELEASE.MD
module: docker	How to handle CVE vulnerabilities in underlying operating system?
module: docker	ImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo'
module: docker	Docker images: faster linker for torch.compile
module: docker	Torch 1.13 for GPU breaks if libcublas is already present.
module: docker	Pytorch not calling to C code from a docker container
module: docker	Remove conda virtualenv from the docker image
module: docker	INTERNAL ASSERT FAILED -When using the PyTorch docker environment released by pytorch, a Vulcan support issue occurs
module: docker	[A ERROR in Docker] RuntimeError: CUDA error: no kernel image is available for execution on the device
module: docker	pytorch/pytorch cpu official Docker images
module: docker	RuntimeError: Tensors of type TensorImpl do not have numel
module: docker	Missing docker directory in tools/
module: docker	CMake Error: File /opt/pytorch/build_variables.bzl does not exist.
module: docker	Some unit tests are failing
module: docker	pytorch/pytorch:1.8.1-cuda10.2-cudnn7-devel docker container contains cudnn 8.2, not 7.x as the name implies
module: docker	Release pytorch docker images with newer python versions
module: docker	Mention docker build process in RELEASE.md and automate building those for release
module: docker	Torch version in docker container does not match tag
module: docker	Missing Docker image for 1.10.1
module: docker	Wrong PyTorch version in Docker image
module: docker	Python version is 3.7.11 in latest pytorch docker image
module: docker	[Docker] Incompatible torchvision for 1.9.0-cuda10.2-cudnn7-runtime tag.
module: docker	Is Python version of Docker image on DockerHub downgraded?
module: docker	pytorch/manylinux-cuda102 support for aarch64
module: docker	Better shared memory allocation under Docker
module: docker	Error building docker image: No module named 'typing_extensions'
module: docker	CPU Tensor with Python MP Freezing in Docker Container
module: docker	Torch hub: object has no attribute nms
module: docker	Dockerfile for people to quick-start contributing
module: docker	Docker issue for Pytorch 1.3
module: docker	Use standard docker image for XLA build
module: docker	Build the docker image from source, but torch.cuda.is_available()==false
module: docker	NVIDIA_DRIVER_CAPABILITIES env variable is missing in pytorch docker images
module: pybind	Undefined Symobl: pybind11::detail::type_caster<at::Tensor, void>::load(pybind11::handle, bool)
module: pybind	pybind11 SymNode binding is a footgun py::cast
module: pybind	Potential bug found with pybind11 dec_ref while gil released
module: pybind	Missing string parsing for some parameter types in python arg parsing logic
module: pybind	Silent promotion of bool to int in the dispatcher
module: pybind	Embedding Pytorch in C++ using pybind fails on interpreter shutdown
module: pybind	__torch_dispatch__ can result in returning None for an op that should return Tensors.
module: pybind	Segmentation fault when using C++/pybind11 module without also importing torch
module: pybind	Building pytorch from source results in "an object with that name is already defined" error on import
module: pybind	c++ use pybind11 to import torch  free(): invalid pointer
module: pybind	Cleanup usage of IS_PYTHON_3_9_PLUS in autograd engine
module: pybind	Audit destructors of classes bound using pybind11::class_ to see if they can block; such cases can deadlock
module: pybind	pybind11 Tensor type caster forces reference count bump
module: pybind	Use METH_FASTCALL protocol in Python bindings
module: pybind	A few years after #701 and PyTorch is still using implicit __all__ imports.
module: pybind	PyEval_SaveThread: the function must be called with the GIL held, but the GIL is released
module: pybind	segfault during shutdown with torch1.7
module: pybind	c10::string_view pybind11 custom type caster.
module: pybind	pybind11_object_dealloc error
module: pybind	Missing caffe2_pybind11_state in pip install after cmake.
module: pybind	Move all torch.Tensor methods to codegen
module: pybind	END_HANDLE_TH_ERRORS_PYBIND prevents pybind11 Exception translation
module: pybind	Remove Ops bound in Python Layer for Legacy Reasons
module: pybind	torch::tensor(scalar) behaves differently from at::tensor(scalar)
module: pybind	Tensor.__reversed__ breaks protocol for reversible objects
module: pybind	torch native functions cannot be used with inspect.signature
module: pybind	Error from PyTorch when finalizing Python embedded in C++
module: pybind	Importing open3d after PyTorch causes free(): invalid pointer
module: pybind	Bad overload order for zeros_like
module: pybind	[pytorch] Make dtype second positional argument of tensor factory methods
module: pybind	Issue when importing both retro (from OpenAI) and torch
module: pybind	Protect user from No module named _C import error
module: fft	torch.fft.ifft crashes for empty input
module: fft	Flawed testing of onesidedness in istft
module: fft	iSTFT gives wrong results for some batched input
module: fft	[Feature request] stft doesn't have pad_value argument
module: fft	Meta implementations of FFT operators often have incorrect strides
module: fft	FFT Samples Inputs with More than Three Dimensions
module: fft	rfftn and irfftn operations in pt2 return different results compared to v1.12.1
module: fft	gpu training  work well, but cpu training not work
module: fft	Support polyphase channelizer
module: fft	fft should ignore dims with shape 1
module: fft	Option to bypass NOLA check in torch.istft
module: fft	fft.fftshift, fft.ifftshift, roll not implemented
module: fft	iSTFT produces RuntimeError with center=False and Blackman/Bartlett/Hann windows
module: fft	MPS device ComplexFloat
module: fft	Speed of torch.istft
module: fft	torch.stft does not normalize non-rectangular windows correctly
module: fft	FFT operators are not supported on MPS device
module: fft	HIPFFT_EXEC_FAILED when using AMD GPU run FFT
module: fft	Feature request: [STFT] Add warning message if signal length is not a multiple of hop_length in torch.stft
module: fft	c2r fft input generation
module: fft	istft gradcheck fails on ROCm
module: fft	Feature request: FFT operations on Metal
module: fft	Modify convolution kernels for ops
module: fft	Enable fft  support for mobile builds
module: fft	[feature request] Low pass filtering of FFT results
module: fft	[feature request] Do zero-padding in high-frequency modes in ifft
module: fft	Deprecate torch.stft returning real-valued tensors and torch.istft accepting real-valued inputs
module: fft	Feature Request:Fast Cosine Transform
module: fft	test_fn_grad_fft_fftn_cpu_complex128 and test_fn_grad_fft_rfftn_cpu_float64 are failing under TSAN
module: fft	Slow convolution with large kernels, should be using FFT
module: fft	[feature request] Singular values and spectral norm for convolutional layers
module: vmap	RuntimeError: CUDA error: an illegal memory access was encountered using vmap and model ensembling call for cuda system
module: vmap	Batched tensor creation inside vmap
module: vmap	There is a performance drop because we have not yet implemented the batching rule for aten::mkldnn_rnn_layer_backward.
module: vmap	performance drop because batching rule for aten::_scaled_dot_product_attention_math is not yet implemented
module: vmap	Batching rule for aten::_scaled_dot_product_attention_math not yet implemented
module: vmap	vmap, jacrev, jacfwd, hessian, etc., in libTorch
module: vmap	vmap causes unpredictable behavior when combined with autocast
module: vmap	Batching rule not implemented for aten::unsafe_chunk
module: vmap	workaround for using vmap when .item() is being used internally
module: vmap	Batching rule for aten::_scaled_dot_product_efficient_attention
module: vmap	Performance Drop for linalg_ldl_factor and ldl_solve
module: vmap	Implementing the batching rule for aten::bucketize.Tensor.
module: vmap	Memory corruptions can be triggered in torch._remove_batch_dim
module: vmap	[feature request] PyTorch vmap for efficient Evolutionary Strategies
module: vmap	DISABLED test_op_has_batch_rule_nn_functional_conv_transpose3d_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)
module: vmap	Segmentation fault in _remove_batch_dim
module: vmap	torch._remove_batch_dim is interceptable by __torch_function__ / batch tensors don't print correctly
module: vmap	Vectorized Jacobian and Hessian errors with ffts
module: vmap	vmap performance warnings from jacobian
module: vmap	functorch transforms are silently incorrect with autograd.Function
module: vmap	OpInfos disabled for batched forward grad computation
module: vmap	Divergent code is needed to record usage streams on different TensorImpl types
module: vmap	[torch.vmap] Support second order grads
module: vmap	vmap gradgradcheck test fails for unfold operation
module: vmap	torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures
module: vmap	Printing a Tensor that is being vmap'ed over in C++ raises an error
module: vmap	Batched grad coverage rollup
module: vmap	Torch  _remove_batch_dim OP out-of-bounds access
module: vmap	Batched gradient computation w/ vmap, feature rollup
module: vmap	torch.vmap giving INTERNAL ASSERT FAILED error
module: vmap	RFC: torch.vmap
module: vulkan	Back out "[PyTorch][Vulkan] Refactor performance test binary"
module: vulkan	vulkan: fix gcc build errors
module: vulkan	[PyTorch][Vulkan] Refactor Vulkan operators performance test binary
module: vulkan	Can't build PyTorch 1.13.1 with Vulkan support
module: vulkan	how to workaround the error "don't have an op for vulkan_prepack::create_linear_context" ?
module: vulkan	Segfault when running vulkan program linked against libtorch
module: vulkan	Build errors in two Vulkan files
module: vulkan	[vulkan] missing aten::reflection_pad1d.out operator
module: vulkan	InstanceNorm operator support for Vulkan devices
module: vulkan	Support for Transformer Models on Android with Vulkan Backend
module: vulkan	Building PyTorch with Vulkan backend fails (1.13 and master)
module: vulkan	Bad string in GLSL shader
module: vulkan	We don't have an op for vulkan_prepack::conv2d_clamp_prepack but it isn't a special case.
module: vulkan	INTERNAL ASSERT FAILED for _jit_pass_vulkan_optimize_for_mobile (Google Colab)
module: vulkan	Add more Vulkan operations
module: vulkan	cant build with USE_VULKAN=1
module: vulkan	optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack
module: vulkan	[vulkan]compiling VulkanOpContext.cpp with some errors
module: vulkan	Adding Vulkan Support
module: vulkan	[vulkan] Vulkan backend fails creating tensor on x86_64 Linux
module: vulkan	Building PyTorch with Vulkan backend don't work
module: vulkan	Specifying USE_VULKAN=0 in launching build_android.sh does not disable Vulkan
module: vulkan	Vulkan: error -5 when trying to use it with Nvidia card
module: vulkan	[pytorch android] use Vulkan backend crash
module: vulkan	Can't perform any operation on Vulkan device - macOS M1
module: vulkan	Unable to get a Vulkan tensor using to('vulkan')
module: vulkan	Vulkan backend on desktop platforms
module: vulkan	[PyTorch Mobile] Can't use Vulkan backend
module: vulkan	GPU Vendor-Agnosticism via Vulkan
module: padding	Check for output_padding <= stride/dilation in ConvTranspose1d
module: padding	F.pad will accept 0 and negative values as parameter
module: padding	Request for deterministic support for reflection_pad2d_backward_cuda
module: padding	Circular padding error for 3D arrays
module: padding	torch.nn.ReplicationPad1d:The description of the exception information thrown is not accurate
module: padding	torch.nn.ReplicationPad2D Report "invalid configuration argument" Error under Compute Sanitizer
module: padding	nn.InstanceNorm and nn.GroupNorm are affected by padding, so they need to masking
module: padding	Need "valid" and "same" padding mode for convTranspose2d
module: padding	Specifying left-right-padding as tuple for asymmetric padding
module: padding	torch.nn.{Constant,Zero}Pad unexpectedly fail
module: padding	'replicate' padding in convolution is 77 times slower on cpu than 'zeros'
module: padding	Feature Request: Add constant padding_mode for grid_sample
module: padding	[feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion
module: padding	[bug] nn.functional.pad (circular) ubsan failure
module: padding	pad(mode='reflect') beyond input.size
module: padding	Add a NumPy-like pad function
module: padding	Circular padding in Convolution layers should not only be wrap for once.
module: padding	Silent incorrect running with zero padding for Conv1d
module: padding	Error message regarding Padding of Conv2d needs improving
module: padding	Allow F.pad(mode = 'reflect') when shape == pad
module: padding	Weird behavior in Conv2d padding when changed to 'reflect'
module: padding	segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large
module: padding	Enhance supported fill value type for constant pad in functional.pad
module: padding	Support "symmetric" reflection padding
module: padding	Optional seq_len argument to torch.nn.utils.rnn.pad_sequence
module: padding	n-dimensional non-constant padding functional
module: padding	reflective padding for 5D tensor
module: padding	[feature request] Adding Pre and Post padding functionalities to pad_sequence function
module: checkpoint	torch.compile + SAC: mutations in backward are not preserved
module: checkpoint	Tensor subclass is not preserved during backward with gradient checkpointing
module: checkpoint	[compile] DDPOptimizer + activation checkpointing not supported
module: checkpoint	Support for activation checkpoint on demand in custom function
module: checkpoint	torch.compile not working with gradient checkpointing
module: checkpoint	torch.utils.checkpoint should avoid updating BatchNorm statistics twice
module: checkpoint	Activation Checkpointing PT2 - AOTAutograd cannot handle set_rng_state
module: checkpoint	Internal assert when ctx.saved_tensors fails when saving results of an intermediate view tensor with torch.utils.checkpoint and use_reentrant=False
module: checkpoint	[Activation Checkpointing] Investigate pin_memory for CPU offload
module: checkpoint	reentrant torch.utils.checkpoint does not work with NamedTuple outputs
module: checkpoint	torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS
module: checkpoint	Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables?
module: checkpoint	pytorch's checkpoint_wrapper does not save memory while fairscale's checkpoint_wrapper saves huge memory
module: checkpoint	[Checkpoint] Support multiple unpack in saved tensor hooks
module: checkpoint	Corner cases of ShardedTensor checkpoint when using TorchRec
module: checkpoint	non-rentrant checkpointing uses same memory as non-checkpointed code
module: checkpoint	Allow users to pass use_reentrant=False to checkpoint_sequential
module: checkpoint	[RFC] Activation Checkpoint API improvements
module: checkpoint	torch.utils.checkpoint.checkpoint_sequential is not optimal
module: checkpoint	Dynamic tensor rematerialization
module: checkpoint	checkpoint_sequential breaks backpropagation
module: checkpoint	gradients inside gradient checkpoint
module: checkpoint	Using  torch.utils.checkpoint.checkpoint_sequential and torch.autograd.grad breaks when used in combination with DistributedDataParallel
module: checkpoint	nn.Module.forward signature with **kwargs
module: checkpoint	torch.utils.checkpoint is not compatible with nn.DataParallel
module: checkpoint	Checkpointing is slow on nn.DataParallel models
module: checkpoint	checkpoint(function, *args) should have the same requires_grad as function(*args)
module: deadlock	dataloader use lmdb stuck
module: deadlock	training hangs at line torch.cuda.synchronize()
module: deadlock	If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac
module: deadlock	fatal_signal_asan_no_sig_test in current master hang.
module: deadlock	Process hangs after calling conv2d() in pytorch 1.11.0 with CUDA 11.3
module: deadlock	Placing model on bfloat16 on CPU make it freeze/hang
module: deadlock	Dataloader hangs. Potential deadlock with set_num_threads in worker processes?
module: deadlock	Hanging Validation
module: deadlock	pytorch hangs during interaction with ray package
module: deadlock	Multiprocessing hang and queue short circuiting
module: deadlock	test_forward_mode_AD hangs for nn.functional.cosine_embedding_loss
module: deadlock	Deadlock in test_multiprocessing_spawn.py
module: deadlock	init_process_group hangs for multi-node, Pytorch > v1.3.1 and file init_method
module: deadlock	OpenCV causes backpropagation to get stuck
module: deadlock	Trying to initialise CUDA twice in a process with no visible devices hangs the process and terminal permanently
module: deadlock	pytorch DDP hangs at .backward() call
module: deadlock	Pytorch 1.7.1 hangs with multi-gpu, while Pytorch 1.6.0 works correctly
module: deadlock	While training the model, GPU util reaches 100% but no progress happends.
module: deadlock	hangs indefinitely at os.waitpid()
module: deadlock	Custom ops get stuck in multiprocess data loader under certain environments
module: deadlock	[distributed] calling nccl reduce with inconsistent dst hangs
module: deadlock	parallel_for may hang when called in main process and then on daemon process
module: deadlock	DataLoader workers fail to die
module: deadlock	Python hang after using torch.exp()
module: deadlock	Multi-gpu example freeze and is not killable
module: deadlock	The training always freezes after some epochs.
module: deadlock	distributed data parallel, gloo backend works, but nccl deadlock
module: ios	[Libtorch/iOS] Build for iOS as dynamic library fails in linker phase.
module: ios	App crashes when I attempt to run it with an iOS xcframework that relies on the LibTorch-Lite CocoaPod
module: ios	[LibTorch/iOS] Building with METAL support script is freezing
module: ios	[LibTorch/iOS] Unknown custom class type quantized.Conv2dPackedParamsBase. Please ensure it is registered
module: ios	LibTorch-Lite 1.13.0.1 Crash on iOS 12 on app startup
module: ios	Build from source,  Undefined symbol: c10::detail::maybe_wrap_dim_slow(long long, long long, bool)
module: ios	Could not run 'aten::as_strided' with arguments from the 'Metal' backend.
module: ios	AUTOGRAD is not working on IOS
module: ios	Pytorch on iOS (iPhone X & XR) throwing can't allocate memory exception. Ref Logs:
module: ios	iOS TestApp from mobile performance recipes tutorial doesn't build on macOS
module: ios	Re-enable DynamicQuantModule in iOS simulator tests
module: ios	Failed to run on iOS - Couldn't find an operator for aten::conv1d
module: ios	Why there are 8 flavors of iOS build jobs for every commit
module: ios	Add and Mul torch tensors on Metal (IOS)
module: ios	build_ios.sh prevents iOS.cmake from configuring ios deployment target correctly
module: ios	[LibTorch-Lite] Add a custom flag to build LibTorch-Lite with LAPACK included
module: ios	Error: copy_to_metal_ is implemented only for float dtype
module: ios	RuntimeError: xnn_status_success == run_status INTERNAL ASSERT FAILED at "/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp":158
module: ios	Unknown type name '__torch__.torch.classes.metal.Conv2dOpContext'
module: ios	error when trying to call "torch::jit::load"method, use Metal backend in PyTorch Mobile
module: ios	Impossible to run tests target with LibTorch as dependency inside a cocoapods.
module: ios	Bitcode enable for iOS
module: ios	After updating to Xcode 12 and LibTorch to 1.7.0. Facing issue when running unit test.
module: ios	Memory leak in nn.MaxPool2d layer when run on iOS
module: ios	Undefined symbol: torch::kNearest when building App iOS
module: ios	Apple Review Rejected. ITMS-90338: Non-public API usage In Pytorch For iOS
module: ios	iOS libtorch superpoint model bug
module: mkl	Support building pytorch using MKL ILP64 model.
module: mkl	Fix sparse windows on CPU with MKL
module: mkl	Conda Pytorch set processor affinity to the first physical core after fork
module: mkl	Linking error with Libtorch
module: mkl	Bug in torch.linalg.svd
module: mkl	Compatibility with newest MKL
module: mkl	build libtorch with the same mkl as Matlab
module: mkl	Slower performance of torch.mm method with sparse CSR tensor
module: mkl	Functions depending on SVD are broken for inputs with non-finite values with MKL 2022+ and OpenBLAS 0.3.15+
module: mkl	Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel(R) Math Kernel Library.
module: mkl	linalg.lstsq INTERNAL ASSERT FAILED
module: mkl	wsl2 install failed from source code
module: mkl	Failed to build with master.
module: mkl	Floating point exception in mkl_vml_serv_GetMinN () on a specific computer
module: mkl	Memory Leak in MKL OpenMP on AVX2 machine
module: mkl	Sparse CSR layout CPU backend tracking issue
module: mkl	MKL csr matmul issue
module: mkl	test_inverse setup is flaky using MKL>=2020.1 on certain CPUs and fails on CUDA
module: mkl	No MKL Compatible Conda installation for PyTorch 1.5
module: mkl	Build without MKL is not possible when MKL is installed
module: mkl	ATen not compiled with MKL support
module: mkl	How to set not to build libtorch_cpu.so and libmkl_*.so dependencies?
module: mkl	Slow (20-50x) RNN tutorial/example when torch is installed using pip comp. to conda installation
module: mkl	Invoking MKL in multiprocessing with importing torch causes blocking
module: mkl	Use the int64 version of MKL calls
module: mkl	Cannot find Intel MKL
module: xla	[PyTorch/XLA] XLAShardedTensor untraceable by Dynamo
module: xla	Try creating a bf16 tensor as a last resort of is_bf16_supported().
module: xla	Can't run autocast on XLA, if PyTorch was compiled with CUDA support.
module: xla	Warning in autograd init regarding to aten::reshape
module: xla	nn.Module.to(dtype) does not work for XLATensor
module: xla	Register a torch op for functorch.experimental.control_flow.map to lower
module: xla	TorchXLA - owner @JackCaoG
module: xla	[DTensor][XLA] refactor DTensor _xla API
module: xla	CapabilityBasedPartitioner returns invalid partitions.
module: xla	pytorch XLA document error
module: xla	Pytorch 2 compile + fsdp + transformers crash
module: xla	Unable to move torch.jit.load-ed models to XLA devices
module: xla	[Functionalization] Some ops need additional meta tensor support after functionalization
module: xla	functorch.functionalize doesn't error out with logcumsumexp.out
module: xla	[RFC] XLA Lazy Backend Support In DistributedTensor API
module: xla	[PyTorch/XLA] Improve the XLA PR landing process
module: xla	PyTorch/XLA's DDP XLABackend is broken by upstream change
module: xla	Could not start gRPC server flakiness in XLA tests
module: xla	Lazy Tensor Core Documentation Out-of-Date
module: xla	XLA not being tested in TestAutogradDeviceType
module: xla	/var/lib/jenkins/workspace/xla/test/test_mp_rendezvous.py potentially flaky
module: xla	How to reference a tensor variable from a superclass of torch.Tensor?
module: xla	Adding a device variable to a torch.Tensor subclass fails
module: xla	Installed successfully the torch-1.9 and torch_xla-1.9. RuntimeError requested XLA_GPU:0，but available devices are [CPU:0, XLA_CPU:0 ]
module: xla	index_copy_  test fails on PyTorch/XLA
module: xla	Use standard docker image for XLA build
module: lint	Enable ruff rule PLW1510 codebase wide
module: lint	Lintrunner on all files fails locally even though it passes in CI
module: lint	[2/N] Apply clang-tidy to c10 CUDA files
module: lint	Enable more flake8-pyi ruff checks
module: lint	lintrunner mypy raises error in numpy
module: lint	[mypy] skipping mypy for a few torch/fx and torch/_subclass files
module: lint	Support for Pylint
module: lint	Refactor linter adapters to avoid code duplication
module: lint	Add doc formatting check to lintrunner
module: lint	lintrunner doesn't give good error message suggesting lintrunner init
module: lint	lintrunner not working
module: lint	TYPEIGNORE lint run locally disagrees with CI
module: lint	pylint segfault
module: lint	make lint should advertise make setup_lint
module: lint	clang-tidy "error: do not use const_cast" cppcoreguidelines-pro-type-const-cast is counterproductive
module: lint	clang format hash mismatched for linux64
module: lint	clang-tidy shows false positive failure on PRs that rename files
module: lint	quicklint clang-tidy step fails with many unrelated errors
module: lint	quicklint mypy step fails with assertion
module: lint	Misreported linting messages
module: lint	clang format on OS X ssl verification failure
module: lint	Lint rule to forbid bare assert() in cuda
module: lint	Run clang-tidy on the aten folder?
module: lint	Lint rule to test for creation of tensor in native/ without options()
module: lint	Lint rule to prevent direct use of #pragma omp
module: lint	Lint check for non-Unicode characters in diffs / Unicode characters without coding
module: hub	Change progressbar for hub
module: hub	Proxy Options for Pytorch Hub
module: hub	Proxy/cache server option/hooks for downloading model checkpoints and dataset archive files in cloud environment
module: hub	SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model within a company that transforms SSL certificates for security purposes
module: hub	[Feature Proposal] Extend torch hub to better support cloud serving and edge deployment
module: hub	torch._dynamo.exc.BackendCompilerFailed: compile_fx raised TypeError: tqdm.__init__() got an unexpected keyword argument 'desc'
module: hub	What causes CPU to degrade when I load the weight with torch.hub.load()
module: hub	torch.hub.load local model
module: hub	Files downloaded with torch.hub should respect umask
module: hub	Cannot use socks5h proxy because of urllib: urlopen error Remote end closed connection without response
module: hub	[CTA] Let's Stamp Out Flaky Tests!
module: hub	Can't download torch vision models for older versions via torch.hub.load: "multiple choices"
module: hub	[feature request] torch.hub.load_state_dict_from_url to be replaced by a new good general download-a-file function and to also support local paths and google drive links / private github release links
module: hub	Possible security issue of torch.hub.load
module: hub	Crash loading fairseq based model
module: hub	Potential race conditions between multiple workers trying to download and cache the same file in torch.hub.load_state_dict_from_url and torch.hub.download_url_to_file <- duplicate dataset/model downloads across DDP workers
module: hub	torch.hub.load can confuse external python package with local python package.
module: hub	A more flexible torch.hub search strategy
module: hub	pickle is a security issue
module: hub	torch.load(..., weights_only=True) currently raises a Deprecation warning + [proposal] weights_only=True should become default for safe legacy-loading pickles
module: hub	Distutils Error in torch.hub Load()
module: hub	[FR] hub uses default github branch
module: hub	Refactor (a bit) torch.hub(.load)
module: hub	make torch.utils._download_url_from_file public and add a docstring
module: backend	PrivateUseOne backend does't support tensor shollow copy
module: backend	Custom backend not called for compiling backward graph
module: backend	when huawei NPU
module: backend	When will Huawei Shengteng atlas be supported
module: backend	[MPS] Track failures of test_module.py for MPS backend
module: backend	Integrate open device privateuse1 customized method registration
module: backend	Contribute to the privateuse1 backend.
module: backend	Add PrivateUse1 folder in aten/src/ATen
module: backend	Request custom backend device memory Allocator.
module: backend	new backend privateuseone with "to" op
module: backend	when I want to use a new backend, how to deal with the op with 'device' argument?
module: backend	[MPS] Improve the performance of torch.linear()
module: backend	[Discussion] Add custom device
module: backend	Improve the overall design of MPSGraphCache
module: backend	Move the MPSGuardImpl to inherit from NoOpDeviceGuardImpl
module: backend	Add Tensor compare support for MPS backend
module: backend	Supporting Pytorch for Custom Compiler Backend
module: backend	[Bug] RuntimeError: could not create a primitive on Xeon
module: backend	Vulkan backend on desktop platforms
module: backend	Optional enhanced logging for operator calls for backend implementor debugging
module: backend	Let backends specify a schema version when registering kernels
module: backend	Whats Pytorch's policy on adding support for a wider range of hardwares for training and inference?
module: backend	Support FPGA Xilinx
module: backend	Better error message in DataChannelTCP::_receive
module: logging	[dynamo][profiler] console spew of ..."torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored" for pages...
module: logging	AOTAutograd logging: log autograd graphs
module: logging	logging stack_info doesn't do anything
module: logging	torch._export has no logging
module: logging	AsyncCompile loses useful exception backtrace in __get_result
module: logging	Report name of defining class along side function name in Dynamo logs
module: logging	CompileId in Dynamo log messages should include restart analysis count
module: logging	Unnecessary record_stream call for backend:cudaMallocAsync
module: logging	[PT2.0][compile] torch._dynamo.config.log_level does not exist
module: logging	[RFC] Emit better Telemetry in PyTorch
module: logging	Logs output_code and inductor do not interact as expected
module: logging	Expand component configurable logging system to C++
module: logging	pytorch log level API and env var
module: logging	TORCH_WARN is executed just once per set of parameters
module: logging	[LibTorch] the C++ api needs detailed error reports like pytorch
module: logging	Modernize logging tensor in torch.testing._internal
module: logging	[Dynamo] Prints, logging, and warnings
module: logging	Feature: a consistent Python and C++ logging facility that handles different classes of warnings
module: logging	How to view VLOG information
module: logging	Create a standalone python execution viewer like livepython
module: logging	how could I print the log in source code
module: logging	libtorch: macros in logging_is_not_google_glog.h have very common names like CHECK or LOG
module: logging	pytest suppresses stderr from Python startup by default
module: logging	Make it easier to figure out what CuDNN convolution algorithm we actually chose
module: embedding	torch.embedding, weight[indices], torch.index_select returns random data with indices on meta device
module: embedding	Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0)
module: embedding	AddressSanitizer: heap-buffer-overflow in test_comprehensive_nn_functional_embedding_bag_cpu_bfloat16
module: embedding	Embedding layer tensor shape
module: embedding	torch.nn.functional.embedding_bag throws an exception when it runs on a CPU, but it runs successfully on a GPU.
module: embedding	Documentation: torch.nn.functional.embedding docs could more clearly state the requirement that weight be a 2D tensor
module: embedding	CI fails for test_compare_cpu_nn_functional_embedding_cuda_float32 which is not reproducible locally
module: embedding	CUDA graph capturing fails for nn.Embedding and large batch sizes
module: embedding	PyTorch Embedding Op with max_norm is not working as expected
module: embedding	EmbeddingBag: Does CUDA calculate error in EmbeddingBag forward when include_last_offset=True ?
module: embedding	Embedding isn't determinstic on linux-xenial-cuda11.3-py3.7-gcc7
module: embedding	torch.nn.functional.embedding behave differently in two cases of cpu and cuda
module: embedding	torch.nn.EmbeddingBag behave differently in two cases of cpu and cuda
module: embedding	Optionally include padding_idx items in the EmbeddingBag reduction
module: embedding	LazyEmbedding, an embedding layer with a dynamically sized vocabulary
module: embedding	embedding_bag has unexpected behavior when given offsets that are not monotonically increasing
module: embedding	Occured error in loss.backward() when using sparse=True in Embedding layer
module: embedding	embedding_bag(..., include_last_offset=True) should always error if offsets[-1] != input.size()
module: embedding	Proposing new features: Developing Echo State Network layers
module: embedding	Sorting in embedding_dense_backward_cuda takes very long time
module: batching	not yet implemented the batching rule for torchaudio::_lfilter
module: batching	Add batching rules for {view}_copy operators
module: batching	check_batched_forward_grad fails for torch.norm and related ops
module: batching	[discussion] torch.flatten to allow unsqueeze of inexisting dimension
module: batching	FeatureAlphaDropout doesn't drop channels for (C, D, H, W)
module: batching	the same input with different batchsize got different precision output
module: batching	[Feature request] Add batched matrix support for torch.diag
module: batching	Implement Truly Parallel Ensemble Layers
module: batching	Allow all torch.nn modules to accept arbitrary batch dimensions
module: batching	[feature request] batch_apply, a general-purpose device-agnostic batch iterator
module: batching	Add batched torch.combinations
module: batching	Support batch linear transformation
module: batching	torch batchwise max with indices
module: batching	Batched torch.eig() and gradient of torch.eig() for real eigenvalues
module: batching	[RFC] NestedTensor - 0.0.1
module: batching	Batch Normalization axis
module: batching	Batched Conv2d for sequence data
module: batching	A bug in parallel.data_parallel when module_kwargs is not None
module: batching	[Feature Request] cdist: pairwise distances between two sets of tensors with batch mode
module: batching	[Feature request] Batch eig/symeig functions (for small matrices, with CUDA)
module: tensorpipe	Import fails when both USE_TENSORPIPE=OFF and USE_DISTRIBUTED=ON.
module: tensorpipe	Torch RPC on multiple nodes with GPU returns a EOF error
module: tensorpipe	Gloo and TensorPipe depend on different version of libuv
module: tensorpipe	[AWS EC2 P3DN, EFA is enabled] Torch RPC tensorpipe/common/ibv.h:172 "": Operation not supported
module: tensorpipe	Key already registered with the same priority: uv
module: tensorpipe	Building fails on a node with libibverbs installed with no Infiniband hardware present on RHEL8.
module: tensorpipe	Error when building PyTorch from source in CentOS Linux 7
module: tensorpipe	Building with USE_TENSORPIPE=0 causes errors on import torch for MacOS
module: tensorpipe	Cuda RPC error when using then()
module: tensorpipe	build broken when USE_TENSORPIPE=OFF and USE_DISTRIBUTED=ON
module: tensorpipe	Support GPU/CPU communication in RPC
module: tensorpipe	Authentication for RPC
module: tensorpipe	Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.
module: tensorpipe	[RFC] Manage CUDA Stream in TensorPipe RPC Agent
module: tensorpipe	[RFC] Device Placement API for RPC
module: tensorpipe	[RFC] [RPC] Automatic retries of all requests in TensorPipe agent
module: tensorpipe	CUDA error: out of memory when running tensorpipe test_cuda
module: tensorpipe	[TensorPipe] Errors in pipeWrite should clear out the future in pendingResponseMessage
module: tensorpipe	[TensorPipe] Avoid wrapping the future message in order to do atomic test-and-set
module: tensorpipe	Guard Gloo and TensorPipe related code in RPC with #ifdef
module: special	Add Lambert W function as torch.special.lambertw
module: special	torch.polygamma inconsistent with scipy.special.polygamma for n >= 1
module: special	deprecate integral and boolean dtype support torch.logit and torch.special.logit
module: special	torch.lgamma CUDA driver error
module: special	torch.special.round doesn't support the same dtypes as torch.round
module: special	Elliptic Functions and Integrals
module: special	Orthogonal Polynomials
module: special	Gamma and Related Functions
module: special	Bessel and Related Functions
module: special	Feature Request: Add J0 J1 J2 H0 H1 H2 Bessel functions
module: special	Feature Request: torch.special.ellipe
module: special	Jacobian elliptic functions
module: special	Complete elliptic integral of the first kind
module: special	Exponentially scalable modified Bessel function of the second kind
module: special	zeta(1/2) returns nan
module: special	Feature request: Complex Number Support for Special Functions
module: special	[feature] torch.polygamma : Support Tensor for argument n
module: special	torch.special tracking issue
module: special	Function request: logerfc, logerfcx special functions
module: pytree	[pytree] extend pytree operations with is_leaf prediction function
module: pytree	[pytree] make context and children_specs as private implementation details
module: pytree	[pytree] update treespec children_specs access
module: pytree	[pytree] update treespec dict keys access
module: pytree	Implement is_leaf for tree_map
module: pytree	[pytree] traverse dict in sorted key ordering
module: pytree	[pytree] freeze attributes of TreeSpec
module: pytree	[BUG][pytree] equal dicts do not imply equal leaves and equal treespecs
module: pytree	[pytree] implement key path API
module: pytree	[pytree] support PyStructSequence types for Python pytree
module: pytree	RFC: [pytree] node registration namespaces
module: pytree	[pytree] Pytree node registration hygeine: deprecate global _register_pytree_node; only allow enabling registered pytree extensions locally
module: pytree	[pytree] pytree.tree_map does not respect type of torch.Size
module: pytree	[BUG][pytree] treespec serialization for locally defined classes and namedtuple types
module: pytree	DDP grads not synced when static_graph=True and module output is a dict subclass?
module: pytree	Functorch pytrees with custom iterables
module: pytree	[Dynamo] Enable dynamo.export for huggingface models w/ ModelOutput
module: pytree	pytreeify decorators
module: pytree	torch.fx.node.map_aggregate and torch.utils._pytree.tree_map do the same thing
module: elastic	PyTorch Distributed Elastic Launch Segmentation Fault with Python 3.12
module: elastic	[RFC] Support for Redundant Hosts in TorchElastic
module: elastic	P0: Improve failure trace back when crashed to identify the cause of a crash and the ranks that the crash, output the real traceback at last.
module: elastic	torchrun discarding --rdzv-endpoint when it should not
module: elastic	[RFC][TorchElastic] topology info in training apps/ranks
module: elastic	DDP Elastic "master_addr" resolution error in environment variables.
module: elastic	TORCHELASTIC_RESTART_COUNT doesn't seem to be broadcasted to all worker
module: elastic	[help] did torch.distributed.launch can be applied on k8s cluster with pytorch-operator
module: elastic	Using ddp training with different machine
module: elastic	In torchelastic support running worker rank 0 on agent rank 0 consistently
module: elastic	torchrun substitutes host names for IP addresses
module: elastic	When running GPT trainning with megatron,  the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
module: elastic	torch.elastic fails to shutdown despite crashed processes
module: elastic	elastic/rendezvous: _matches_machine_hostname doesn't resolve hostnames fully
module: elastic	[torch.distributed] Make dynamic_rendezvous log handler configurable
module: elastic	[RFC] Add torch.distributed.run as a console script in pytorch's setup.py
module: elastic	[bug] torch.distributed.elastic logging: Failed to print the first info statement
module: elastic	[documentation] torch.distributed.elastic: illustrate how to write load_checkpoint and save_checkpoint in Train Script
module: elastic	[elastic launcher] redirects/tee support for global rank
module: abi	Import order issue with torch and pybind11 Library Statically Linked to libstdc++
module: abi	Support Delay Loading of c10.dll in when using libtorch as a thirdparty library.
module: abi	Libtorch compile error when defining D_GLIBCXX_DEBUG
module: abi	after add /path_to_libtorch/libtorch/lib to LD_LIBRARY_PATH, I can't import torch_scatter.
module: abi	Pytorch CXX11 ABI version
module: abi	Run libtorch examples, export error "undefined reference to xxx"
module: abi	libtorch_cuda links against wrong libnccl.so
module: abi	Compilation from source fails
module: abi	Error while importing torch: libtorch_python.so: undefined symbol: PyThread_tss_alloc
module: abi	libtorch_cpu.so: undefined symbol: _ZN3c1010ThreadPool3runESt8functionIFvvEE
module: abi	C++ version 1.9.0 libtorch dynamic load fails -- GCC only
module: abi	Libtorch lock thread
module: abi	Status of pip wheels with _GLIBCXX_USE_CXX11_ABI=1
module: abi	[C++] Libtorch error in Release build only
module: abi	ABI backwards compatibility
module: abi	Segmentation fault when use torch::from_blob
module: abi	C++ API: Crash in cudnnDestroy() when deconstructing
module: abi	libtorch exports protobuf symbols
module: sanitizers	Race condition on shutdown involving PThreadPool and autograd
module: sanitizers	valgrind failure Conditional jump or move depends on uninitialised value(s)
module: sanitizers	torch.nn.CTCLoss Trigger out-of-bound Read under Compute Sanitizer
module: sanitizers	torch.nn.functional.embedding_bag Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.set_rng_state Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.Tensor.msort Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.linalg.eigvals Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.topk Trigger RuntimError under UndefinedBehaviorSanitizer
module: sanitizers	torch.vander Trigger RuntimeError with UndefinedBehaviorSanitizer
module: sanitizers	torch.svd_lowrank Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.linalg.lstsq Trigger RuntimeError under UndefinedBehaviorSanitizer
module: sanitizers	torch.mm Trigger RuntimeError with UndefinedBehaviorSanitizer
module: sanitizers	torch.nn.CTCLoss Trigger heap-buffer-overflow under AddressSanitizer
module: sanitizers	max_unpool3d will trigger an assertion fail under compute sanitizer
module: sanitizers	addcmul, byte_channels_last fail test_nnc_correctness opinfo tests under UBSAN
module: sanitizers	TSAN issue in autograd "set_next_edges"
module: sanitizers	test_fn_grad_fft_fftn_cpu_complex128 and test_fn_grad_fft_rfftn_cpu_float64 are failing under TSAN
module: sanitizers	Model loaded in C++ runtime is not thread safe
module: vectorization	Wrong vector shift results on PowerPC
module: vectorization	vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division
module: vectorization	Compiling PyTorch from Source on Xavier
module: vectorization	Fails to build on ppc64le with clang
module: vectorization	Build check for AVX512 fails with AMD CPU and march=native
module: vectorization	error: ‘_mm512_set_epi8’ was not declared in this scope when build from source on AMD 3600 CPU
module: vectorization	AVX512 and Vec512
module: vectorization	More AVX2 vectorization support for half (float16)
module: vectorization	[bug] torch.{sinh, cosh}: Incorrect values for vectorized path
module: vectorization	Better documentation of vec256 API
module: vectorization	Vec256<int64_t> does not handle LONG_MAX on minimum
module: vectorization	No vectorization for int8 and uint8
module: vectorization	Pytorch 1.4 compilation hangs on AMD Epyc
module: vectorization	Vectorized torch.eig()
module: vectorization	TensorIterator does not work with different input/output types
module: vectorization	Illegal instruction (core dumped) when running in qemu
module: vectorization	Better dev docs for writing native CPU kernels with Vec256
module: openmp	pytorch consuming all cpu cores 100% on ARM
module: openmp	Import of torch breaks standard multiprocessing
module: openmp	Unexpected modification to CPU affinity of Dataloader workers
module: openmp	Weird dataloader performance degradation caused by torch and numpy import order
module: openmp	large number of temporary files generated when using dataloader with num_workers>0
module: openmp	Upgrade to a newer llvm-openmp version to avoid /dev/shm pollution
module: openmp	I have the same issue as @samgelman on my MacOS.
module: openmp	PyTorch crashes when running with OpenACC
module: openmp	[feature request] DataLoader to accept num_threads argument to auto-set number of threads for OpenMP / intra-op parallelism
module: openmp	[bug] NATIVE and OMP parallel_for implementations are inconsistent.
module: openmp	Inconsistent multi-node latency with NCCL and OpenMPI
module: openmp	at::parallel_for created max_threads for inputs larger than GRAIN_SIZE
module: openmp	bundled libiomp5 causing segfaults in other libraries that use libomp
module: openmp	Segmentation Fault when importing torch on macOS Big Sur
module: openmp	Missing -fopenmp when used in torchvision
module: openmp	PyTorch wheel's own OpenMP library clashing with system-wide OpenMP library at runtime
module: openmp	[CMake] Linking against Intel OpenMP
module: printing	About the different ways to print models
module: printing	Symbolic tensors are not printable
module: printing	Add ability to add custom suffixes to tensor repr
module: printing	torch.set_printoptions overwrites settings of its own previous calls
module: printing	Idiom for extensible string printing for TensorImpl subclasses
module: printing	Include tensor shape in its default print formatting
module: printing	Consider adding context manager support for torch.set_printoptions
module: printing	Cannot print 32-bit complex tensors
module: printing	KeyError: 'track_running_stats' in batchnorm.extra_repr
module: printing	C++ tensor print doesn't show requires_grad and grad_fn like Python tensor print
module: printing	Options for printing the shape with print(tensor)
module: printing	bytearray(tensor) behaves very differently from bytearray(tensor.numpy())
module: printing	print uses lots of GPU memory
module: printing	Zero-dim Tensors (scalars) should be printed at full precision
module: printing	Port dragon4_scientific for pretty float tensor print.
module: printing	Storages still use legacy printing
module: mta	[Feature] _foreach_copy_ supports different src/dst dtype with fusion
module: mta	Consider adding y/x -> y * 1/x optimization for _foreach_div_.ScalarList and other div Scalar overloads
module: mta	_foreach_copy_ supports fast copy between cpu and cuda devices.
module: mta	_foreach_copy_ with scalar second arg
module: mta	Other overloads of _foreach_clamp
module: mta	torch.nn.utils.clip_grad_norm_() causes H2D sync with foreach ops.
module: mta	Avoid incrementing refcount of grad_fn in unpack_list
module: mta	[RFC] Let in-place foreach functions return a list of Tensors
module: mta	Improve _group_tensors_by_device_and_dtype
module: mta	test_foreach failing cuda memory leak check
module: mta	[Proposal] Use batched oprations to accelerate PowerSGD
module: mta	[CTA] Let's Stamp Out Flaky Tests!
module: mta	EMA optimizer: class-form and function-form (using new foreach_lerp) - can be used for explicit robust updates of BatchNorm stats
module: mta	[RFC] APEX style fused optimizers in PyTorch
module: mta	TestForeachCUDA.test_binary_op_tensorlists_fastpath__foreach_add_cuda_bool and TestForeachCUDA.test_pointwise_op_fastpath__foreach_addcmul_cuda_uint8 fail intermittently
module: mta	Foreach Functions Tracking Issue
module: unknown	DISABLED test_noncontiguous_samples_nn_functional_conv3d_cuda_float32 (__main__.TestCommonCUDA)
module: unknown	DISABLED test_variant_consistency_eager_nn_functional_conv3d_cuda_complex64 (__main__.TestCommonCUDA)
module: unknown	DISABLED test_make_fx_symbolic_exhaustive_round_decimals_neg_3_cpu_float32 (__main__.TestProxyTensorOpInfoCPU)
module: unknown	DISABLED test_make_fx_symbolic_exhaustive_round_decimals_0_cpu_float32 (__main__.TestProxyTensorOpInfoCPU)
module: unknown	DISABLED test_out_warning__refs_clamp_cpu (__main__.TestCommonCPU)
module: unknown	DISABLED test_fake_crossref_backward_no_amp_index_fill_cuda_float32 (__main__.TestFakeTensorCUDA)
module: unknown	DISABLED test_checkpoint_trigger (__main__.TestCheckpoint)
module: unknown	DISABLED test_nn_sequential_invocation_dynamic_shapes (torch._dynamo.testing.DynamicShapesMiscTests)
module: unknown	DISABLED test_variant_consistency_jit_linalg_lstsq_cpu_complex64 (__main__.TestJitCPU)
module: unknown	DISABLED test_inplace_grad_index_put_cuda_float64 (__main__.TestBwdGradientsCUDA)
module: unknown	DISABLED test_cuda_variable_sharing (__main__.TestMultiprocessing)
module: unknown	DISABLED test_fn_gradgrad_linalg_lu_factor_cuda_complex128 (__main__.TestBwdGradientsCUDA)
module: unknown	DISABLED test_variant_consistency_jit_linalg_lu_cuda_complex64 (__main__.TestJitCUDA)
module: unknown	DISABLED test_variant_consistency_jit_linalg_lu_factor_ex_cuda_complex64 (__main__.TestJitCUDA)
module: unknown	DISABLED test_variant_consistency_jit_linalg_lu_cuda_float32 (__main__.TestJitCUDA)
module: unknown	Can you provide the torch.trt module to directly convert the pytorch weights to tensorrt?
module: bazel	Support for Bazel workspace function or Bazel module
module: bazel	[bazel] add inductor to bazel build
module: bazel	Shared library loading logic breaks when CUDA packages are installed in a non-standard location
module: bazel	[bazel] replace //c10:headers dependency by //c10 dependency
module: bazel	path in WORKSPACE
module: bazel	move bazel files out of pytorch repo root
module: bazel	[bazel] [ci] //:lazy_tests Could not run 'aten::mul.Tensor' with arguments from the 'Lazy' backend
module: bazel	[bazel] [ci] //:module_test CUDA error: CUDA driver version is insufficient for CUDA runtime version
module: bazel	[bazel] build spams warnings
module: bazel	[bazel] ability to run gpu tests on gpu machines in RBE
module: bazel	Bazel fails in an obscure way if submodules are not initialized
module: bazel	PyTorch Profiler built with Bazel doesn't produce GPU trace
module: bazel	Bazel target all_tests improperly reports failures on CPU-only (non-CUDA) build
module: bazel	bazel build warning: Artifact 'torch/csrc/api/include/torch/version.h' is duplicated
module: bazel	error in bazel build //...
module: CUDACachingAllocator	Document required semantics of allocation functions in CUDAPluggableAllocator
module: CUDACachingAllocator	[memory] extend CUDAPluggableAllocator to support caching algorithms of segments, blocks and calling frames
module: CUDACachingAllocator	RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at "/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp":1154, please report a bug to PyTorch.
module: CUDACachingAllocator	RuntimeError: CUDAPluggableAllocator does not yet support cacheInfo
module: CUDACachingAllocator	Simulating lower memory on GPU does not indicate simulated memory in error message
module: CUDACachingAllocator	cudaMallocAsync cause too much fragmentation.
module: CUDACachingAllocator	Question about garbage collection without GPU sync
module: CUDACachingAllocator	Bring CudaPluggableAllocator to feature parity with the Native Allocator
module: CUDACachingAllocator	Request custom backend device memory Allocator.
module: CUDACachingAllocator	cuDNN doesn't support convolutions with more than INT_MAX elements and native kernel uses too much memory
module: CUDACachingAllocator	RuntimeError: p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED at "../c10/cuda/CUDACachingAllocator.cpp":1275, please report a bug to PyTorch.
module: CUDACachingAllocator	out of memory with pytorch version after 1.8.1
module: CUDACachingAllocator	CUDA allocator feature requests
module: CUDACachingAllocator	CUDACachingAllocator should be cuda memory merge/compact friendly
module: pickle	Sequential/Partial unpickling and loading of models
module: pickle	Pickling OneCycleLR.state_dict() with an unpickleable optimizer will result in an error.
module: pickle	[RFC] Allow device override during Tensor unpickling without torch.load
module: pickle	torch.package unpickling transforms: ModuleNotFoundError: No module named 'torch._C._linalg'; 'torch._C' is not a package
module: pickle	pickle is a security issue
module: pickle	TypeError: can't pickle _thread.lock objects when using spawn / forkserver
module: pickle	Yolov5 detect.py(ingerence) Error:Torch.nn.modules.module.ModuleAttributeError: 'Hardswish' object has no attribute 'inplace'
module: pickle	torch.load(.., map_location='cpu') fails when unserializing cuda tensors on a cpu-only device serialized with pickle
module: pickle	Pickling a Tensor or a Storage is not deterministic
module: pickle	failed to load model which is saved as text format(pickle_protocol=0) instead of binary format
module: pickle	UnpicklingError when trying to load multiple objects from a file
module: pickle	Well documented, safe method to deserialize model parameters from untrusted sources
module: pickle	Feature Request: Distributed send arbitrary objects
module: deploy	The torch::deploy document is not updated
module: deploy	Problems in built-from-source pytorch with USE_DEPLOY=1 in Ubuntu
module: deploy	Improve interaction of PyTorch downstream libraries and torchdeploy
module: deploy	DISABLED test_trace_dependencies (test_analyze.TestAnalyze)
module: deploy	[torch::deploy] Remove manager_ from the constructor and deconstructor of InterpreterSession
module: deploy	Package and Deploy: Generate External Python Registration files at runtime [WIP]
module: deploy	Add detection of interned submodule of externed module in PackageExporter
module: deploy	Improve torch::deploy documentation
module: deploy	Move torch::deploy tests to their own workflow job
module: deploy	Interpreter initialized state changed by torch dispatch
module: deploy	[deploy] Enable torch.distributed.rpc Python bindings in deploy
module: deploy	torchdeploy doesn't prevent Obj from being used on wrong interpreter
module: deploy	[torchdeploy] ar: _bz2module.o: No such file or directory
module: tf32	cuda/tf32 docs are outdated
module: tf32	TF32 conv_transpose2d with groups has bad precision compared to fp32
module: tf32	Large numerical inconsistency for torch.einsum on RTX30 series GPU.
module: tf32	Discrepancy in einsum when done in batch vs non-batch
module: tf32	Numerical instability: matrix multiplication got different results on cpu and gpu
module: tf32	torch.matmul produces wrong results on A4000 for matrices (n*m) with large m and small n
module: tf32	Large numerical error when applying nn.Linear in RTX A6000 with cuda>=11.1
module: tf32	TestCommonCUDA.test_noncontiguous_samples_pca_lowrank_cuda_float32 fails on A100 due to TF32 operation in svd_lowrank
module: tf32	Context manager to enable/disabled TensorFloat32 on demand
module: tf32	The matrix multiplication operator can't get correct results on 3090 !!
module: tf32	torch.cdist returns high diagonal values with CUDA
module: tf32	simple matrix multiplication yields wrong result on Ampere (3080)
module: tf32	Incorrect output loss value under specific CUDA version
module: TensorIterator	INTERNAL ASSERT FAILED !(has_different_input_dtypes && !config.promote_inputs_to_common_dtype_ && (has_undefined_outputs || config.enforce_safe_casting_to_output_ || config.cast_common_dtype_to_outputs_))
module: TensorIterator	TensorIterator: provide a two-argument version of set_output
module: TensorIterator	TensorIterator: refactor build_ternary_op to match binary versions
module: TensorIterator	[perf] 10x improvement on element-wise operations with manual broadcast
module: TensorIterator	RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at "/opt/conda/conda-bld/pytorch_1607370116979/work/aten/src/ATen/native/cuda/Loops.cuh":94, please report a bug to PyTorch.
module: TensorIterator	Massive Performance bottlenecks in some of the Reduce operations.
module: TensorIterator	CPU out of bound memory access in CUDA reduction kernel config
module: TensorIterator	Reduction for torch.int8 is super slow on CUDA
module: TensorIterator	Poor elmenetwise_kernel performance becomes critical on small mini-batch sizes
module: TensorIterator	TensorIterator does not work with different input/output types
module: TensorIterator	torch.sum(tensor, dim=()) is different from np.sum(arr, axis=())
module: TensorIterator	Can't torch.sum(tensor, dim) where dim >= 64
module: jetson	Jetson CI needs Updates
module: jetson	Installation on Jetson target board
module: jetson	libtorch1.8 torch::sigmoid is wrong
module: jetson	Zero-copy on shared memory of NVIDIA Jetson devices
module: jetson	Tensor precision manifests differently between CPU and GPU.
module: jetson	PyTorch inference on tensors of a particular size cause Illegal Instruction (core dumped) on Jetson Nano
module: jetson	Incorrect exponential calculation on Jetson devices with float32 dtype
module: jetson	NaN values on torch.nn.functional.conv2d (aarch64)
module: jetson	pytorch build failure on arm64
module: jetson	Possibly alignment issues on NEON vectorized ops, Jetson platforms
module: jetson	from_numpy function rounds float values on Jetson NX
module: jetson	Install Jetson TX2 Max Regcount Error
module: initialization	Best practices clarification for initialization strategies
module: initialization	Tensor.uniform_ fails to compile when using torch._dynamo
module: initialization	Unification of model initialization methods / naming across domain libraries + support of skip_init
module: initialization	Wrong initialisation gain for SNNs/SELU
module: initialization	[FR] Configurable gain value for kaiming_normal_ / kaiming_uniform_ in torch.nn.init
module: initialization	Add support of random state generator objects to nn.init module
module: initialization	Fan out calculation broken for group (depthwise) convolution
module: initialization	nn.init.orthogonal_ doesn't work with multiprocessing
module: initialization	LSTM forget bias must be initialized properly
module: initialization	[Feature request] truncated normal initializer(sampler)
module: protobuf	Error building Pytorch 13.1 from Source on OS X 12.5
module: protobuf	Protocol buffers library mismatch.
module: protobuf	libtorch compile problem. How to get the correct protobuf version? what PROTOBUF_VERSION <3011000 and 3011004 <PROTOBUF_MIN_PROTOC_VERSION?
module: protobuf	Compilation from source fails
module: protobuf	Pytorch "Import torch" lead to "core dump"
module: protobuf	ASAN build breaks when using third_party/protobuf
module: protobuf	Tests do not pass with the latest protobuf
module: protobuf	Failed to build with system protobuf
module: protobuf	libtorch exports protobuf symbols
module: protobuf	Provide Protobuf library if libtorch was built with included version
module: xnnpack	[discussion] Route pointwise Conv1d/Conv2d to matmul?
module: xnnpack	Question  on XNNPACK support for PowerPC
module: xnnpack	test_qnnpack_add fails
module: xnnpack	After XNNPack update TestXNNPACKSerDes.test_linear started to fail
module: xnnpack	Fail to build with gcc11
module: xnnpack	undefined reference to pthreadpool_compute*
module: xnnpack	Multiprocess DataLoader with DLPack conversion sometimes corrupts memory
module: xnnpack	XNNPACK operators are not actually registered under xnnpack namespace
module: xnnpack	Mac build from source error
module: wsl	Unexpected behaviour with shared modules in multiprocessing on WSL2
module: wsl	torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details)
module: wsl	torch.cppExtension won't work with wsl2
module: wsl	torch.compile not work in WSL
module: wsl	multiprocessing not work on WSL2
module: wsl	Pytorch does not recognize GPU in WSL2
module: wsl	PyTorch not recognizing GPU on WSL - installed cudnn and cuda
module: wsl	with the same environment ,pytorch 1.8 worked but 1.10 can't work
module: wsl	Run time error
module: nnpack	NNPACK slow down M1/M2 Mac CPU
module: nnpack	COREMLTOOLs/NNPACK Python Issue
module: nnpack	Conv1d: NNPACK SpatialConvolution_updateOutput failed when batchsize or padding is too large
module: nnpack	Floating point exception in _nnpack_spatial_convolution
module: nnpack	Can't forward pass conv2d with kernel_size=1, and padding=1
module: nnpack	M1 Pro Apple Silicon chip support.
module: nnpack	About 2 minor bug fixes on CUDA macOSX 10.13.6
module: nnpack	NNPACK condition should be changed (ARM processors)
module: pruning	Pruning under channels_last format
module: pruning	torch.nn.utils.prune.remove reorders the parameters of a module unexpectedly
module: pruning	Pruning torch.nn.MultiheadAttention causes RuntimeError
module: pruning	Pruners' compute_mask returns tensors with dtypes that are not consistent with each other
module: pruning	global_pruning costs memory after model is trained
module: pruning	Do we have plan to offer C++ binding for prune related features.
module: pruning	Support mainstream pruning techniques
module: pruning	CustomFromMask pruning stores a copy of the user-provided mask
module: magma	Wrong results with torch.linalg.inv on batched matrices when using cuda
module: magma	Expose more MAGMA backends for solve_triangular
module: magma	RFC: Improve the performance and usability of linear algebra on CUDA devices
module: magma	Update MKL version used with MAGMA
module: magma	Magma : Intel MKL Errors
module: magma	test_cholesky_solve gradgradcheck fails sometimes
module: magma	Cusolver handle may decrease MAGMA performance on GPU
module: magma	Cant compile from source 1.8.0a0 because 'magma_v2.h' file not found and undefined references to `magma_*'
module: jiterator	tan/tanh discrepancies with complex due to jiterator
module: jiterator	Accuracy issues with Jitterated complex kernels for acos, acosh, asin, asinh, tan and tanh
module: jiterator	A better error msg for cuda.jiterator when input is on cpu
module: jiterator	Abnormal GPU memory usage when using CUDA tensors with multiprocessing
module: jiterator	[jiterator] perf regression when jiterating few ops for complex dtype
module: jiterator	[jiterator] Jiterate Complex Ops
module: jiterator	Warning: "Specified kernel cache directory could not be created"
module: benchmark	Enable an MPS benchmark
module: benchmark	Timer: Add Standard Deviation for Single Measurement
module: benchmark	Timer benchmark stores only one time value, and therefore has broken mean/median/etc metrics
module: benchmark	benchmarks/dynamo/ci_expected_accuracy/update_expected.py truncates file if only one shard succeeds
module: benchmark	torch.utils.benchmark.examples.blas_compare can not be parsed by Python-3.7 runtime
module: benchmark	Bug? :Run torch.unique twice get different running time?
module: benchmark	benchmark.Compare raises: TypeError: object of type 'NoneType' has no len()
module: library	Custom tensor attributes not preserved with registered functions
module: library	New variables in torch._ops.py pollute the torch.ops namespace
module: library	Incorrect type hint for torch.library.Library.define
module: library	torch.ops.aten.empty is not discoverable from dir(torch.ops.aten) until explicitly calling getattr
module: library	Segfault when using torch.ops.* to access de-registered op
module: library	Silent promotion of bool to int in the dispatcher
module: library	Expose docs from the yaml for each torch.Tag in Python
module: lts	conda CPU installation for LTS fails with UnsatisfiableError
module: lts	"munmap_chunk(): invalid pointer" interaction error with pytorch (< 1.10), pybind, and cv_bridge
module: lts	locally installed PyTorch upgraded or superseded on windows/macos conda tests
module: lts	Dot product return completely incorrect result when using pip but not when using conda
module: lts	[bug] the LTS torch==1.8.2 pip package is incomplete
module: lts	question on installing GPU-enabled Pytorch
module: torchbind	Building LibTorch on Ubuntu with Mac M1
module: torchbind	please report a bug to PyTorch. Expected Object but got PyObject
module: torchbind	Cannot script when a static method or cross module function calls custom op
module: torchbind	TorchBind C++ Enum Class
module: torchbind	Verify TorchBind works with nested class type
module: derivatives	Higher order derivatives of sinc explode
module: derivatives	forward-mode support for "logically composite" operators
module: derivatives	The syncbatchnorm can not  be employed in the MODEL that the gradients including in the loss function
module: derivatives	second derivatives of unfold
module: derivatives	Conjugate gradient method
module: mpi	Add NCCL and MPI version printing to torch.utils.collect_env
module: mpi	Initialize NCCL backend with MPI
module: mpi	Remote memory access similar to MPI one-sided in pytorch
module: mpi	How to use mpi backend without CUDA_aware
module: numba	Some Numba tests are failing
module: numba	as_tensor returns CPU copy of a CUDA buffer object implementing CUDA Array Interface
module: numba	__cuda_array_interface__ conversion does not support readonly arrays
module: numba	creation of a tensor from a numba.cuda array
module: porting	Port masked_fill operator from the TH code to Aten
module: porting	Port fmod operator from the TH code to Aten
module: porting	Port SpatialConvolutionMM and VolumetricConvolutionMM to ATen
module: porting	High leverage TH operations to port to ATen
module: tbb	build: failure when building pytorch with TBB
module: tbb	[RFC] Add torch.backends.tbb.is_available()
module: tbb	build: failure when upgrade oneTBB to 2021.7.0
module: tbb	Build failed with TBB
module: dlpack	Support managed memory backed dlpack with  torch.from_dlpack
module: dlpack	One dlpack to rule them all
module: dlpack	getDLContext in DLConvertor.h cannot be found
module: dlpack	contiguous() not work for rank 1 length 1 tensor.
module: sleef	Enable SLEEF on ARM
module: sleef	USE_SYSTEM_SLEEF: undefined reference to symbol 'Sleef_expd4_u10'
module: sleef	[bug] torch.{sinh, cosh}: Incorrect values for vectorized path
module: tensorflow	Importing torch after TensorFlow results in std::runtime_error
module: tensorflow	Support "symmetric" reflection padding
module: openblas	Can not access to "sbgemm" routine with user-defined OpenBLAS
module: openblas	Functions depending on SVD are broken for inputs with non-finite values with MKL 2022+ and OpenBLAS 0.3.15+
module: CapabilityBasedPartitioner	CapabilityBasedPartitioner treats non-compute ops inconsistently
module: CapabilityBasedPartitioner	CapabilityBasedPartitioner does not work correctly with mutating operations
module: bottleneck	[utils.bottleneck] List of improvements
module: models	Can't download torch vision models for older versions via torch.hub.load: "multiple choices"
