module: tests	1  feature request  a rank revealing svd for better stability in backward
module: docs	1  feature request  a rank revealing svd for better stability in backward
module: numpy	1  feature request  a rank revealing svd for better stability in backward
module: cpp	1  feature request  a rank revealing svd for better stability in backward
module: cuda	1  feature request  a rank revealing svd for better stability in backward
module: autograd	1  feature request  a rank revealing svd for better stability in backward
module: nn	1  feature request  a rank revealing svd for better stability in backward
module: onnx	1  feature request  a rank revealing svd for better stability in backward
module: optimizer	1  feature request  a rank revealing svd for better stability in backward
module: tests	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: docs	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: numpy	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: cpp	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: cuda	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: autograd	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: nn	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: onnx	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: optimizer	2  libtorch can not save a  vector int  to autogradcontex  saved data
module: tests	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: docs	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: numpy	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: cpp	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: cuda	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: autograd	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: nn	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: onnx	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: optimizer	3  question  how to extract expose the complete pytorch computation graph  forward and backward
module: tests	4 add flag for functional jacobian to return output as well
module: docs	4 add flag for functional jacobian to return output as well
module: numpy	4 add flag for functional jacobian to return output as well
module: cpp	4 add flag for functional jacobian to return output as well
module: cuda	4 add flag for functional jacobian to return output as well
module: autograd	4 add flag for functional jacobian to return output as well
module: nn	4 add flag for functional jacobian to return output as well
module: onnx	4 add flag for functional jacobian to return output as well
module: optimizer	4 add flag for functional jacobian to return output as well
module: tests	5 add more explanation on multithreaded graph building of autograd
module: docs	5 add more explanation on multithreaded graph building of autograd
module: numpy	5 add more explanation on multithreaded graph building of autograd
module: cpp	5 add more explanation on multithreaded graph building of autograd
module: cuda	5 add more explanation on multithreaded graph building of autograd
module: autograd	5 add more explanation on multithreaded graph building of autograd
module: nn	5 add more explanation on multithreaded graph building of autograd
module: onnx	5 add more explanation on multithreaded graph building of autograd
module: optimizer	5 add more explanation on multithreaded graph building of autograd
module: tests	6 addmv  mv will trigger internal assert failed when input requiring grad
module: docs	6 addmv  mv will trigger internal assert failed when input requiring grad
module: numpy	6 addmv  mv will trigger internal assert failed when input requiring grad
module: cpp	6 addmv  mv will trigger internal assert failed when input requiring grad
module: cuda	6 addmv  mv will trigger internal assert failed when input requiring grad
module: autograd	6 addmv  mv will trigger internal assert failed when input requiring grad
module: nn	6 addmv  mv will trigger internal assert failed when input requiring grad
module: onnx	6 addmv  mv will trigger internal assert failed when input requiring grad
module: optimizer	6 addmv  mv will trigger internal assert failed when input requiring grad
module: tests	7 arctan  fp   error when optimising
module: docs	7 arctan  fp   error when optimising
module: numpy	7 arctan  fp   error when optimising
module: cpp	7 arctan  fp   error when optimising
module: cuda	7 arctan  fp   error when optimising
module: autograd	7 arctan  fp   error when optimising
module: nn	7 arctan  fp   error when optimising
module: onnx	7 arctan  fp   error when optimising
module: optimizer	7 arctan  fp   error when optimising
module: tests	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: docs	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: numpy	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: cpp	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: cuda	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: autograd	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: nn	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: onnx	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: optimizer	8 autocast   torch no grad inference cause backward graph nodes to be lost
module: tests	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: docs	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: numpy	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: cpp	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: cuda	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: autograd	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: nn	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: onnx	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: optimizer	9 backward and grad behave inconsistently w r t  set  on leaf variable
module: tests	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: docs	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: numpy	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: cpp	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: cuda	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: autograd	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: nn	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: onnx	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: optimizer	10 backward implicit conversion from tuple to torch tensor results in an indexing error message
module: tests	11 backward inputs  does not need to execute grad fn of the inputs
module: docs	11 backward inputs  does not need to execute grad fn of the inputs
module: numpy	11 backward inputs  does not need to execute grad fn of the inputs
module: cpp	11 backward inputs  does not need to execute grad fn of the inputs
module: cuda	11 backward inputs  does not need to execute grad fn of the inputs
module: autograd	11 backward inputs  does not need to execute grad fn of the inputs
module: nn	11 backward inputs  does not need to execute grad fn of the inputs
module: onnx	11 backward inputs  does not need to execute grad fn of the inputs
module: optimizer	11 backward inputs  does not need to execute grad fn of the inputs
module: tests	12 conv d padding same gradgradcheck fails on cuda
module: docs	12 conv d padding same gradgradcheck fails on cuda
module: numpy	12 conv d padding same gradgradcheck fails on cuda
module: cpp	12 conv d padding same gradgradcheck fails on cuda
module: cuda	12 conv d padding same gradgradcheck fails on cuda
module: autograd	12 conv d padding same gradgradcheck fails on cuda
module: nn	12 conv d padding same gradgradcheck fails on cuda
module: onnx	12 conv d padding same gradgradcheck fails on cuda
module: optimizer	12 conv d padding same gradgradcheck fails on cuda
module: tests	13 copy deepcopy does not copy gradients of nn parameter
module: docs	13 copy deepcopy does not copy gradients of nn parameter
module: numpy	13 copy deepcopy does not copy gradients of nn parameter
module: cpp	13 copy deepcopy does not copy gradients of nn parameter
module: cuda	13 copy deepcopy does not copy gradients of nn parameter
module: autograd	13 copy deepcopy does not copy gradients of nn parameter
module: nn	13 copy deepcopy does not copy gradients of nn parameter
module: onnx	13 copy deepcopy does not copy gradients of nn parameter
module: optimizer	13 copy deepcopy does not copy gradients of nn parameter
module: tests	14 ctc loss will backward crash
module: docs	14 ctc loss will backward crash
module: numpy	14 ctc loss will backward crash
module: cpp	14 ctc loss will backward crash
module: cuda	14 ctc loss will backward crash
module: autograd	14 ctc loss will backward crash
module: nn	14 ctc loss will backward crash
module: onnx	14 ctc loss will backward crash
module: optimizer	14 ctc loss will backward crash
module: tests	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: docs	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: numpy	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: cpp	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: cuda	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: autograd	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: nn	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: onnx	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: optimizer	15 disabled test fn gradgrad linalg lu factor cuda complex       main   testbwdgradientscuda
module: tests	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: docs	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: numpy	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: cpp	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: cuda	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: autograd	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: nn	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: onnx	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: optimizer	16 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: tests	17 forward program terminated from   cxa pure virtual
module: docs	17 forward program terminated from   cxa pure virtual
module: numpy	17 forward program terminated from   cxa pure virtual
module: cpp	17 forward program terminated from   cxa pure virtual
module: cuda	17 forward program terminated from   cxa pure virtual
module: autograd	17 forward program terminated from   cxa pure virtual
module: nn	17 forward program terminated from   cxa pure virtual
module: onnx	17 forward program terminated from   cxa pure virtual
module: optimizer	17 forward program terminated from   cxa pure virtual
module: tests	18 improved error checking for custom function when saving intermediates
module: docs	18 improved error checking for custom function when saving intermediates
module: numpy	18 improved error checking for custom function when saving intermediates
module: cpp	18 improved error checking for custom function when saving intermediates
module: cuda	18 improved error checking for custom function when saving intermediates
module: autograd	18 improved error checking for custom function when saving intermediates
module: nn	18 improved error checking for custom function when saving intermediates
module: onnx	18 improved error checking for custom function when saving intermediates
module: optimizer	18 improved error checking for custom function when saving intermediates
module: tests	19 incorrect gradient calculation for upsample nearest on cuda
module: docs	19 incorrect gradient calculation for upsample nearest on cuda
module: numpy	19 incorrect gradient calculation for upsample nearest on cuda
module: cpp	19 incorrect gradient calculation for upsample nearest on cuda
module: cuda	19 incorrect gradient calculation for upsample nearest on cuda
module: autograd	19 incorrect gradient calculation for upsample nearest on cuda
module: nn	19 incorrect gradient calculation for upsample nearest on cuda
module: onnx	19 incorrect gradient calculation for upsample nearest on cuda
module: optimizer	19 incorrect gradient calculation for upsample nearest on cuda
module: tests	20 jacobian mismatch for nn functional ctc loss
module: docs	20 jacobian mismatch for nn functional ctc loss
module: numpy	20 jacobian mismatch for nn functional ctc loss
module: cpp	20 jacobian mismatch for nn functional ctc loss
module: cuda	20 jacobian mismatch for nn functional ctc loss
module: autograd	20 jacobian mismatch for nn functional ctc loss
module: nn	20 jacobian mismatch for nn functional ctc loss
module: onnx	20 jacobian mismatch for nn functional ctc loss
module: optimizer	20 jacobian mismatch for nn functional ctc loss
module: tests	21 leakyrelu and elu use more vram than needed
module: docs	21 leakyrelu and elu use more vram than needed
module: numpy	21 leakyrelu and elu use more vram than needed
module: cpp	21 leakyrelu and elu use more vram than needed
module: cuda	21 leakyrelu and elu use more vram than needed
module: autograd	21 leakyrelu and elu use more vram than needed
module: nn	21 leakyrelu and elu use more vram than needed
module: onnx	21 leakyrelu and elu use more vram than needed
module: optimizer	21 leakyrelu and elu use more vram than needed
module: tests	22 matmul  mm triggers internal assert failed when input requires grad
module: docs	22 matmul  mm triggers internal assert failed when input requires grad
module: numpy	22 matmul  mm triggers internal assert failed when input requires grad
module: cpp	22 matmul  mm triggers internal assert failed when input requires grad
module: cuda	22 matmul  mm triggers internal assert failed when input requires grad
module: autograd	22 matmul  mm triggers internal assert failed when input requires grad
module: nn	22 matmul  mm triggers internal assert failed when input requires grad
module: onnx	22 matmul  mm triggers internal assert failed when input requires grad
module: optimizer	22 matmul  mm triggers internal assert failed when input requires grad
module: tests	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: docs	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: numpy	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: cpp	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: cuda	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: autograd	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: nn	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: onnx	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: optimizer	23 nesting no grad in autocast causes backwards graph to be  partially  lost outside of no grad
module: tests	24 oom during backward   leads to memory leaks
module: docs	24 oom during backward   leads to memory leaks
module: numpy	24 oom during backward   leads to memory leaks
module: cpp	24 oom during backward   leads to memory leaks
module: cuda	24 oom during backward   leads to memory leaks
module: autograd	24 oom during backward   leads to memory leaks
module: nn	24 oom during backward   leads to memory leaks
module: onnx	24 oom during backward   leads to memory leaks
module: optimizer	24 oom during backward   leads to memory leaks
module: tests	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: docs	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: numpy	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: cpp	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: cuda	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: autograd	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: nn	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: onnx	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: optimizer	25 pow cuda tensor raised to cpu scalar tensor result can t backward properly
module: tests	26 pytorch  torch autograd grad returns nonetype
module: docs	26 pytorch  torch autograd grad returns nonetype
module: numpy	26 pytorch  torch autograd grad returns nonetype
module: cpp	26 pytorch  torch autograd grad returns nonetype
module: cuda	26 pytorch  torch autograd grad returns nonetype
module: autograd	26 pytorch  torch autograd grad returns nonetype
module: nn	26 pytorch  torch autograd grad returns nonetype
module: onnx	26 pytorch  torch autograd grad returns nonetype
module: optimizer	26 pytorch  torch autograd grad returns nonetype
module: tests	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: docs	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: numpy	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: cpp	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: cuda	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: autograd	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: nn	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: onnx	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: optimizer	27 registering a global fallback for all operators that defaults us to assuming that autograd is not implemented
module: tests	28 sparse updates to logits in distributions categorical
module: docs	28 sparse updates to logits in distributions categorical
module: numpy	28 sparse updates to logits in distributions categorical
module: cpp	28 sparse updates to logits in distributions categorical
module: cuda	28 sparse updates to logits in distributions categorical
module: autograd	28 sparse updates to logits in distributions categorical
module: nn	28 sparse updates to logits in distributions categorical
module: onnx	28 sparse updates to logits in distributions categorical
module: optimizer	28 sparse updates to logits in distributions categorical
module: tests	29 svd triggers internal assert failed when computing jacobian in forward mode
module: docs	29 svd triggers internal assert failed when computing jacobian in forward mode
module: numpy	29 svd triggers internal assert failed when computing jacobian in forward mode
module: cpp	29 svd triggers internal assert failed when computing jacobian in forward mode
module: cuda	29 svd triggers internal assert failed when computing jacobian in forward mode
module: autograd	29 svd triggers internal assert failed when computing jacobian in forward mode
module: nn	29 svd triggers internal assert failed when computing jacobian in forward mode
module: onnx	29 svd triggers internal assert failed when computing jacobian in forward mode
module: optimizer	29 svd triggers internal assert failed when computing jacobian in forward mode
module: tests	30 torch clamp does not distribute gradients as element wisemin max do
module: docs	30 torch clamp does not distribute gradients as element wisemin max do
module: numpy	30 torch clamp does not distribute gradients as element wisemin max do
module: cpp	30 torch clamp does not distribute gradients as element wisemin max do
module: cuda	30 torch clamp does not distribute gradients as element wisemin max do
module: autograd	30 torch clamp does not distribute gradients as element wisemin max do
module: nn	30 torch clamp does not distribute gradients as element wisemin max do
module: onnx	30 torch clamp does not distribute gradients as element wisemin max do
module: optimizer	30 torch clamp does not distribute gradients as element wisemin max do
module: tests	31 torch compile incorrect when imperative autograd apis are used
module: docs	31 torch compile incorrect when imperative autograd apis are used
module: numpy	31 torch compile incorrect when imperative autograd apis are used
module: cpp	31 torch compile incorrect when imperative autograd apis are used
module: cuda	31 torch compile incorrect when imperative autograd apis are used
module: autograd	31 torch compile incorrect when imperative autograd apis are used
module: nn	31 torch compile incorrect when imperative autograd apis are used
module: onnx	31 torch compile incorrect when imperative autograd apis are used
module: optimizer	31 torch compile incorrect when imperative autograd apis are used
module: tests	32 torch func jacrev fails if model contains full backward hook
module: docs	32 torch func jacrev fails if model contains full backward hook
module: numpy	32 torch func jacrev fails if model contains full backward hook
module: cpp	32 torch func jacrev fails if model contains full backward hook
module: cuda	32 torch func jacrev fails if model contains full backward hook
module: autograd	32 torch func jacrev fails if model contains full backward hook
module: nn	32 torch func jacrev fails if model contains full backward hook
module: onnx	32 torch func jacrev fails if model contains full backward hook
module: optimizer	32 torch func jacrev fails if model contains full backward hook
module: tests	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: docs	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: numpy	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: cpp	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: cuda	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: autograd	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: nn	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: onnx	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: optimizer	33 torch inference mode and tensor subclass  runtimeerror  cannot set version counter for inference tensor
module: tests	34 torch ldexp generated tests fail on call to torch mul
module: docs	34 torch ldexp generated tests fail on call to torch mul
module: numpy	34 torch ldexp generated tests fail on call to torch mul
module: cpp	34 torch ldexp generated tests fail on call to torch mul
module: cuda	34 torch ldexp generated tests fail on call to torch mul
module: autograd	34 torch ldexp generated tests fail on call to torch mul
module: nn	34 torch ldexp generated tests fail on call to torch mul
module: onnx	34 torch ldexp generated tests fail on call to torch mul
module: optimizer	34 torch ldexp generated tests fail on call to torch mul
module: tests	35 torch linalg cond has different results for tensor requiring autograd
module: docs	35 torch linalg cond has different results for tensor requiring autograd
module: numpy	35 torch linalg cond has different results for tensor requiring autograd
module: cpp	35 torch linalg cond has different results for tensor requiring autograd
module: cuda	35 torch linalg cond has different results for tensor requiring autograd
module: autograd	35 torch linalg cond has different results for tensor requiring autograd
module: nn	35 torch linalg cond has different results for tensor requiring autograd
module: onnx	35 torch linalg cond has different results for tensor requiring autograd
module: optimizer	35 torch linalg cond has different results for tensor requiring autograd
module: tests	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: docs	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: numpy	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: cpp	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: cuda	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: autograd	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: nn	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: onnx	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: optimizer	36 torch nn functional l  loss fails gradgradcheck for complex inputs
module: tests	37 tsan issue in autograd  set next edges
module: docs	37 tsan issue in autograd  set next edges
module: numpy	37 tsan issue in autograd  set next edges
module: cpp	37 tsan issue in autograd  set next edges
module: cuda	37 tsan issue in autograd  set next edges
module: autograd	37 tsan issue in autograd  set next edges
module: nn	37 tsan issue in autograd  set next edges
module: onnx	37 tsan issue in autograd  set next edges
module: optimizer	37 tsan issue in autograd  set next edges
module: tests	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: docs	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: numpy	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: cpp	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: cuda	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: autograd	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: nn	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: onnx	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: optimizer	38 undocumented error on torch autograd function jvp for non tensor forward returns
module: tests	39 wrong return type from operation on custom tensor inside registered hook
module: docs	39 wrong return type from operation on custom tensor inside registered hook
module: numpy	39 wrong return type from operation on custom tensor inside registered hook
module: cpp	39 wrong return type from operation on custom tensor inside registered hook
module: cuda	39 wrong return type from operation on custom tensor inside registered hook
module: autograd	39 wrong return type from operation on custom tensor inside registered hook
module: nn	39 wrong return type from operation on custom tensor inside registered hook
module: onnx	39 wrong return type from operation on custom tensor inside registered hook
module: optimizer	39 wrong return type from operation on custom tensor inside registered hook
module: tests	40       cu    package missing proper libnvrtc builtins so
module: docs	40       cu    package missing proper libnvrtc builtins so
module: numpy	40       cu    package missing proper libnvrtc builtins so
module: cpp	40       cu    package missing proper libnvrtc builtins so
module: cuda	40       cu    package missing proper libnvrtc builtins so
module: autograd	40       cu    package missing proper libnvrtc builtins so
module: nn	40       cu    package missing proper libnvrtc builtins so
module: onnx	40       cu    package missing proper libnvrtc builtins so
module: optimizer	40       cu    package missing proper libnvrtc builtins so
module: tests	41  cpp  allow binding config structs into the python front end
module: docs	41  cpp  allow binding config structs into the python front end
module: numpy	41  cpp  allow binding config structs into the python front end
module: cpp	41  cpp  allow binding config structs into the python front end
module: cuda	41  cpp  allow binding config structs into the python front end
module: autograd	41  cpp  allow binding config structs into the python front end
module: nn	41  cpp  allow binding config structs into the python front end
module: onnx	41  cpp  allow binding config structs into the python front end
module: optimizer	41  cpp  allow binding config structs into the python front end
module: tests	42  cpp op  torch library schema doesn t respect keyword only
module: docs	42  cpp op  torch library schema doesn t respect keyword only
module: numpy	42  cpp op  torch library schema doesn t respect keyword only
module: cpp	42  cpp op  torch library schema doesn t respect keyword only
module: cuda	42  cpp op  torch library schema doesn t respect keyword only
module: autograd	42  cpp op  torch library schema doesn t respect keyword only
module: nn	42  cpp op  torch library schema doesn t respect keyword only
module: onnx	42  cpp op  torch library schema doesn t respect keyword only
module: optimizer	42  cpp op  torch library schema doesn t respect keyword only
module: tests	43  cppdocs  torch  load function istringstream example typo
module: docs	43  cppdocs  torch  load function istringstream example typo
module: numpy	43  cppdocs  torch  load function istringstream example typo
module: cpp	43  cppdocs  torch  load function istringstream example typo
module: cuda	43  cppdocs  torch  load function istringstream example typo
module: autograd	43  cppdocs  torch  load function istringstream example typo
module: nn	43  cppdocs  torch  load function istringstream example typo
module: onnx	43  cppdocs  torch  load function istringstream example typo
module: optimizer	43  cppdocs  torch  load function istringstream example typo
module: tests	44 a few functions in fbgemm utils cpp are defined in global namespace
module: docs	44 a few functions in fbgemm utils cpp are defined in global namespace
module: numpy	44 a few functions in fbgemm utils cpp are defined in global namespace
module: cpp	44 a few functions in fbgemm utils cpp are defined in global namespace
module: cuda	44 a few functions in fbgemm utils cpp are defined in global namespace
module: autograd	44 a few functions in fbgemm utils cpp are defined in global namespace
module: nn	44 a few functions in fbgemm utils cpp are defined in global namespace
module: onnx	44 a few functions in fbgemm utils cpp are defined in global namespace
module: optimizer	44 a few functions in fbgemm utils cpp are defined in global namespace
module: tests	45 batchnormfuncoptions object cant be printed in c
module: docs	45 batchnormfuncoptions object cant be printed in c
module: numpy	45 batchnormfuncoptions object cant be printed in c
module: cpp	45 batchnormfuncoptions object cant be printed in c
module: cuda	45 batchnormfuncoptions object cant be printed in c
module: autograd	45 batchnormfuncoptions object cant be printed in c
module: nn	45 batchnormfuncoptions object cant be printed in c
module: onnx	45 batchnormfuncoptions object cant be printed in c
module: optimizer	45 batchnormfuncoptions object cant be printed in c
module: tests	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: docs	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: numpy	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: cpp	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: cuda	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: autograd	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: nn	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: onnx	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: optimizer	46 c   api  nn  sequential  has inconsistent behavior with python conterpart
module: tests	47 c   api for torch autograd functional jacobian
module: docs	47 c   api for torch autograd functional jacobian
module: numpy	47 c   api for torch autograd functional jacobian
module: cpp	47 c   api for torch autograd functional jacobian
module: cuda	47 c   api for torch autograd functional jacobian
module: autograd	47 c   api for torch autograd functional jacobian
module: nn	47 c   api for torch autograd functional jacobian
module: onnx	47 c   api for torch autograd functional jacobian
module: optimizer	47 c   api for torch autograd functional jacobian
module: tests	48 compiling libtorch from source on mac beyond v
module: docs	48 compiling libtorch from source on mac beyond v
module: numpy	48 compiling libtorch from source on mac beyond v
module: cpp	48 compiling libtorch from source on mac beyond v
module: cuda	48 compiling libtorch from source on mac beyond v
module: autograd	48 compiling libtorch from source on mac beyond v
module: nn	48 compiling libtorch from source on mac beyond v
module: onnx	48 compiling libtorch from source on mac beyond v
module: optimizer	48 compiling libtorch from source on mac beyond v
module: tests	49 consider not checking in autogenerated core  tensor h tensormethods h
module: docs	49 consider not checking in autogenerated core  tensor h tensormethods h
module: numpy	49 consider not checking in autogenerated core  tensor h tensormethods h
module: cpp	49 consider not checking in autogenerated core  tensor h tensormethods h
module: cuda	49 consider not checking in autogenerated core  tensor h tensormethods h
module: autograd	49 consider not checking in autogenerated core  tensor h tensormethods h
module: nn	49 consider not checking in autogenerated core  tensor h tensormethods h
module: onnx	49 consider not checking in autogenerated core  tensor h tensormethods h
module: optimizer	49 consider not checking in autogenerated core  tensor h tensormethods h
module: tests	50 custom c   extension build process doesn t preserve color from compiler
module: docs	50 custom c   extension build process doesn t preserve color from compiler
module: numpy	50 custom c   extension build process doesn t preserve color from compiler
module: cpp	50 custom c   extension build process doesn t preserve color from compiler
module: cuda	50 custom c   extension build process doesn t preserve color from compiler
module: autograd	50 custom c   extension build process doesn t preserve color from compiler
module: nn	50 custom c   extension build process doesn t preserve color from compiler
module: onnx	50 custom c   extension build process doesn t preserve color from compiler
module: optimizer	50 custom c   extension build process doesn t preserve color from compiler
module: tests	51 document how to generate pybind bindings for c   autograd
module: docs	51 document how to generate pybind bindings for c   autograd
module: numpy	51 document how to generate pybind bindings for c   autograd
module: cpp	51 document how to generate pybind bindings for c   autograd
module: cuda	51 document how to generate pybind bindings for c   autograd
module: autograd	51 document how to generate pybind bindings for c   autograd
module: nn	51 document how to generate pybind bindings for c   autograd
module: onnx	51 document how to generate pybind bindings for c   autograd
module: optimizer	51 document how to generate pybind bindings for c   autograd
module: tests	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: docs	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: numpy	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: cpp	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: cuda	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: autograd	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: nn	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: onnx	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: optimizer	52 error   str  is not a member of  c     did you mean  c    aten  str   while using libtorch
module: tests	53 generator c   api should match python api
module: docs	53 generator c   api should match python api
module: numpy	53 generator c   api should match python api
module: cpp	53 generator c   api should match python api
module: cuda	53 generator c   api should match python api
module: autograd	53 generator c   api should match python api
module: nn	53 generator c   api should match python api
module: onnx	53 generator c   api should match python api
module: optimizer	53 generator c   api should match python api
module: tests	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: docs	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: numpy	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: cpp	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: cuda	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: autograd	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: nn	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: onnx	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: optimizer	54 get errors after compiling and running pytorch minimal example for c   mac m  with make
module: tests	55 gradient checkpointing support in c   api
module: docs	55 gradient checkpointing support in c   api
module: numpy	55 gradient checkpointing support in c   api
module: cpp	55 gradient checkpointing support in c   api
module: cuda	55 gradient checkpointing support in c   api
module: autograd	55 gradient checkpointing support in c   api
module: nn	55 gradient checkpointing support in c   api
module: onnx	55 gradient checkpointing support in c   api
module: optimizer	55 gradient checkpointing support in c   api
module: tests	56 how libtorch can work with  tensor data as same as  pytorch
module: docs	56 how libtorch can work with  tensor data as same as  pytorch
module: numpy	56 how libtorch can work with  tensor data as same as  pytorch
module: cpp	56 how libtorch can work with  tensor data as same as  pytorch
module: cuda	56 how libtorch can work with  tensor data as same as  pytorch
module: autograd	56 how libtorch can work with  tensor data as same as  pytorch
module: nn	56 how libtorch can work with  tensor data as same as  pytorch
module: onnx	56 how libtorch can work with  tensor data as same as  pytorch
module: optimizer	56 how libtorch can work with  tensor data as same as  pytorch
module: tests	57 is there doc that explains how to call an extension op in another extension implementation
module: docs	57 is there doc that explains how to call an extension op in another extension implementation
module: numpy	57 is there doc that explains how to call an extension op in another extension implementation
module: cpp	57 is there doc that explains how to call an extension op in another extension implementation
module: cuda	57 is there doc that explains how to call an extension op in another extension implementation
module: autograd	57 is there doc that explains how to call an extension op in another extension implementation
module: nn	57 is there doc that explains how to call an extension op in another extension implementation
module: onnx	57 is there doc that explains how to call an extension op in another extension implementation
module: optimizer	57 is there doc that explains how to call an extension op in another extension implementation
module: tests	58 libtorch consumes too much memory as
module: docs	58 libtorch consumes too much memory as
module: numpy	58 libtorch consumes too much memory as
module: cpp	58 libtorch consumes too much memory as
module: cuda	58 libtorch consumes too much memory as
module: autograd	58 libtorch consumes too much memory as
module: nn	58 libtorch consumes too much memory as
module: onnx	58 libtorch consumes too much memory as
module: optimizer	58 libtorch consumes too much memory as
module: tests	59 libtorch for windows  mnist example does no work
module: docs	59 libtorch for windows  mnist example does no work
module: numpy	59 libtorch for windows  mnist example does no work
module: cpp	59 libtorch for windows  mnist example does no work
module: cuda	59 libtorch for windows  mnist example does no work
module: autograd	59 libtorch for windows  mnist example does no work
module: nn	59 libtorch for windows  mnist example does no work
module: onnx	59 libtorch for windows  mnist example does no work
module: optimizer	59 libtorch for windows  mnist example does no work
module: tests	60 libtorch forward memory leak
module: docs	60 libtorch forward memory leak
module: numpy	60 libtorch forward memory leak
module: cpp	60 libtorch forward memory leak
module: cuda	60 libtorch forward memory leak
module: autograd	60 libtorch forward memory leak
module: nn	60 libtorch forward memory leak
module: onnx	60 libtorch forward memory leak
module: optimizer	60 libtorch forward memory leak
module: tests	61 libtorch gpu set id bug
module: docs	61 libtorch gpu set id bug
module: numpy	61 libtorch gpu set id bug
module: cpp	61 libtorch gpu set id bug
module: cuda	61 libtorch gpu set id bug
module: autograd	61 libtorch gpu set id bug
module: nn	61 libtorch gpu set id bug
module: onnx	61 libtorch gpu set id bug
module: optimizer	61 libtorch gpu set id bug
module: tests	62 libtorch memory leak
module: docs	62 libtorch memory leak
module: numpy	62 libtorch memory leak
module: cpp	62 libtorch memory leak
module: cuda	62 libtorch memory leak
module: autograd	62 libtorch memory leak
module: nn	62 libtorch memory leak
module: onnx	62 libtorch memory leak
module: optimizer	62 libtorch memory leak
module: tests	63 libtorch opencv mat result error  different from the python ones
module: docs	63 libtorch opencv mat result error  different from the python ones
module: numpy	63 libtorch opencv mat result error  different from the python ones
module: cpp	63 libtorch opencv mat result error  different from the python ones
module: cuda	63 libtorch opencv mat result error  different from the python ones
module: autograd	63 libtorch opencv mat result error  different from the python ones
module: nn	63 libtorch opencv mat result error  different from the python ones
module: onnx	63 libtorch opencv mat result error  different from the python ones
module: optimizer	63 libtorch opencv mat result error  different from the python ones
module: tests	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: docs	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: numpy	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: cpp	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: cuda	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: autograd	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: nn	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: onnx	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: optimizer	64 libtorch segfault in packed gru evaluation with cuda batch sizes
module: tests	65 libtorch so file size is very large
module: docs	65 libtorch so file size is very large
module: numpy	65 libtorch so file size is very large
module: cpp	65 libtorch so file size is very large
module: cuda	65 libtorch so file size is very large
module: autograd	65 libtorch so file size is very large
module: nn	65 libtorch so file size is very large
module: onnx	65 libtorch so file size is very large
module: optimizer	65 libtorch so file size is very large
module: tests	66 make         no rule to make target  libtorch lib libc   so
module: docs	66 make         no rule to make target  libtorch lib libc   so
module: numpy	66 make         no rule to make target  libtorch lib libc   so
module: cpp	66 make         no rule to make target  libtorch lib libc   so
module: cuda	66 make         no rule to make target  libtorch lib libc   so
module: autograd	66 make         no rule to make target  libtorch lib libc   so
module: nn	66 make         no rule to make target  libtorch lib libc   so
module: onnx	66 make         no rule to make target  libtorch lib libc   so
module: optimizer	66 make         no rule to make target  libtorch lib libc   so
module: tests	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: docs	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: numpy	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: cpp	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: cuda	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: autograd	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: nn	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: onnx	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: optimizer	67 make setter non optional  e g   tensoroptions  device optional device      device device   and add a device opt setter
module: tests	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: docs	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: numpy	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: cpp	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: cuda	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: autograd	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: nn	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: onnx	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: optimizer	68 newoperatorregistrationtest testimplnodefgetscaught failed
module: tests	69 output values not same and much slower than python api
module: docs	69 output values not same and much slower than python api
module: numpy	69 output values not same and much slower than python api
module: cpp	69 output values not same and much slower than python api
module: cuda	69 output values not same and much slower than python api
module: autograd	69 output values not same and much slower than python api
module: nn	69 output values not same and much slower than python api
module: onnx	69 output values not same and much slower than python api
module: optimizer	69 output values not same and much slower than python api
module: tests	70 path tp torch torch h        fatal error c      internal compiler error
module: docs	70 path tp torch torch h        fatal error c      internal compiler error
module: numpy	70 path tp torch torch h        fatal error c      internal compiler error
module: cpp	70 path tp torch torch h        fatal error c      internal compiler error
module: cuda	70 path tp torch torch h        fatal error c      internal compiler error
module: autograd	70 path tp torch torch h        fatal error c      internal compiler error
module: nn	70 path tp torch torch h        fatal error c      internal compiler error
module: onnx	70 path tp torch torch h        fatal error c      internal compiler error
module: optimizer	70 path tp torch torch h        fatal error c      internal compiler error
module: tests	71 properly design manual cpp binding  make it less error prone
module: docs	71 properly design manual cpp binding  make it less error prone
module: numpy	71 properly design manual cpp binding  make it less error prone
module: cpp	71 properly design manual cpp binding  make it less error prone
module: cuda	71 properly design manual cpp binding  make it less error prone
module: autograd	71 properly design manual cpp binding  make it less error prone
module: nn	71 properly design manual cpp binding  make it less error prone
module: onnx	71 properly design manual cpp binding  make it less error prone
module: optimizer	71 properly design manual cpp binding  make it less error prone
module: tests	72 pytorch      can not using namespace torch  indexing
module: docs	72 pytorch      can not using namespace torch  indexing
module: numpy	72 pytorch      can not using namespace torch  indexing
module: cpp	72 pytorch      can not using namespace torch  indexing
module: cuda	72 pytorch      can not using namespace torch  indexing
module: autograd	72 pytorch      can not using namespace torch  indexing
module: nn	72 pytorch      can not using namespace torch  indexing
module: onnx	72 pytorch      can not using namespace torch  indexing
module: optimizer	72 pytorch      can not using namespace torch  indexing
module: tests	73 pytorch and c   inference disagree
module: docs	73 pytorch and c   inference disagree
module: numpy	73 pytorch and c   inference disagree
module: cpp	73 pytorch and c   inference disagree
module: cuda	73 pytorch and c   inference disagree
module: autograd	73 pytorch and c   inference disagree
module: nn	73 pytorch and c   inference disagree
module: onnx	73 pytorch and c   inference disagree
module: optimizer	73 pytorch and c   inference disagree
module: tests	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: docs	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: numpy	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: cpp	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: cuda	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: autograd	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: nn	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: onnx	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: optimizer	74 pytorch c   api cannot call operator   on torch  nn  sequential
module: tests	75 request to revise the pytorch tutorial
module: docs	75 request to revise the pytorch tutorial
module: numpy	75 request to revise the pytorch tutorial
module: cpp	75 request to revise the pytorch tutorial
module: cuda	75 request to revise the pytorch tutorial
module: autograd	75 request to revise the pytorch tutorial
module: nn	75 request to revise the pytorch tutorial
module: onnx	75 request to revise the pytorch tutorial
module: optimizer	75 request to revise the pytorch tutorial
module: tests	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: docs	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: numpy	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: cpp	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: cuda	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: autograd	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: nn	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: onnx	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: optimizer	76 requesting to add a section to the installing c   distributions of pytorch documentation for apple m  m  processors
module: tests	77 runtime errors with convolution backward out when handling optional bias gradient
module: docs	77 runtime errors with convolution backward out when handling optional bias gradient
module: numpy	77 runtime errors with convolution backward out when handling optional bias gradient
module: cpp	77 runtime errors with convolution backward out when handling optional bias gradient
module: cuda	77 runtime errors with convolution backward out when handling optional bias gradient
module: autograd	77 runtime errors with convolution backward out when handling optional bias gradient
module: nn	77 runtime errors with convolution backward out when handling optional bias gradient
module: onnx	77 runtime errors with convolution backward out when handling optional bias gradient
module: optimizer	77 runtime errors with convolution backward out when handling optional bias gradient
module: tests	78 scalar construction in c    pytorch tensor        c  at  tensor
module: docs	78 scalar construction in c    pytorch tensor        c  at  tensor
module: numpy	78 scalar construction in c    pytorch tensor        c  at  tensor
module: cpp	78 scalar construction in c    pytorch tensor        c  at  tensor
module: cuda	78 scalar construction in c    pytorch tensor        c  at  tensor
module: autograd	78 scalar construction in c    pytorch tensor        c  at  tensor
module: nn	78 scalar construction in c    pytorch tensor        c  at  tensor
module: onnx	78 scalar construction in c    pytorch tensor        c  at  tensor
module: optimizer	78 scalar construction in c    pytorch tensor        c  at  tensor
module: tests	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: docs	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: numpy	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: cpp	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: cuda	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: autograd	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: nn	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: onnx	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: optimizer	79 simple c   custom autograd function code throws error  cuda error  driver shutting down
module: tests	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: docs	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: numpy	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: cpp	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: cuda	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: autograd	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: nn	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: onnx	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: optimizer	80 tensorexpr loopnest get loops for misbehaved after loop distribution transformation
module: tests	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: docs	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: numpy	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: cpp	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: cuda	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: autograd	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: nn	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: onnx	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: optimizer	81 the python module installs cmake files and headers under the python s site packages directory that can t be used
module: tests	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: docs	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: numpy	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: cpp	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: cuda	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: autograd	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: nn	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: onnx	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: optimizer	82 torch  tensor scalar  behaves differently from at  tensor scalar
module: tests	83 undefined symbol error when compiling and loading c   extension
module: docs	83 undefined symbol error when compiling and loading c   extension
module: numpy	83 undefined symbol error when compiling and loading c   extension
module: cpp	83 undefined symbol error when compiling and loading c   extension
module: cuda	83 undefined symbol error when compiling and loading c   extension
module: autograd	83 undefined symbol error when compiling and loading c   extension
module: nn	83 undefined symbol error when compiling and loading c   extension
module: onnx	83 undefined symbol error when compiling and loading c   extension
module: optimizer	83 undefined symbol error when compiling and loading c   extension
module: tests	84 unexpected poor performance of c   extension   wish for a fast operator
module: docs	84 unexpected poor performance of c   extension   wish for a fast operator
module: numpy	84 unexpected poor performance of c   extension   wish for a fast operator
module: cpp	84 unexpected poor performance of c   extension   wish for a fast operator
module: cuda	84 unexpected poor performance of c   extension   wish for a fast operator
module: autograd	84 unexpected poor performance of c   extension   wish for a fast operator
module: nn	84 unexpected poor performance of c   extension   wish for a fast operator
module: onnx	84 unexpected poor performance of c   extension   wish for a fast operator
module: optimizer	84 unexpected poor performance of c   extension   wish for a fast operator
module: tests	85  bug  big difference between the output of conv float precision and double precision
module: docs	85  bug  big difference between the output of conv float precision and double precision
module: numpy	85  bug  big difference between the output of conv float precision and double precision
module: cpp	85  bug  big difference between the output of conv float precision and double precision
module: cuda	85  bug  big difference between the output of conv float precision and double precision
module: autograd	85  bug  big difference between the output of conv float precision and double precision
module: nn	85  bug  big difference between the output of conv float precision and double precision
module: onnx	85  bug  big difference between the output of conv float precision and double precision
module: optimizer	85  bug  big difference between the output of conv float precision and double precision
module: tests	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: docs	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: numpy	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: cpp	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: cuda	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: autograd	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: nn	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: onnx	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: optimizer	86  cuda rpc  incorrect results of gpu tensor transferring using rpc when parallelized with other gpu programs
module: tests	87  tracker  inconsistencies between cpu and gpu computation
module: docs	87  tracker  inconsistencies between cpu and gpu computation
module: numpy	87  tracker  inconsistencies between cpu and gpu computation
module: cpp	87  tracker  inconsistencies between cpu and gpu computation
module: cuda	87  tracker  inconsistencies between cpu and gpu computation
module: autograd	87  tracker  inconsistencies between cpu and gpu computation
module: nn	87  tracker  inconsistencies between cpu and gpu computation
module: onnx	87  tracker  inconsistencies between cpu and gpu computation
module: optimizer	87  tracker  inconsistencies between cpu and gpu computation
module: tests	88 adam  fused true  issues
module: docs	88 adam  fused true  issues
module: numpy	88 adam  fused true  issues
module: cpp	88 adam  fused true  issues
module: cuda	88 adam  fused true  issues
module: autograd	88 adam  fused true  issues
module: nn	88 adam  fused true  issues
module: onnx	88 adam  fused true  issues
module: optimizer	88 adam  fused true  issues
module: tests	89 can only import torch after tensorflow accessed its gpu device
module: docs	89 can only import torch after tensorflow accessed its gpu device
module: numpy	89 can only import torch after tensorflow accessed its gpu device
module: cpp	89 can only import torch after tensorflow accessed its gpu device
module: cuda	89 can only import torch after tensorflow accessed its gpu device
module: autograd	89 can only import torch after tensorflow accessed its gpu device
module: nn	89 can only import torch after tensorflow accessed its gpu device
module: onnx	89 can only import torch after tensorflow accessed its gpu device
module: optimizer	89 can only import torch after tensorflow accessed its gpu device
module: tests	90 can t reproduce non deterministic results with cuda
module: docs	90 can t reproduce non deterministic results with cuda
module: numpy	90 can t reproduce non deterministic results with cuda
module: cpp	90 can t reproduce non deterministic results with cuda
module: cuda	90 can t reproduce non deterministic results with cuda
module: autograd	90 can t reproduce non deterministic results with cuda
module: nn	90 can t reproduce non deterministic results with cuda
module: onnx	90 can t reproduce non deterministic results with cuda
module: optimizer	90 can t reproduce non deterministic results with cuda
module: tests	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: docs	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: numpy	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: cpp	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: cuda	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: autograd	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: nn	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: onnx	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: optimizer	91 convtranspose fails on cpu but returns an empty tensor on cuda
module: tests	92 coo   coo tries to allocate way too much memory on cuda
module: docs	92 coo   coo tries to allocate way too much memory on cuda
module: numpy	92 coo   coo tries to allocate way too much memory on cuda
module: cpp	92 coo   coo tries to allocate way too much memory on cuda
module: cuda	92 coo   coo tries to allocate way too much memory on cuda
module: autograd	92 coo   coo tries to allocate way too much memory on cuda
module: nn	92 coo   coo tries to allocate way too much memory on cuda
module: onnx	92 coo   coo tries to allocate way too much memory on cuda
module: optimizer	92 coo   coo tries to allocate way too much memory on cuda
module: tests	93 cublas status not supported
module: docs	93 cublas status not supported
module: numpy	93 cublas status not supported
module: cpp	93 cublas status not supported
module: cuda	93 cublas status not supported
module: autograd	93 cublas status not supported
module: nn	93 cublas status not supported
module: onnx	93 cublas status not supported
module: optimizer	93 cublas status not supported
module: tests	94 cuda      cudnn       run conv d error
module: docs	94 cuda      cudnn       run conv d error
module: numpy	94 cuda      cudnn       run conv d error
module: cpp	94 cuda      cudnn       run conv d error
module: cuda	94 cuda      cudnn       run conv d error
module: autograd	94 cuda      cudnn       run conv d error
module: nn	94 cuda      cudnn       run conv d error
module: onnx	94 cuda      cudnn       run conv d error
module: optimizer	94 cuda      cudnn       run conv d error
module: tests	95 cuda extension error message doesn t look correct
module: docs	95 cuda extension error message doesn t look correct
module: numpy	95 cuda extension error message doesn t look correct
module: cpp	95 cuda extension error message doesn t look correct
module: cuda	95 cuda extension error message doesn t look correct
module: autograd	95 cuda extension error message doesn t look correct
module: nn	95 cuda extension error message doesn t look correct
module: onnx	95 cuda extension error message doesn t look correct
module: optimizer	95 cuda extension error message doesn t look correct
module: tests	96 cuda is available   error
module: docs	96 cuda is available   error
module: numpy	96 cuda is available   error
module: cpp	96 cuda is available   error
module: cuda	96 cuda is available   error
module: autograd	96 cuda is available   error
module: nn	96 cuda is available   error
module: onnx	96 cuda is available   error
module: optimizer	96 cuda is available   error
module: tests	97 cuda unknown error after suspend during debugging
module: docs	97 cuda unknown error after suspend during debugging
module: numpy	97 cuda unknown error after suspend during debugging
module: cpp	97 cuda unknown error after suspend during debugging
module: cuda	97 cuda unknown error after suspend during debugging
module: autograd	97 cuda unknown error after suspend during debugging
module: nn	97 cuda unknown error after suspend during debugging
module: onnx	97 cuda unknown error after suspend during debugging
module: optimizer	97 cuda unknown error after suspend during debugging
module: tests	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: docs	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: numpy	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: cpp	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: cuda	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: autograd	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: nn	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: onnx	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: optimizer	98 cudnn error  cudnn status mapping error on gtx      a   when conv d is called
module: tests	99 dataparallel scatter method split tensor wrong
module: docs	99 dataparallel scatter method split tensor wrong
module: numpy	99 dataparallel scatter method split tensor wrong
module: cpp	99 dataparallel scatter method split tensor wrong
module: cuda	99 dataparallel scatter method split tensor wrong
module: autograd	99 dataparallel scatter method split tensor wrong
module: nn	99 dataparallel scatter method split tensor wrong
module: onnx	99 dataparallel scatter method split tensor wrong
module: optimizer	99 dataparallel scatter method split tensor wrong
module: tests	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: docs	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: numpy	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: cpp	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: cuda	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: autograd	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: nn	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: onnx	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: optimizer	100 disabled test variant consistency jit linalg lu cuda complex      main   testjitcuda
module: tests	101 enable postlocalsgdoptimizer on cuda tensors
module: docs	101 enable postlocalsgdoptimizer on cuda tensors
module: numpy	101 enable postlocalsgdoptimizer on cuda tensors
module: cpp	101 enable postlocalsgdoptimizer on cuda tensors
module: cuda	101 enable postlocalsgdoptimizer on cuda tensors
module: autograd	101 enable postlocalsgdoptimizer on cuda tensors
module: nn	101 enable postlocalsgdoptimizer on cuda tensors
module: onnx	101 enable postlocalsgdoptimizer on cuda tensors
module: optimizer	101 enable postlocalsgdoptimizer on cuda tensors
module: tests	102 error with monai swinunetr and checkpointing
module: docs	102 error with monai swinunetr and checkpointing
module: numpy	102 error with monai swinunetr and checkpointing
module: cpp	102 error with monai swinunetr and checkpointing
module: cuda	102 error with monai swinunetr and checkpointing
module: autograd	102 error with monai swinunetr and checkpointing
module: nn	102 error with monai swinunetr and checkpointing
module: onnx	102 error with monai swinunetr and checkpointing
module: optimizer	102 error with monai swinunetr and checkpointing
module: tests	103 flashattentionv  will oom when building on ci cd with default settings
module: docs	103 flashattentionv  will oom when building on ci cd with default settings
module: numpy	103 flashattentionv  will oom when building on ci cd with default settings
module: cpp	103 flashattentionv  will oom when building on ci cd with default settings
module: cuda	103 flashattentionv  will oom when building on ci cd with default settings
module: autograd	103 flashattentionv  will oom when building on ci cd with default settings
module: nn	103 flashattentionv  will oom when building on ci cd with default settings
module: onnx	103 flashattentionv  will oom when building on ci cd with default settings
module: optimizer	103 flashattentionv  will oom when building on ci cd with default settings
module: tests	104 groupnorm   instancenorm does not handle channels last correctly
module: docs	104 groupnorm   instancenorm does not handle channels last correctly
module: numpy	104 groupnorm   instancenorm does not handle channels last correctly
module: cpp	104 groupnorm   instancenorm does not handle channels last correctly
module: cuda	104 groupnorm   instancenorm does not handle channels last correctly
module: autograd	104 groupnorm   instancenorm does not handle channels last correctly
module: nn	104 groupnorm   instancenorm does not handle channels last correctly
module: onnx	104 groupnorm   instancenorm does not handle channels last correctly
module: optimizer	104 groupnorm   instancenorm does not handle channels last correctly
module: tests	105 implement a torch cuda visible device indexes function
module: docs	105 implement a torch cuda visible device indexes function
module: numpy	105 implement a torch cuda visible device indexes function
module: cpp	105 implement a torch cuda visible device indexes function
module: cuda	105 implement a torch cuda visible device indexes function
module: autograd	105 implement a torch cuda visible device indexes function
module: nn	105 implement a torch cuda visible device indexes function
module: onnx	105 implement a torch cuda visible device indexes function
module: optimizer	105 implement a torch cuda visible device indexes function
module: tests	106 inconsistent behaviour of torch all
module: docs	106 inconsistent behaviour of torch all
module: numpy	106 inconsistent behaviour of torch all
module: cpp	106 inconsistent behaviour of torch all
module: cuda	106 inconsistent behaviour of torch all
module: autograd	106 inconsistent behaviour of torch all
module: nn	106 inconsistent behaviour of torch all
module: onnx	106 inconsistent behaviour of torch all
module: optimizer	106 inconsistent behaviour of torch all
module: tests	107 inconsistent performance degradation of  x  convolution  torch       cu
module: docs	107 inconsistent performance degradation of  x  convolution  torch       cu
module: numpy	107 inconsistent performance degradation of  x  convolution  torch       cu
module: cpp	107 inconsistent performance degradation of  x  convolution  torch       cu
module: cuda	107 inconsistent performance degradation of  x  convolution  torch       cu
module: autograd	107 inconsistent performance degradation of  x  convolution  torch       cu
module: nn	107 inconsistent performance degradation of  x  convolution  torch       cu
module: onnx	107 inconsistent performance degradation of  x  convolution  torch       cu
module: optimizer	107 inconsistent performance degradation of  x  convolution  torch       cu
module: tests	108 increasing batch size makes network forward      times slower
module: docs	108 increasing batch size makes network forward      times slower
module: numpy	108 increasing batch size makes network forward      times slower
module: cpp	108 increasing batch size makes network forward      times slower
module: cuda	108 increasing batch size makes network forward      times slower
module: autograd	108 increasing batch size makes network forward      times slower
module: nn	108 increasing batch size makes network forward      times slower
module: onnx	108 increasing batch size makes network forward      times slower
module: optimizer	108 increasing batch size makes network forward      times slower
module: tests	109 internal assert failed
module: docs	109 internal assert failed
module: numpy	109 internal assert failed
module: cpp	109 internal assert failed
module: cuda	109 internal assert failed
module: autograd	109 internal assert failed
module: nn	109 internal assert failed
module: onnx	109 internal assert failed
module: optimizer	109 internal assert failed
module: tests	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: docs	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: numpy	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: cpp	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: cuda	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: autograd	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: nn	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: onnx	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: optimizer	110 log softmax   on cpu and gpu has expected numerical error when used with low precision bfloat
module: tests	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: docs	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: numpy	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: cpp	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: cuda	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: autograd	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: nn	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: onnx	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: optimizer	111 max pool d with indices backward out cuda  remove useless  code gradinput zero
module: tests	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: docs	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: numpy	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: cpp	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: cuda	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: autograd	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: nn	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: onnx	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: optimizer	112 nn functional embedding bag trigger out of bound read under compute sanitizer
module: tests	113 out of memory with pytorch version after
module: docs	113 out of memory with pytorch version after
module: numpy	113 out of memory with pytorch version after
module: cpp	113 out of memory with pytorch version after
module: cuda	113 out of memory with pytorch version after
module: autograd	113 out of memory with pytorch version after
module: nn	113 out of memory with pytorch version after
module: onnx	113 out of memory with pytorch version after
module: optimizer	113 out of memory with pytorch version after
module: tests	114 overflow     on cuda tensor after matrix multiplication
module: docs	114 overflow     on cuda tensor after matrix multiplication
module: numpy	114 overflow     on cuda tensor after matrix multiplication
module: cpp	114 overflow     on cuda tensor after matrix multiplication
module: cuda	114 overflow     on cuda tensor after matrix multiplication
module: autograd	114 overflow     on cuda tensor after matrix multiplication
module: nn	114 overflow     on cuda tensor after matrix multiplication
module: onnx	114 overflow     on cuda tensor after matrix multiplication
module: optimizer	114 overflow     on cuda tensor after matrix multiplication
module: tests	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: docs	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: numpy	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: cpp	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: cuda	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: autograd	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: nn	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: onnx	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: optimizer	115 perf drop  factor    ubuntu vs windows on same pc  dual boot
module: tests	116 pytorch could not build from source with cudnn
module: docs	116 pytorch could not build from source with cudnn
module: numpy	116 pytorch could not build from source with cudnn
module: cpp	116 pytorch could not build from source with cudnn
module: cuda	116 pytorch could not build from source with cudnn
module: autograd	116 pytorch could not build from source with cudnn
module: nn	116 pytorch could not build from source with cudnn
module: onnx	116 pytorch could not build from source with cudnn
module: optimizer	116 pytorch could not build from source with cudnn
module: tests	117 runtimeerror in use torch       cuda
module: docs	117 runtimeerror in use torch       cuda
module: numpy	117 runtimeerror in use torch       cuda
module: cpp	117 runtimeerror in use torch       cuda
module: cuda	117 runtimeerror in use torch       cuda
module: autograd	117 runtimeerror in use torch       cuda
module: nn	117 runtimeerror in use torch       cuda
module: onnx	117 runtimeerror in use torch       cuda
module: optimizer	117 runtimeerror in use torch       cuda
module: tests	118 sm    support
module: docs	118 sm    support
module: numpy	118 sm    support
module: cpp	118 sm    support
module: cuda	118 sm    support
module: autograd	118 sm    support
module: nn	118 sm    support
module: onnx	118 sm    support
module: optimizer	118 sm    support
module: tests	119 support cuda
module: docs	119 support cuda
module: numpy	119 support cuda
module: cpp	119 support cuda
module: cuda	119 support cuda
module: autograd	119 support cuda
module: nn	119 support cuda
module: onnx	119 support cuda
module: optimizer	119 support cuda
module: tests	120 tan tanh discrepancies with complex due to jiterator
module: docs	120 tan tanh discrepancies with complex due to jiterator
module: numpy	120 tan tanh discrepancies with complex due to jiterator
module: cpp	120 tan tanh discrepancies with complex due to jiterator
module: cuda	120 tan tanh discrepancies with complex due to jiterator
module: autograd	120 tan tanh discrepancies with complex due to jiterator
module: nn	120 tan tanh discrepancies with complex due to jiterator
module: onnx	120 tan tanh discrepancies with complex due to jiterator
module: optimizer	120 tan tanh discrepancies with complex due to jiterator
module: tests	121 tensor copied over to multiple gpus on its own
module: docs	121 tensor copied over to multiple gpus on its own
module: numpy	121 tensor copied over to multiple gpus on its own
module: cpp	121 tensor copied over to multiple gpus on its own
module: cuda	121 tensor copied over to multiple gpus on its own
module: autograd	121 tensor copied over to multiple gpus on its own
module: nn	121 tensor copied over to multiple gpus on its own
module: onnx	121 tensor copied over to multiple gpus on its own
module: optimizer	121 tensor copied over to multiple gpus on its own
module: tests	122 test foreach failing cuda memory leak check
module: docs	122 test foreach failing cuda memory leak check
module: numpy	122 test foreach failing cuda memory leak check
module: cpp	122 test foreach failing cuda memory leak check
module: cuda	122 test foreach failing cuda memory leak check
module: autograd	122 test foreach failing cuda memory leak check
module: nn	122 test foreach failing cuda memory leak check
module: onnx	122 test foreach failing cuda memory leak check
module: optimizer	122 test foreach failing cuda memory leak check
module: tests	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: docs	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: numpy	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: cpp	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: cuda	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: autograd	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: nn	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: onnx	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: optimizer	123 the difference between channels last backward and channels first backward of avgpool d on cuda is too large
module: tests	124 the speed of matrix inversion is relatively slow for many small matrices
module: docs	124 the speed of matrix inversion is relatively slow for many small matrices
module: numpy	124 the speed of matrix inversion is relatively slow for many small matrices
module: cpp	124 the speed of matrix inversion is relatively slow for many small matrices
module: cuda	124 the speed of matrix inversion is relatively slow for many small matrices
module: autograd	124 the speed of matrix inversion is relatively slow for many small matrices
module: nn	124 the speed of matrix inversion is relatively slow for many small matrices
module: onnx	124 the speed of matrix inversion is relatively slow for many small matrices
module: optimizer	124 the speed of matrix inversion is relatively slow for many small matrices
module: tests	125 torch     import hangs forever
module: docs	125 torch     import hangs forever
module: numpy	125 torch     import hangs forever
module: cpp	125 torch     import hangs forever
module: cuda	125 torch     import hangs forever
module: autograd	125 torch     import hangs forever
module: nn	125 torch     import hangs forever
module: onnx	125 torch     import hangs forever
module: optimizer	125 torch     import hangs forever
module: tests	126 torch addbmm throws different exception differences on cpu and gpu
module: docs	126 torch addbmm throws different exception differences on cpu and gpu
module: numpy	126 torch addbmm throws different exception differences on cpu and gpu
module: cpp	126 torch addbmm throws different exception differences on cpu and gpu
module: cuda	126 torch addbmm throws different exception differences on cpu and gpu
module: autograd	126 torch addbmm throws different exception differences on cpu and gpu
module: nn	126 torch addbmm throws different exception differences on cpu and gpu
module: onnx	126 torch addbmm throws different exception differences on cpu and gpu
module: optimizer	126 torch addbmm throws different exception differences on cpu and gpu
module: tests	127 torch cuda device count cached return value does not reflect environment changes
module: docs	127 torch cuda device count cached return value does not reflect environment changes
module: numpy	127 torch cuda device count cached return value does not reflect environment changes
module: cpp	127 torch cuda device count cached return value does not reflect environment changes
module: cuda	127 torch cuda device count cached return value does not reflect environment changes
module: autograd	127 torch cuda device count cached return value does not reflect environment changes
module: nn	127 torch cuda device count cached return value does not reflect environment changes
module: onnx	127 torch cuda device count cached return value does not reflect environment changes
module: optimizer	127 torch cuda device count cached return value does not reflect environment changes
module: tests	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: docs	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: numpy	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: cpp	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: cuda	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: autograd	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: nn	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: onnx	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: optimizer	128 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: tests	129 unexpected none value for stream with dynamo
module: docs	129 unexpected none value for stream with dynamo
module: numpy	129 unexpected none value for stream with dynamo
module: cpp	129 unexpected none value for stream with dynamo
module: cuda	129 unexpected none value for stream with dynamo
module: autograd	129 unexpected none value for stream with dynamo
module: nn	129 unexpected none value for stream with dynamo
module: onnx	129 unexpected none value for stream with dynamo
module: optimizer	129 unexpected none value for stream with dynamo
module: tests	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: docs	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: numpy	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: cpp	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: cuda	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: autograd	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: nn	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: onnx	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: optimizer	130  docs  add reference decomp impl snippets to functions in online docs for hackability and educational purposes   compensate for some unclear language in existing docs
module: tests	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: docs	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: numpy	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: cpp	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: cuda	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: autograd	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: nn	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: onnx	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: optimizer	131  docs  distributed  add migration notes for   local rank option style change for torchrun in pytorch
module: tests	132  docs  document dtype conversions dtype to complex   dtype to real
module: docs	132  docs  document dtype conversions dtype to complex   dtype to real
module: numpy	132  docs  document dtype conversions dtype to complex   dtype to real
module: cpp	132  docs  document dtype conversions dtype to complex   dtype to real
module: cuda	132  docs  document dtype conversions dtype to complex   dtype to real
module: autograd	132  docs  document dtype conversions dtype to complex   dtype to real
module: nn	132  docs  document dtype conversions dtype to complex   dtype to real
module: onnx	132  docs  document dtype conversions dtype to complex   dtype to real
module: optimizer	132  docs  document dtype conversions dtype to complex   dtype to real
module: tests	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: docs	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: numpy	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: cpp	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: cuda	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: autograd	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: nn	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: onnx	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: optimizer	133  docs  f interpolate uint  input  mode    bicubic        overshoot behavior  adjust the note in docs to explain that for uint  saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint  inputs
module: tests	134 adaptivemaxpool documentation is not detailed
module: docs	134 adaptivemaxpool documentation is not detailed
module: numpy	134 adaptivemaxpool documentation is not detailed
module: cpp	134 adaptivemaxpool documentation is not detailed
module: cuda	134 adaptivemaxpool documentation is not detailed
module: autograd	134 adaptivemaxpool documentation is not detailed
module: nn	134 adaptivemaxpool documentation is not detailed
module: onnx	134 adaptivemaxpool documentation is not detailed
module: optimizer	134 adaptivemaxpool documentation is not detailed
module: tests	135 add a diagram showing the code structure to contributing md
module: docs	135 add a diagram showing the code structure to contributing md
module: numpy	135 add a diagram showing the code structure to contributing md
module: cpp	135 add a diagram showing the code structure to contributing md
module: cuda	135 add a diagram showing the code structure to contributing md
module: autograd	135 add a diagram showing the code structure to contributing md
module: nn	135 add a diagram showing the code structure to contributing md
module: onnx	135 add a diagram showing the code structure to contributing md
module: optimizer	135 add a diagram showing the code structure to contributing md
module: tests	136 add docs for   tensor flatten       tensor unflatten
module: docs	136 add docs for   tensor flatten       tensor unflatten
module: numpy	136 add docs for   tensor flatten       tensor unflatten
module: cpp	136 add docs for   tensor flatten       tensor unflatten
module: cuda	136 add docs for   tensor flatten       tensor unflatten
module: autograd	136 add docs for   tensor flatten       tensor unflatten
module: nn	136 add docs for   tensor flatten       tensor unflatten
module: onnx	136 add docs for   tensor flatten       tensor unflatten
module: optimizer	136 add docs for   tensor flatten       tensor unflatten
module: tests	137 ambiguous docstring on register module forward hook
module: docs	137 ambiguous docstring on register module forward hook
module: numpy	137 ambiguous docstring on register module forward hook
module: cpp	137 ambiguous docstring on register module forward hook
module: cuda	137 ambiguous docstring on register module forward hook
module: autograd	137 ambiguous docstring on register module forward hook
module: nn	137 ambiguous docstring on register module forward hook
module: onnx	137 ambiguous docstring on register module forward hook
module: optimizer	137 ambiguous docstring on register module forward hook
module: tests	138 autocast documentation examples would break
module: docs	138 autocast documentation examples would break
module: numpy	138 autocast documentation examples would break
module: cpp	138 autocast documentation examples would break
module: cuda	138 autocast documentation examples would break
module: autograd	138 autocast documentation examples would break
module: nn	138 autocast documentation examples would break
module: onnx	138 autocast documentation examples would break
module: optimizer	138 autocast documentation examples would break
module: tests	139 can t build pytorch     from source by gcc      on m  macos
module: docs	139 can t build pytorch     from source by gcc      on m  macos
module: numpy	139 can t build pytorch     from source by gcc      on m  macos
module: cpp	139 can t build pytorch     from source by gcc      on m  macos
module: cuda	139 can t build pytorch     from source by gcc      on m  macos
module: autograd	139 can t build pytorch     from source by gcc      on m  macos
module: nn	139 can t build pytorch     from source by gcc      on m  macos
module: onnx	139 can t build pytorch     from source by gcc      on m  macos
module: optimizer	139 can t build pytorch     from source by gcc      on m  macos
module: tests	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: docs	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: numpy	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: cpp	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: cuda	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: autograd	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: nn	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: onnx	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: optimizer	140 citation request for  probabilities for each class  in the doc s description on cross entropy
module: tests	141 coverage test is only checking packages and not all submodules
module: docs	141 coverage test is only checking packages and not all submodules
module: numpy	141 coverage test is only checking packages and not all submodules
module: cpp	141 coverage test is only checking packages and not all submodules
module: cuda	141 coverage test is only checking packages and not all submodules
module: autograd	141 coverage test is only checking packages and not all submodules
module: nn	141 coverage test is only checking packages and not all submodules
module: onnx	141 coverage test is only checking packages and not all submodules
module: optimizer	141 coverage test is only checking packages and not all submodules
module: tests	142 documentation and typing hints for rprop
module: docs	142 documentation and typing hints for rprop
module: numpy	142 documentation and typing hints for rprop
module: cpp	142 documentation and typing hints for rprop
module: cuda	142 documentation and typing hints for rprop
module: autograd	142 documentation and typing hints for rprop
module: nn	142 documentation and typing hints for rprop
module: onnx	142 documentation and typing hints for rprop
module: optimizer	142 documentation and typing hints for rprop
module: tests	143 documentation for torch cuda event blocking true  is wrong
module: docs	143 documentation for torch cuda event blocking true  is wrong
module: numpy	143 documentation for torch cuda event blocking true  is wrong
module: cpp	143 documentation for torch cuda event blocking true  is wrong
module: cuda	143 documentation for torch cuda event blocking true  is wrong
module: autograd	143 documentation for torch cuda event blocking true  is wrong
module: nn	143 documentation for torch cuda event blocking true  is wrong
module: onnx	143 documentation for torch cuda event blocking true  is wrong
module: optimizer	143 documentation for torch cuda event blocking true  is wrong
module: tests	144 errors in contributing md
module: docs	144 errors in contributing md
module: numpy	144 errors in contributing md
module: cpp	144 errors in contributing md
module: cuda	144 errors in contributing md
module: autograd	144 errors in contributing md
module: nn	144 errors in contributing md
module: onnx	144 errors in contributing md
module: optimizer	144 errors in contributing md
module: tests	145 extra information messages for mac in setup py would help
module: docs	145 extra information messages for mac in setup py would help
module: numpy	145 extra information messages for mac in setup py would help
module: cpp	145 extra information messages for mac in setup py would help
module: cuda	145 extra information messages for mac in setup py would help
module: autograd	145 extra information messages for mac in setup py would help
module: nn	145 extra information messages for mac in setup py would help
module: onnx	145 extra information messages for mac in setup py would help
module: optimizer	145 extra information messages for mac in setup py would help
module: tests	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: docs	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: numpy	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: cpp	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: cuda	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: autograd	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: nn	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: onnx	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: optimizer	146 fix docstring errors in  torch docs py  serialization py  overrides py   utils py
module: tests	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: docs	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: numpy	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: cpp	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: cuda	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: autograd	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: nn	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: onnx	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: optimizer	147 fix docstring errors in embedding py   limiter utils py   dynamo utils py  embedding bag py  tensor ops py  api py   internals py   common py  init py   exec order utils py   traversal utils py  chunk sharding spec py   trace utils py
module: tests	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: docs	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: numpy	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: cpp	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: cuda	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: autograd	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: nn	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: onnx	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: optimizer	148 fix docstring errors in nadam py  radam py  sgd py  anomaly mode py  rprop py    init   py  swa utils py  rmsprop py  optimizer py  lr scheduler py
module: tests	149 guide for diagnosing excess graph breaks
module: docs	149 guide for diagnosing excess graph breaks
module: numpy	149 guide for diagnosing excess graph breaks
module: cpp	149 guide for diagnosing excess graph breaks
module: cuda	149 guide for diagnosing excess graph breaks
module: autograd	149 guide for diagnosing excess graph breaks
module: nn	149 guide for diagnosing excess graph breaks
module: onnx	149 guide for diagnosing excess graph breaks
module: optimizer	149 guide for diagnosing excess graph breaks
module: tests	150 improving clarity in the docs of different losses
module: docs	150 improving clarity in the docs of different losses
module: numpy	150 improving clarity in the docs of different losses
module: cpp	150 improving clarity in the docs of different losses
module: cuda	150 improving clarity in the docs of different losses
module: autograd	150 improving clarity in the docs of different losses
module: nn	150 improving clarity in the docs of different losses
module: onnx	150 improving clarity in the docs of different losses
module: optimizer	150 improving clarity in the docs of different losses
module: tests	151 incorrect version in the instructions on official website
module: docs	151 incorrect version in the instructions on official website
module: numpy	151 incorrect version in the instructions on official website
module: cpp	151 incorrect version in the instructions on official website
module: cuda	151 incorrect version in the instructions on official website
module: autograd	151 incorrect version in the instructions on official website
module: nn	151 incorrect version in the instructions on official website
module: onnx	151 incorrect version in the instructions on official website
module: optimizer	151 incorrect version in the instructions on official website
module: tests	152 is there doc that explains how to call an extension op in another extension implementation
module: docs	152 is there doc that explains how to call an extension op in another extension implementation
module: numpy	152 is there doc that explains how to call an extension op in another extension implementation
module: cpp	152 is there doc that explains how to call an extension op in another extension implementation
module: cuda	152 is there doc that explains how to call an extension op in another extension implementation
module: autograd	152 is there doc that explains how to call an extension op in another extension implementation
module: nn	152 is there doc that explains how to call an extension op in another extension implementation
module: onnx	152 is there doc that explains how to call an extension op in another extension implementation
module: optimizer	152 is there doc that explains how to call an extension op in another extension implementation
module: tests	153 masked scatter  is very lacking
module: docs	153 masked scatter  is very lacking
module: numpy	153 masked scatter  is very lacking
module: cpp	153 masked scatter  is very lacking
module: cuda	153 masked scatter  is very lacking
module: autograd	153 masked scatter  is very lacking
module: nn	153 masked scatter  is very lacking
module: onnx	153 masked scatter  is very lacking
module: optimizer	153 masked scatter  is very lacking
module: tests	154 mps backend doc  model   yourfavoritenet   not defined
module: docs	154 mps backend doc  model   yourfavoritenet   not defined
module: numpy	154 mps backend doc  model   yourfavoritenet   not defined
module: cpp	154 mps backend doc  model   yourfavoritenet   not defined
module: cuda	154 mps backend doc  model   yourfavoritenet   not defined
module: autograd	154 mps backend doc  model   yourfavoritenet   not defined
module: nn	154 mps backend doc  model   yourfavoritenet   not defined
module: onnx	154 mps backend doc  model   yourfavoritenet   not defined
module: optimizer	154 mps backend doc  model   yourfavoritenet   not defined
module: tests	155 multiple invalid summaries in torch nn documentation page
module: docs	155 multiple invalid summaries in torch nn documentation page
module: numpy	155 multiple invalid summaries in torch nn documentation page
module: cpp	155 multiple invalid summaries in torch nn documentation page
module: cuda	155 multiple invalid summaries in torch nn documentation page
module: autograd	155 multiple invalid summaries in torch nn documentation page
module: nn	155 multiple invalid summaries in torch nn documentation page
module: onnx	155 multiple invalid summaries in torch nn documentation page
module: optimizer	155 multiple invalid summaries in torch nn documentation page
module: tests	156 no document for parameter load debug files in torch  jit  load in c   api
module: docs	156 no document for parameter load debug files in torch  jit  load in c   api
module: numpy	156 no document for parameter load debug files in torch  jit  load in c   api
module: cpp	156 no document for parameter load debug files in torch  jit  load in c   api
module: cuda	156 no document for parameter load debug files in torch  jit  load in c   api
module: autograd	156 no document for parameter load debug files in torch  jit  load in c   api
module: nn	156 no document for parameter load debug files in torch  jit  load in c   api
module: onnx	156 no document for parameter load debug files in torch  jit  load in c   api
module: optimizer	156 no document for parameter load debug files in torch  jit  load in c   api
module: tests	157 no documentation to show how to implement aten  view for custom backend
module: docs	157 no documentation to show how to implement aten  view for custom backend
module: numpy	157 no documentation to show how to implement aten  view for custom backend
module: cpp	157 no documentation to show how to implement aten  view for custom backend
module: cuda	157 no documentation to show how to implement aten  view for custom backend
module: autograd	157 no documentation to show how to implement aten  view for custom backend
module: nn	157 no documentation to show how to implement aten  view for custom backend
module: onnx	157 no documentation to show how to implement aten  view for custom backend
module: optimizer	157 no documentation to show how to implement aten  view for custom backend
module: tests	158 odd hand wavy mathematical notation for conv d
module: docs	158 odd hand wavy mathematical notation for conv d
module: numpy	158 odd hand wavy mathematical notation for conv d
module: cpp	158 odd hand wavy mathematical notation for conv d
module: cuda	158 odd hand wavy mathematical notation for conv d
module: autograd	158 odd hand wavy mathematical notation for conv d
module: nn	158 odd hand wavy mathematical notation for conv d
module: onnx	158 odd hand wavy mathematical notation for conv d
module: optimizer	158 odd hand wavy mathematical notation for conv d
module: tests	159 patch
module: docs	159 patch
module: numpy	159 patch
module: cpp	159 patch
module: cuda	159 patch
module: autograd	159 patch
module: nn	159 patch
module: onnx	159 patch
module: optimizer	159 patch
module: tests	160 permute
module: docs	160 permute
module: numpy	160 permute
module: cpp	160 permute
module: cuda	160 permute
module: autograd	160 permute
module: nn	160 permute
module: onnx	160 permute
module: optimizer	160 permute
module: tests	161 profiler documentation
module: docs	161 profiler documentation
module: numpy	161 profiler documentation
module: cpp	161 profiler documentation
module: cuda	161 profiler documentation
module: autograd	161 profiler documentation
module: nn	161 profiler documentation
module: onnx	161 profiler documentation
module: optimizer	161 profiler documentation
module: tests	162 sdpa tutorial   fails for cpu runs on google colab
module: docs	162 sdpa tutorial   fails for cpu runs on google colab
module: numpy	162 sdpa tutorial   fails for cpu runs on google colab
module: cpp	162 sdpa tutorial   fails for cpu runs on google colab
module: cuda	162 sdpa tutorial   fails for cpu runs on google colab
module: autograd	162 sdpa tutorial   fails for cpu runs on google colab
module: nn	162 sdpa tutorial   fails for cpu runs on google colab
module: onnx	162 sdpa tutorial   fails for cpu runs on google colab
module: optimizer	162 sdpa tutorial   fails for cpu runs on google colab
module: tests	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: docs	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: numpy	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: cpp	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: cuda	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: autograd	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: nn	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: onnx	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: optimizer	163 semantics of sparse operations clarification   sparsity of the gradient with respect to a sparse tensor input
module: tests	164 some c   library docstrings incorrectly linked repeated
module: docs	164 some c   library docstrings incorrectly linked repeated
module: numpy	164 some c   library docstrings incorrectly linked repeated
module: cpp	164 some c   library docstrings incorrectly linked repeated
module: cuda	164 some c   library docstrings incorrectly linked repeated
module: autograd	164 some c   library docstrings incorrectly linked repeated
module: nn	164 some c   library docstrings incorrectly linked repeated
module: onnx	164 some c   library docstrings incorrectly linked repeated
module: optimizer	164 some c   library docstrings incorrectly linked repeated
module: tests	165 specify version
module: docs	165 specify version
module: numpy	165 specify version
module: cpp	165 specify version
module: cuda	165 specify version
module: autograd	165 specify version
module: nn	165 specify version
module: onnx	165 specify version
module: optimizer	165 specify version
module: tests	166 the formula for kl divloss is wrong in the document
module: docs	166 the formula for kl divloss is wrong in the document
module: numpy	166 the formula for kl divloss is wrong in the document
module: cpp	166 the formula for kl divloss is wrong in the document
module: cuda	166 the formula for kl divloss is wrong in the document
module: autograd	166 the formula for kl divloss is wrong in the document
module: nn	166 the formula for kl divloss is wrong in the document
module: onnx	166 the formula for kl divloss is wrong in the document
module: optimizer	166 the formula for kl divloss is wrong in the document
module: tests	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: docs	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: numpy	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: cpp	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: cuda	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: autograd	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: nn	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: onnx	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: optimizer	167 torch addcdiv  input  tensor   and tensor  parameters should be of the same type
module: tests	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: docs	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: numpy	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: cpp	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: cuda	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: autograd	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: nn	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: onnx	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: optimizer	168 torch jit load documentation doesn t specify if it is safe to load untrusted models or not
module: tests	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: docs	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: numpy	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: cpp	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: cuda	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: autograd	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: nn	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: onnx	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: optimizer	169 torch tensor and torch as tensor keyword argument device documentation wrong
module: tests	170 torch tensor layout is not documented
module: docs	170 torch tensor layout is not documented
module: numpy	170 torch tensor layout is not documented
module: cpp	170 torch tensor layout is not documented
module: cuda	170 torch tensor layout is not documented
module: autograd	170 torch tensor layout is not documented
module: nn	170 torch tensor layout is not documented
module: onnx	170 torch tensor layout is not documented
module: optimizer	170 torch tensor layout is not documented
module: tests	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: docs	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: numpy	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: cpp	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: cuda	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: autograd	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: nn	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: onnx	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: optimizer	171 undocumented error on torch autograd function jvp for non tensor forward returns
module: tests	172 wrong example of sliced computation in doc page numerical accuracy
module: docs	172 wrong example of sliced computation in doc page numerical accuracy
module: numpy	172 wrong example of sliced computation in doc page numerical accuracy
module: cpp	172 wrong example of sliced computation in doc page numerical accuracy
module: cuda	172 wrong example of sliced computation in doc page numerical accuracy
module: autograd	172 wrong example of sliced computation in doc page numerical accuracy
module: nn	172 wrong example of sliced computation in doc page numerical accuracy
module: onnx	172 wrong example of sliced computation in doc page numerical accuracy
module: optimizer	172 wrong example of sliced computation in doc page numerical accuracy
module: tests	173 wrong functionalization of as strided leads to wrong results
module: docs	173 wrong functionalization of as strided leads to wrong results
module: numpy	173 wrong functionalization of as strided leads to wrong results
module: cpp	173 wrong functionalization of as strided leads to wrong results
module: cuda	173 wrong functionalization of as strided leads to wrong results
module: autograd	173 wrong functionalization of as strided leads to wrong results
module: nn	173 wrong functionalization of as strided leads to wrong results
module: onnx	173 wrong functionalization of as strided leads to wrong results
module: optimizer	173 wrong functionalization of as strided leads to wrong results
module: tests	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: docs	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: numpy	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: cpp	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: cuda	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: autograd	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: nn	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: onnx	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: optimizer	174  bc breaking  change default behavior of scaled dot product attention s causal masking alignment
module: tests	175  rfc  scaled dot product attention  api changes
module: docs	175  rfc  scaled dot product attention  api changes
module: numpy	175  rfc  scaled dot product attention  api changes
module: cpp	175  rfc  scaled dot product attention  api changes
module: cuda	175  rfc  scaled dot product attention  api changes
module: autograd	175  rfc  scaled dot product attention  api changes
module: nn	175  rfc  scaled dot product attention  api changes
module: onnx	175  rfc  scaled dot product attention  api changes
module: optimizer	175  rfc  scaled dot product attention  api changes
module: tests	176  torchdistx  future of the large model initialization
module: docs	176  torchdistx  future of the large model initialization
module: numpy	176  torchdistx  future of the large model initialization
module: cpp	176  torchdistx  future of the large model initialization
module: cuda	176  torchdistx  future of the large model initialization
module: autograd	176  torchdistx  future of the large model initialization
module: nn	176  torchdistx  future of the large model initialization
module: onnx	176  torchdistx  future of the large model initialization
module: optimizer	176  torchdistx  future of the large model initialization
module: tests	177 adaptiveavgpool d failed in the lower version
module: docs	177 adaptiveavgpool d failed in the lower version
module: numpy	177 adaptiveavgpool d failed in the lower version
module: cpp	177 adaptiveavgpool d failed in the lower version
module: cuda	177 adaptiveavgpool d failed in the lower version
module: autograd	177 adaptiveavgpool d failed in the lower version
module: nn	177 adaptiveavgpool d failed in the lower version
module: onnx	177 adaptiveavgpool d failed in the lower version
module: optimizer	177 adaptiveavgpool d failed in the lower version
module: tests	178 add a deterministic version of reflection pad d backward cuda
module: docs	178 add a deterministic version of reflection pad d backward cuda
module: numpy	178 add a deterministic version of reflection pad d backward cuda
module: cpp	178 add a deterministic version of reflection pad d backward cuda
module: cuda	178 add a deterministic version of reflection pad d backward cuda
module: autograd	178 add a deterministic version of reflection pad d backward cuda
module: nn	178 add a deterministic version of reflection pad d backward cuda
module: onnx	178 add a deterministic version of reflection pad d backward cuda
module: optimizer	178 add a deterministic version of reflection pad d backward cuda
module: tests	179 add additional  sigmoid  approximation to gelu activation
module: docs	179 add additional  sigmoid  approximation to gelu activation
module: numpy	179 add additional  sigmoid  approximation to gelu activation
module: cpp	179 add additional  sigmoid  approximation to gelu activation
module: cuda	179 add additional  sigmoid  approximation to gelu activation
module: autograd	179 add additional  sigmoid  approximation to gelu activation
module: nn	179 add additional  sigmoid  approximation to gelu activation
module: onnx	179 add additional  sigmoid  approximation to gelu activation
module: optimizer	179 add additional  sigmoid  approximation to gelu activation
module: tests	180 add head mask for transformers
module: docs	180 add head mask for transformers
module: numpy	180 add head mask for transformers
module: cpp	180 add head mask for transformers
module: cuda	180 add head mask for transformers
module: autograd	180 add head mask for transformers
module: nn	180 add head mask for transformers
module: onnx	180 add head mask for transformers
module: optimizer	180 add head mask for transformers
module: tests	181 backward hook execution order changes when input requires grad is false
module: docs	181 backward hook execution order changes when input requires grad is false
module: numpy	181 backward hook execution order changes when input requires grad is false
module: cpp	181 backward hook execution order changes when input requires grad is false
module: cuda	181 backward hook execution order changes when input requires grad is false
module: autograd	181 backward hook execution order changes when input requires grad is false
module: nn	181 backward hook execution order changes when input requires grad is false
module: onnx	181 backward hook execution order changes when input requires grad is false
module: optimizer	181 backward hook execution order changes when input requires grad is false
module: tests	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: docs	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: numpy	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: cpp	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: cuda	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: autograd	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: nn	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: onnx	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: optimizer	182 conv d  nnpack spatialconvolution updateoutput failed when batchsize or padding is too large
module: tests	183 could be clearer that cross entropy takes logits as input
module: docs	183 could be clearer that cross entropy takes logits as input
module: numpy	183 could be clearer that cross entropy takes logits as input
module: cpp	183 could be clearer that cross entropy takes logits as input
module: cuda	183 could be clearer that cross entropy takes logits as input
module: autograd	183 could be clearer that cross entropy takes logits as input
module: nn	183 could be clearer that cross entropy takes logits as input
module: onnx	183 could be clearer that cross entropy takes logits as input
module: optimizer	183 could be clearer that cross entropy takes logits as input
module: tests	184 depthwise conv d slower than normal conv d
module: docs	184 depthwise conv d slower than normal conv d
module: numpy	184 depthwise conv d slower than normal conv d
module: cpp	184 depthwise conv d slower than normal conv d
module: cuda	184 depthwise conv d slower than normal conv d
module: autograd	184 depthwise conv d slower than normal conv d
module: nn	184 depthwise conv d slower than normal conv d
module: onnx	184 depthwise conv d slower than normal conv d
module: optimizer	184 depthwise conv d slower than normal conv d
module: tests	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: docs	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: numpy	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: cpp	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: cuda	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: autograd	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: nn	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: onnx	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: optimizer	185 disabled test decoder padding and src mask bool cpu    main   testtransformerscpu
module: tests	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: docs	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: numpy	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: cpp	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: cuda	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: autograd	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: nn	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: onnx	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: optimizer	186 disabled test grad nn functional conv d cuda float      main   testoperatorscuda
module: tests	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: docs	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: numpy	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: cpp	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: cuda	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: autograd	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: nn	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: onnx	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: optimizer	187 disabled test grad nn groupnorm cuda float      main   testmodulecuda
module: tests	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: docs	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: numpy	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: cpp	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: cuda	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: autograd	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: nn	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: onnx	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: optimizer	188 disabled test gradgrad nn groupnorm cuda float      main   testmodulecuda
module: tests	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: docs	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: numpy	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: cpp	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: cuda	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: autograd	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: nn	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: onnx	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: optimizer	189 disabled test memory format nn convtranspose d cuda complex      main   testmodulecuda
module: tests	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: docs	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: numpy	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: cpp	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: cuda	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: autograd	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: nn	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: onnx	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: optimizer	190 dynamo can t parse torch rrelu or torch nn functional rrelu
module: tests	191 easy way to  freeze  batchnorm running mean running var
module: docs	191 easy way to  freeze  batchnorm running mean running var
module: numpy	191 easy way to  freeze  batchnorm running mean running var
module: cpp	191 easy way to  freeze  batchnorm running mean running var
module: cuda	191 easy way to  freeze  batchnorm running mean running var
module: autograd	191 easy way to  freeze  batchnorm running mean running var
module: nn	191 easy way to  freeze  batchnorm running mean running var
module: onnx	191 easy way to  freeze  batchnorm running mean running var
module: optimizer	191 easy way to  freeze  batchnorm running mean running var
module: tests	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: docs	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: numpy	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: cpp	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: cuda	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: autograd	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: nn	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: onnx	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: optimizer	192 expandedweights sometimes fail silently and doesn t compute  grad sample attribute
module: tests	193 exported model with dropout incorrectly applies dropout during eval
module: docs	193 exported model with dropout incorrectly applies dropout during eval
module: numpy	193 exported model with dropout incorrectly applies dropout during eval
module: cpp	193 exported model with dropout incorrectly applies dropout during eval
module: cuda	193 exported model with dropout incorrectly applies dropout during eval
module: autograd	193 exported model with dropout incorrectly applies dropout during eval
module: nn	193 exported model with dropout incorrectly applies dropout during eval
module: onnx	193 exported model with dropout incorrectly applies dropout during eval
module: optimizer	193 exported model with dropout incorrectly applies dropout during eval
module: tests	194 groupnorm   instancenorm does not handle channels last correctly
module: docs	194 groupnorm   instancenorm does not handle channels last correctly
module: numpy	194 groupnorm   instancenorm does not handle channels last correctly
module: cpp	194 groupnorm   instancenorm does not handle channels last correctly
module: cuda	194 groupnorm   instancenorm does not handle channels last correctly
module: autograd	194 groupnorm   instancenorm does not handle channels last correctly
module: nn	194 groupnorm   instancenorm does not handle channels last correctly
module: onnx	194 groupnorm   instancenorm does not handle channels last correctly
module: optimizer	194 groupnorm   instancenorm does not handle channels last correctly
module: tests	195 hidden rule in nn fractionalmaxpool d
module: docs	195 hidden rule in nn fractionalmaxpool d
module: numpy	195 hidden rule in nn fractionalmaxpool d
module: cpp	195 hidden rule in nn fractionalmaxpool d
module: cuda	195 hidden rule in nn fractionalmaxpool d
module: autograd	195 hidden rule in nn fractionalmaxpool d
module: nn	195 hidden rule in nn fractionalmaxpool d
module: onnx	195 hidden rule in nn fractionalmaxpool d
module: optimizer	195 hidden rule in nn fractionalmaxpool d
module: tests	196 inconsistent behavior of convtranspose d on cpu and cuda
module: docs	196 inconsistent behavior of convtranspose d on cpu and cuda
module: numpy	196 inconsistent behavior of convtranspose d on cpu and cuda
module: cpp	196 inconsistent behavior of convtranspose d on cpu and cuda
module: cuda	196 inconsistent behavior of convtranspose d on cpu and cuda
module: autograd	196 inconsistent behavior of convtranspose d on cpu and cuda
module: nn	196 inconsistent behavior of convtranspose d on cpu and cuda
module: onnx	196 inconsistent behavior of convtranspose d on cpu and cuda
module: optimizer	196 inconsistent behavior of convtranspose d on cpu and cuda
module: tests	197 instancenorm does not catch dim mismatch
module: docs	197 instancenorm does not catch dim mismatch
module: numpy	197 instancenorm does not catch dim mismatch
module: cpp	197 instancenorm does not catch dim mismatch
module: cuda	197 instancenorm does not catch dim mismatch
module: autograd	197 instancenorm does not catch dim mismatch
module: nn	197 instancenorm does not catch dim mismatch
module: onnx	197 instancenorm does not catch dim mismatch
module: optimizer	197 instancenorm does not catch dim mismatch
module: tests	198 investigate lazy   norm   d modules no batch dim support
module: docs	198 investigate lazy   norm   d modules no batch dim support
module: numpy	198 investigate lazy   norm   d modules no batch dim support
module: cpp	198 investigate lazy   norm   d modules no batch dim support
module: cuda	198 investigate lazy   norm   d modules no batch dim support
module: autograd	198 investigate lazy   norm   d modules no batch dim support
module: nn	198 investigate lazy   norm   d modules no batch dim support
module: onnx	198 investigate lazy   norm   d modules no batch dim support
module: optimizer	198 investigate lazy   norm   d modules no batch dim support
module: tests	199 log softmax could be        to         times more accurate on small outputs
module: docs	199 log softmax could be        to         times more accurate on small outputs
module: numpy	199 log softmax could be        to         times more accurate on small outputs
module: cpp	199 log softmax could be        to         times more accurate on small outputs
module: cuda	199 log softmax could be        to         times more accurate on small outputs
module: autograd	199 log softmax could be        to         times more accurate on small outputs
module: nn	199 log softmax could be        to         times more accurate on small outputs
module: onnx	199 log softmax could be        to         times more accurate on small outputs
module: optimizer	199 log softmax could be        to         times more accurate on small outputs
module: tests	200 make dropout take a dim     argument
module: docs	200 make dropout take a dim     argument
module: numpy	200 make dropout take a dim     argument
module: cpp	200 make dropout take a dim     argument
module: cuda	200 make dropout take a dim     argument
module: autograd	200 make dropout take a dim     argument
module: nn	200 make dropout take a dim     argument
module: onnx	200 make dropout take a dim     argument
module: optimizer	200 make dropout take a dim     argument
module: tests	201 many padding module fail memory format tests
module: docs	201 many padding module fail memory format tests
module: numpy	201 many padding module fail memory format tests
module: cpp	201 many padding module fail memory format tests
module: cuda	201 many padding module fail memory format tests
module: autograd	201 many padding module fail memory format tests
module: nn	201 many padding module fail memory format tests
module: onnx	201 many padding module fail memory format tests
module: optimizer	201 many padding module fail memory format tests
module: tests	202 max pool d can succeed when padding is negative for tensor requiring grad
module: docs	202 max pool d can succeed when padding is negative for tensor requiring grad
module: numpy	202 max pool d can succeed when padding is negative for tensor requiring grad
module: cpp	202 max pool d can succeed when padding is negative for tensor requiring grad
module: cuda	202 max pool d can succeed when padding is negative for tensor requiring grad
module: autograd	202 max pool d can succeed when padding is negative for tensor requiring grad
module: nn	202 max pool d can succeed when padding is negative for tensor requiring grad
module: onnx	202 max pool d can succeed when padding is negative for tensor requiring grad
module: optimizer	202 max pool d can succeed when padding is negative for tensor requiring grad
module: tests	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: docs	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: numpy	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: cpp	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: cuda	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: autograd	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: nn	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: onnx	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: optimizer	203 max pool d with indices backward cuda and avg pool d backward cuda does not have a deterministic implementation
module: tests	204 max unpool d is not deterministic
module: docs	204 max unpool d is not deterministic
module: numpy	204 max unpool d is not deterministic
module: cpp	204 max unpool d is not deterministic
module: cuda	204 max unpool d is not deterministic
module: autograd	204 max unpool d is not deterministic
module: nn	204 max unpool d is not deterministic
module: onnx	204 max unpool d is not deterministic
module: optimizer	204 max unpool d is not deterministic
module: tests	205 max unpool gives wrong gradient when indices has duplicate
module: docs	205 max unpool gives wrong gradient when indices has duplicate
module: numpy	205 max unpool gives wrong gradient when indices has duplicate
module: cpp	205 max unpool gives wrong gradient when indices has duplicate
module: cuda	205 max unpool gives wrong gradient when indices has duplicate
module: autograd	205 max unpool gives wrong gradient when indices has duplicate
module: nn	205 max unpool gives wrong gradient when indices has duplicate
module: onnx	205 max unpool gives wrong gradient when indices has duplicate
module: optimizer	205 max unpool gives wrong gradient when indices has duplicate
module: tests	206 need  valid  and  same  padding mode for convtranspose d
module: docs	206 need  valid  and  same  padding mode for convtranspose d
module: numpy	206 need  valid  and  same  padding mode for convtranspose d
module: cpp	206 need  valid  and  same  padding mode for convtranspose d
module: cuda	206 need  valid  and  same  padding mode for convtranspose d
module: autograd	206 need  valid  and  same  padding mode for convtranspose d
module: nn	206 need  valid  and  same  padding mode for convtranspose d
module: onnx	206 need  valid  and  same  padding mode for convtranspose d
module: optimizer	206 need  valid  and  same  padding mode for convtranspose d
module: tests	207 negative values still produced by torch nn functional kl div
module: docs	207 negative values still produced by torch nn functional kl div
module: numpy	207 negative values still produced by torch nn functional kl div
module: cpp	207 negative values still produced by torch nn functional kl div
module: cuda	207 negative values still produced by torch nn functional kl div
module: autograd	207 negative values still produced by torch nn functional kl div
module: nn	207 negative values still produced by torch nn functional kl div
module: onnx	207 negative values still produced by torch nn functional kl div
module: optimizer	207 negative values still produced by torch nn functional kl div
module: tests	208 parameter   deepcopy   doesn t preserve view relationships
module: docs	208 parameter   deepcopy   doesn t preserve view relationships
module: numpy	208 parameter   deepcopy   doesn t preserve view relationships
module: cpp	208 parameter   deepcopy   doesn t preserve view relationships
module: cuda	208 parameter   deepcopy   doesn t preserve view relationships
module: autograd	208 parameter   deepcopy   doesn t preserve view relationships
module: nn	208 parameter   deepcopy   doesn t preserve view relationships
module: onnx	208 parameter   deepcopy   doesn t preserve view relationships
module: optimizer	208 parameter   deepcopy   doesn t preserve view relationships
module: tests	209 position embedding aware global circular convolution
module: docs	209 position embedding aware global circular convolution
module: numpy	209 position embedding aware global circular convolution
module: cpp	209 position embedding aware global circular convolution
module: cuda	209 position embedding aware global circular convolution
module: autograd	209 position embedding aware global circular convolution
module: nn	209 position embedding aware global circular convolution
module: onnx	209 position embedding aware global circular convolution
module: optimizer	209 position embedding aware global circular convolution
module: tests	210 potential issue with custom transformer masks when using fast path and batch first true
module: docs	210 potential issue with custom transformer masks when using fast path and batch first true
module: numpy	210 potential issue with custom transformer masks when using fast path and batch first true
module: cpp	210 potential issue with custom transformer masks when using fast path and batch first true
module: cuda	210 potential issue with custom transformer masks when using fast path and batch first true
module: autograd	210 potential issue with custom transformer masks when using fast path and batch first true
module: nn	210 potential issue with custom transformer masks when using fast path and batch first true
module: onnx	210 potential issue with custom transformer masks when using fast path and batch first true
module: optimizer	210 potential issue with custom transformer masks when using fast path and batch first true
module: tests	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: docs	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: numpy	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: cpp	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: cuda	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: autograd	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: nn	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: onnx	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: optimizer	211 regression bug in torch nn relu  and torch nn hardtanh that inplace true doesn t work in pytorch
module: tests	212 runtimeerror when calling conv transpose d with groups
module: docs	212 runtimeerror when calling conv transpose d with groups
module: numpy	212 runtimeerror when calling conv transpose d with groups
module: cpp	212 runtimeerror when calling conv transpose d with groups
module: cuda	212 runtimeerror when calling conv transpose d with groups
module: autograd	212 runtimeerror when calling conv transpose d with groups
module: nn	212 runtimeerror when calling conv transpose d with groups
module: onnx	212 runtimeerror when calling conv transpose d with groups
module: optimizer	212 runtimeerror when calling conv transpose d with groups
module: tests	213 should  native batch norm legit functional be in native functions yaml
module: docs	213 should  native batch norm legit functional be in native functions yaml
module: numpy	213 should  native batch norm legit functional be in native functions yaml
module: cpp	213 should  native batch norm legit functional be in native functions yaml
module: cuda	213 should  native batch norm legit functional be in native functions yaml
module: autograd	213 should  native batch norm legit functional be in native functions yaml
module: nn	213 should  native batch norm legit functional be in native functions yaml
module: onnx	213 should  native batch norm legit functional be in native functions yaml
module: optimizer	213 should  native batch norm legit functional be in native functions yaml
module: tests	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: docs	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: numpy	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: cpp	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: cuda	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: autograd	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: nn	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: onnx	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: optimizer	214 support bytetensor and shorttensor for nn embedding and nn embeddingbag
module: tests	215 support for learnable p values in lppool like pool
module: docs	215 support for learnable p values in lppool like pool
module: numpy	215 support for learnable p values in lppool like pool
module: cpp	215 support for learnable p values in lppool like pool
module: cuda	215 support for learnable p values in lppool like pool
module: autograd	215 support for learnable p values in lppool like pool
module: nn	215 support for learnable p values in lppool like pool
module: onnx	215 support for learnable p values in lppool like pool
module: optimizer	215 support for learnable p values in lppool like pool
module: tests	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: docs	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: numpy	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: cpp	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: cuda	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: autograd	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: nn	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: onnx	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: optimizer	216 torch nn conv d conv d s padding mode circular cannot accept   dim input
module: tests	217 torch nn crossentropyloss  class weighting changes label smoothing
module: docs	217 torch nn crossentropyloss  class weighting changes label smoothing
module: numpy	217 torch nn crossentropyloss  class weighting changes label smoothing
module: cpp	217 torch nn crossentropyloss  class weighting changes label smoothing
module: cuda	217 torch nn crossentropyloss  class weighting changes label smoothing
module: autograd	217 torch nn crossentropyloss  class weighting changes label smoothing
module: nn	217 torch nn crossentropyloss  class weighting changes label smoothing
module: onnx	217 torch nn crossentropyloss  class weighting changes label smoothing
module: optimizer	217 torch nn crossentropyloss  class weighting changes label smoothing
module: tests	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: docs	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: numpy	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: cpp	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: cuda	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: autograd	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: nn	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: onnx	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: optimizer	218 torch nn functional embedding bag throws an exception when it runs on a cpu  but it runs successfully on a gpu
module: tests	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: docs	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: numpy	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: cpp	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: cuda	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: autograd	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: nn	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: onnx	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: optimizer	219 torch nn functional softplus   torch nn softplus parameter beta can be set to zero
module: tests	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: docs	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: numpy	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: cpp	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: cuda	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: autograd	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: nn	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: onnx	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: optimizer	220 torch nn instancenorm       d doesn t verify the value type of parameter num features
module: tests	221 torch nn pairwisedistance the results vary widely from version to version
module: docs	221 torch nn pairwisedistance the results vary widely from version to version
module: numpy	221 torch nn pairwisedistance the results vary widely from version to version
module: cpp	221 torch nn pairwisedistance the results vary widely from version to version
module: cuda	221 torch nn pairwisedistance the results vary widely from version to version
module: autograd	221 torch nn pairwisedistance the results vary widely from version to version
module: nn	221 torch nn pairwisedistance the results vary widely from version to version
module: onnx	221 torch nn pairwisedistance the results vary widely from version to version
module: optimizer	221 torch nn pairwisedistance the results vary widely from version to version
module: tests	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: docs	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: numpy	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: cpp	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: cuda	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: autograd	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: nn	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: onnx	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: optimizer	222 torch nn replicationpad d report  invalid configuration argument  error under compute sanitizer
module: tests	223 transformer with convolutional position wise feed forward network
module: docs	223 transformer with convolutional position wise feed forward network
module: numpy	223 transformer with convolutional position wise feed forward network
module: cpp	223 transformer with convolutional position wise feed forward network
module: cuda	223 transformer with convolutional position wise feed forward network
module: autograd	223 transformer with convolutional position wise feed forward network
module: nn	223 transformer with convolutional position wise feed forward network
module: onnx	223 transformer with convolutional position wise feed forward network
module: optimizer	223 transformer with convolutional position wise feed forward network
module: tests	224 use isinstance instead of type when checking for torch nn parameter
module: docs	224 use isinstance instead of type when checking for torch nn parameter
module: numpy	224 use isinstance instead of type when checking for torch nn parameter
module: cpp	224 use isinstance instead of type when checking for torch nn parameter
module: cuda	224 use isinstance instead of type when checking for torch nn parameter
module: autograd	224 use isinstance instead of type when checking for torch nn parameter
module: nn	224 use isinstance instead of type when checking for torch nn parameter
module: onnx	224 use isinstance instead of type when checking for torch nn parameter
module: optimizer	224 use isinstance instead of type when checking for torch nn parameter
module: tests	225  bug  numpy is no longer a required dependency
module: docs	225  bug  numpy is no longer a required dependency
module: numpy	225  bug  numpy is no longer a required dependency
module: cpp	225  bug  numpy is no longer a required dependency
module: cuda	225  bug  numpy is no longer a required dependency
module: autograd	225  bug  numpy is no longer a required dependency
module: nn	225  bug  numpy is no longer a required dependency
module: onnx	225  bug  numpy is no longer a required dependency
module: optimizer	225  bug  numpy is no longer a required dependency
module: tests	226  complex  torch  exp   does not match numpy
module: docs	226  complex  torch  exp   does not match numpy
module: numpy	226  complex  torch  exp   does not match numpy
module: cpp	226  complex  torch  exp   does not match numpy
module: cuda	226  complex  torch  exp   does not match numpy
module: autograd	226  complex  torch  exp   does not match numpy
module: nn	226  complex  torch  exp   does not match numpy
module: onnx	226  complex  torch  exp   does not match numpy
module: optimizer	226  complex  torch  exp   does not match numpy
module: tests	227  doc  view appears to mean different things  view reshape vs transpose permute
module: docs	227  doc  view appears to mean different things  view reshape vs transpose permute
module: numpy	227  doc  view appears to mean different things  view reshape vs transpose permute
module: cpp	227  doc  view appears to mean different things  view reshape vs transpose permute
module: cuda	227  doc  view appears to mean different things  view reshape vs transpose permute
module: autograd	227  doc  view appears to mean different things  view reshape vs transpose permute
module: nn	227  doc  view appears to mean different things  view reshape vs transpose permute
module: onnx	227  doc  view appears to mean different things  view reshape vs transpose permute
module: optimizer	227  doc  view appears to mean different things  view reshape vs transpose permute
module: tests	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: docs	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: numpy	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: cpp	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: cuda	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: autograd	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: nn	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: onnx	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: optimizer	228  feature request  type promotions for boolean tensors with sub operation   numpy compatability
module: tests	229  numpy  add iscomplexobj and isrealobj
module: docs	229  numpy  add iscomplexobj and isrealobj
module: numpy	229  numpy  add iscomplexobj and isrealobj
module: cpp	229  numpy  add iscomplexobj and isrealobj
module: cuda	229  numpy  add iscomplexobj and isrealobj
module: autograd	229  numpy  add iscomplexobj and isrealobj
module: nn	229  numpy  add iscomplexobj and isrealobj
module: onnx	229  numpy  add iscomplexobj and isrealobj
module: optimizer	229  numpy  add iscomplexobj and isrealobj
module: tests	230  numpy  add torch newdim torch newaxis
module: docs	230  numpy  add torch newdim torch newaxis
module: numpy	230  numpy  add torch newdim torch newaxis
module: cpp	230  numpy  add torch newdim torch newaxis
module: cuda	230  numpy  add torch newdim torch newaxis
module: autograd	230  numpy  add torch newdim torch newaxis
module: nn	230  numpy  add torch newdim torch newaxis
module: onnx	230  numpy  add torch newdim torch newaxis
module: optimizer	230  numpy  add torch newdim torch newaxis
module: tests	231 add a numpy like pad function
module: docs	231 add a numpy like pad function
module: numpy	231 add a numpy like pad function
module: cpp	231 add a numpy like pad function
module: cuda	231 add a numpy like pad function
module: autograd	231 add a numpy like pad function
module: nn	231 add a numpy like pad function
module: onnx	231 add a numpy like pad function
module: optimizer	231 add a numpy like pad function
module: tests	232 add docs on pytorch   numpy interaction
module: docs	232 add docs on pytorch   numpy interaction
module: numpy	232 add docs on pytorch   numpy interaction
module: cpp	232 add docs on pytorch   numpy interaction
module: cuda	232 add docs on pytorch   numpy interaction
module: autograd	232 add docs on pytorch   numpy interaction
module: nn	232 add docs on pytorch   numpy interaction
module: onnx	232 add docs on pytorch   numpy interaction
module: optimizer	232 add docs on pytorch   numpy interaction
module: tests	233 add support for reading the whole file in from file
module: docs	233 add support for reading the whole file in from file
module: numpy	233 add support for reading the whole file in from file
module: cpp	233 add support for reading the whole file in from file
module: cuda	233 add support for reading the whole file in from file
module: autograd	233 add support for reading the whole file in from file
module: nn	233 add support for reading the whole file in from file
module: onnx	233 add support for reading the whole file in from file
module: optimizer	233 add support for reading the whole file in from file
module: tests	234 allow try except check for numpy bfloat   representation
module: docs	234 allow try except check for numpy bfloat   representation
module: numpy	234 allow try except check for numpy bfloat   representation
module: cpp	234 allow try except check for numpy bfloat   representation
module: cuda	234 allow try except check for numpy bfloat   representation
module: autograd	234 allow try except check for numpy bfloat   representation
module: nn	234 allow try except check for numpy bfloat   representation
module: onnx	234 allow try except check for numpy bfloat   representation
module: optimizer	234 allow try except check for numpy bfloat   representation
module: tests	235 better argument names for torch atan  and other math functions
module: docs	235 better argument names for torch atan  and other math functions
module: numpy	235 better argument names for torch atan  and other math functions
module: cpp	235 better argument names for torch atan  and other math functions
module: cuda	235 better argument names for torch atan  and other math functions
module: autograd	235 better argument names for torch atan  and other math functions
module: nn	235 better argument names for torch atan  and other math functions
module: onnx	235 better argument names for torch atan  and other math functions
module: optimizer	235 better argument names for torch atan  and other math functions
module: tests	236 better support for operators that return  named  tuples of tensors
module: docs	236 better support for operators that return  named  tuples of tensors
module: numpy	236 better support for operators that return  named  tuples of tensors
module: cpp	236 better support for operators that return  named  tuples of tensors
module: cuda	236 better support for operators that return  named  tuples of tensors
module: autograd	236 better support for operators that return  named  tuples of tensors
module: nn	236 better support for operators that return  named  tuples of tensors
module: onnx	236 better support for operators that return  named  tuples of tensors
module: optimizer	236 better support for operators that return  named  tuples of tensors
module: tests	237 broadcasting for torch cross
module: docs	237 broadcasting for torch cross
module: numpy	237 broadcasting for torch cross
module: cpp	237 broadcasting for torch cross
module: cuda	237 broadcasting for torch cross
module: autograd	237 broadcasting for torch cross
module: nn	237 broadcasting for torch cross
module: onnx	237 broadcasting for torch cross
module: optimizer	237 broadcasting for torch cross
module: tests	238 comparison ops for complex tensors
module: docs	238 comparison ops for complex tensors
module: numpy	238 comparison ops for complex tensors
module: cpp	238 comparison ops for complex tensors
module: cuda	238 comparison ops for complex tensors
module: autograd	238 comparison ops for complex tensors
module: nn	238 comparison ops for complex tensors
module: onnx	238 comparison ops for complex tensors
module: optimizer	238 comparison ops for complex tensors
module: tests	239 creating torch tensor as a function of index value
module: docs	239 creating torch tensor as a function of index value
module: numpy	239 creating torch tensor as a function of index value
module: cpp	239 creating torch tensor as a function of index value
module: cuda	239 creating torch tensor as a function of index value
module: autograd	239 creating torch tensor as a function of index value
module: nn	239 creating torch tensor as a function of index value
module: onnx	239 creating torch tensor as a function of index value
module: optimizer	239 creating torch tensor as a function of index value
module: tests	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: docs	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: numpy	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: cpp	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: cuda	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: autograd	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: nn	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: onnx	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: optimizer	240 disabled test dunder round edgecases val            ndigits       main   testnonarrayargs
module: tests	241 endpoint false for torch linspace and torch logspace
module: docs	241 endpoint false for torch linspace and torch logspace
module: numpy	241 endpoint false for torch linspace and torch logspace
module: cpp	241 endpoint false for torch linspace and torch logspace
module: cuda	241 endpoint false for torch linspace and torch logspace
module: autograd	241 endpoint false for torch linspace and torch logspace
module: nn	241 endpoint false for torch linspace and torch logspace
module: onnx	241 endpoint false for torch linspace and torch logspace
module: optimizer	241 endpoint false for torch linspace and torch logspace
module: tests	242 enhance supported types of functional pad
module: docs	242 enhance supported types of functional pad
module: numpy	242 enhance supported types of functional pad
module: cpp	242 enhance supported types of functional pad
module: cuda	242 enhance supported types of functional pad
module: autograd	242 enhance supported types of functional pad
module: nn	242 enhance supported types of functional pad
module: onnx	242 enhance supported types of functional pad
module: optimizer	242 enhance supported types of functional pad
module: tests	243 feature request  tests for int should be tests for numbers integral
module: docs	243 feature request  tests for int should be tests for numbers integral
module: numpy	243 feature request  tests for int should be tests for numbers integral
module: cpp	243 feature request  tests for int should be tests for numbers integral
module: cuda	243 feature request  tests for int should be tests for numbers integral
module: autograd	243 feature request  tests for int should be tests for numbers integral
module: nn	243 feature request  tests for int should be tests for numbers integral
module: onnx	243 feature request  tests for int should be tests for numbers integral
module: optimizer	243 feature request  tests for int should be tests for numbers integral
module: tests	244 function request  scipy interpolate griddata
module: docs	244 function request  scipy interpolate griddata
module: numpy	244 function request  scipy interpolate griddata
module: cpp	244 function request  scipy interpolate griddata
module: cuda	244 function request  scipy interpolate griddata
module: autograd	244 function request  scipy interpolate griddata
module: nn	244 function request  scipy interpolate griddata
module: onnx	244 function request  scipy interpolate griddata
module: optimizer	244 function request  scipy interpolate griddata
module: tests	245 function request  scipy interpolate interpolatedunivariatespline
module: docs	245 function request  scipy interpolate interpolatedunivariatespline
module: numpy	245 function request  scipy interpolate interpolatedunivariatespline
module: cpp	245 function request  scipy interpolate interpolatedunivariatespline
module: cuda	245 function request  scipy interpolate interpolatedunivariatespline
module: autograd	245 function request  scipy interpolate interpolatedunivariatespline
module: nn	245 function request  scipy interpolate interpolatedunivariatespline
module: onnx	245 function request  scipy interpolate interpolatedunivariatespline
module: optimizer	245 function request  scipy interpolate interpolatedunivariatespline
module: tests	246 implement missing torch nan  operators
module: docs	246 implement missing torch nan  operators
module: numpy	246 implement missing torch nan  operators
module: cpp	246 implement missing torch nan  operators
module: cuda	246 implement missing torch nan  operators
module: autograd	246 implement missing torch nan  operators
module: nn	246 implement missing torch nan  operators
module: onnx	246 implement missing torch nan  operators
module: optimizer	246 implement missing torch nan  operators
module: tests	247 implementing packbits
module: docs	247 implementing packbits
module: numpy	247 implementing packbits
module: cpp	247 implementing packbits
module: cuda	247 implementing packbits
module: autograd	247 implementing packbits
module: nn	247 implementing packbits
module: onnx	247 implementing packbits
module: optimizer	247 implementing packbits
module: tests	248 importing numpy interacts with tensor sum perf
module: docs	248 importing numpy interacts with tensor sum perf
module: numpy	248 importing numpy interacts with tensor sum perf
module: cpp	248 importing numpy interacts with tensor sum perf
module: cuda	248 importing numpy interacts with tensor sum perf
module: autograd	248 importing numpy interacts with tensor sum perf
module: nn	248 importing numpy interacts with tensor sum perf
module: onnx	248 importing numpy interacts with tensor sum perf
module: optimizer	248 importing numpy interacts with tensor sum perf
module: tests	249 inconsistent complex results with numpy when computing non positive power of
module: docs	249 inconsistent complex results with numpy when computing non positive power of
module: numpy	249 inconsistent complex results with numpy when computing non positive power of
module: cpp	249 inconsistent complex results with numpy when computing non positive power of
module: cuda	249 inconsistent complex results with numpy when computing non positive power of
module: autograd	249 inconsistent complex results with numpy when computing non positive power of
module: nn	249 inconsistent complex results with numpy when computing non positive power of
module: onnx	249 inconsistent complex results with numpy when computing non positive power of
module: optimizer	249 inconsistent complex results with numpy when computing non positive power of
module: tests	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: docs	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: numpy	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: cpp	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: cuda	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: autograd	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: nn	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: onnx	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: optimizer	250 indexing a tensor with a numpy array sometimes works and sometimes doesn t
module: tests	251 interpolation tracking issue
module: docs	251 interpolation tracking issue
module: numpy	251 interpolation tracking issue
module: cpp	251 interpolation tracking issue
module: cuda	251 interpolation tracking issue
module: autograd	251 interpolation tracking issue
module: nn	251 interpolation tracking issue
module: onnx	251 interpolation tracking issue
module: optimizer	251 interpolation tracking issue
module: tests	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: docs	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: numpy	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: cpp	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: cuda	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: autograd	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: nn	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: onnx	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: optimizer	252 list of tensors can t be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor
module: tests	253 make torch cross dim parameter work intuitively
module: docs	253 make torch cross dim parameter work intuitively
module: numpy	253 make torch cross dim parameter work intuitively
module: cpp	253 make torch cross dim parameter work intuitively
module: cuda	253 make torch cross dim parameter work intuitively
module: autograd	253 make torch cross dim parameter work intuitively
module: nn	253 make torch cross dim parameter work intuitively
module: onnx	253 make torch cross dim parameter work intuitively
module: optimizer	253 make torch cross dim parameter work intuitively
module: tests	254 mixing numpy s arrays and pytorch tensors
module: docs	254 mixing numpy s arrays and pytorch tensors
module: numpy	254 mixing numpy s arrays and pytorch tensors
module: cpp	254 mixing numpy s arrays and pytorch tensors
module: cuda	254 mixing numpy s arrays and pytorch tensors
module: autograd	254 mixing numpy s arrays and pytorch tensors
module: nn	254 mixing numpy s arrays and pytorch tensors
module: onnx	254 mixing numpy s arrays and pytorch tensors
module: optimizer	254 mixing numpy s arrays and pytorch tensors
module: tests	255 multiplication of torch tensor with np array does the operation with numpy
module: docs	255 multiplication of torch tensor with np array does the operation with numpy
module: numpy	255 multiplication of torch tensor with np array does the operation with numpy
module: cpp	255 multiplication of torch tensor with np array does the operation with numpy
module: cuda	255 multiplication of torch tensor with np array does the operation with numpy
module: autograd	255 multiplication of torch tensor with np array does the operation with numpy
module: nn	255 multiplication of torch tensor with np array does the operation with numpy
module: onnx	255 multiplication of torch tensor with np array does the operation with numpy
module: optimizer	255 multiplication of torch tensor with np array does the operation with numpy
module: tests	256 new feature   a very fast algorithm for computing matrix rank
module: docs	256 new feature   a very fast algorithm for computing matrix rank
module: numpy	256 new feature   a very fast algorithm for computing matrix rank
module: cpp	256 new feature   a very fast algorithm for computing matrix rank
module: cuda	256 new feature   a very fast algorithm for computing matrix rank
module: autograd	256 new feature   a very fast algorithm for computing matrix rank
module: nn	256 new feature   a very fast algorithm for computing matrix rank
module: onnx	256 new feature   a very fast algorithm for computing matrix rank
module: optimizer	256 new feature   a very fast algorithm for computing matrix rank
module: tests	257 numpy compatibility tracking issue
module: docs	257 numpy compatibility tracking issue
module: numpy	257 numpy compatibility tracking issue
module: cpp	257 numpy compatibility tracking issue
module: cuda	257 numpy compatibility tracking issue
module: autograd	257 numpy compatibility tracking issue
module: nn	257 numpy compatibility tracking issue
module: onnx	257 numpy compatibility tracking issue
module: optimizer	257 numpy compatibility tracking issue
module: tests	258 out  resizing  and restriding  behavior is confusing
module: docs	258 out  resizing  and restriding  behavior is confusing
module: numpy	258 out  resizing  and restriding  behavior is confusing
module: cpp	258 out  resizing  and restriding  behavior is confusing
module: cuda	258 out  resizing  and restriding  behavior is confusing
module: autograd	258 out  resizing  and restriding  behavior is confusing
module: nn	258 out  resizing  and restriding  behavior is confusing
module: onnx	258 out  resizing  and restriding  behavior is confusing
module: optimizer	258 out  resizing  and restriding  behavior is confusing
module: tests	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: docs	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: numpy	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: cpp	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: cuda	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: autograd	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: nn	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: onnx	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: optimizer	259 please add  tensor astype dtype string   syntax for numpy interoperability
module: tests	260 precision consistency issue in linspace
module: docs	260 precision consistency issue in linspace
module: numpy	260 precision consistency issue in linspace
module: cpp	260 precision consistency issue in linspace
module: cuda	260 precision consistency issue in linspace
module: autograd	260 precision consistency issue in linspace
module: nn	260 precision consistency issue in linspace
module: onnx	260 precision consistency issue in linspace
module: optimizer	260 precision consistency issue in linspace
module: tests	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: docs	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: numpy	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: cpp	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: cuda	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: autograd	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: nn	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: onnx	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: optimizer	261 pytorch s flip returns a new tensor  but numpy s flip returns a view
module: tests	262 reciprocals of complex tensors with infinities are different from numpy
module: docs	262 reciprocals of complex tensors with infinities are different from numpy
module: numpy	262 reciprocals of complex tensors with infinities are different from numpy
module: cpp	262 reciprocals of complex tensors with infinities are different from numpy
module: cuda	262 reciprocals of complex tensors with infinities are different from numpy
module: autograd	262 reciprocals of complex tensors with infinities are different from numpy
module: nn	262 reciprocals of complex tensors with infinities are different from numpy
module: onnx	262 reciprocals of complex tensors with infinities are different from numpy
module: optimizer	262 reciprocals of complex tensors with infinities are different from numpy
module: tests	263 reducing over empty dimensions for reductions without identity
module: docs	263 reducing over empty dimensions for reductions without identity
module: numpy	263 reducing over empty dimensions for reductions without identity
module: cpp	263 reducing over empty dimensions for reductions without identity
module: cuda	263 reducing over empty dimensions for reductions without identity
module: autograd	263 reducing over empty dimensions for reductions without identity
module: nn	263 reducing over empty dimensions for reductions without identity
module: onnx	263 reducing over empty dimensions for reductions without identity
module: optimizer	263 reducing over empty dimensions for reductions without identity
module: tests	264 set dtype if tensor converted to numpy
module: docs	264 set dtype if tensor converted to numpy
module: numpy	264 set dtype if tensor converted to numpy
module: cpp	264 set dtype if tensor converted to numpy
module: cuda	264 set dtype if tensor converted to numpy
module: autograd	264 set dtype if tensor converted to numpy
module: nn	264 set dtype if tensor converted to numpy
module: onnx	264 set dtype if tensor converted to numpy
module: optimizer	264 set dtype if tensor converted to numpy
module: tests	265 support alternate casting rules
module: docs	265 support alternate casting rules
module: numpy	265 support alternate casting rules
module: cpp	265 support alternate casting rules
module: cuda	265 support alternate casting rules
module: autograd	265 support alternate casting rules
module: nn	265 support alternate casting rules
module: onnx	265 support alternate casting rules
module: optimizer	265 support alternate casting rules
module: tests	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: docs	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: numpy	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: cpp	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: cuda	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: autograd	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: nn	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: onnx	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: optimizer	266 support creating a cpu tensor from ctypes pointer in python   from blob ptr  shape  strides  dtype
module: tests	267 support divmod for tensors
module: docs	267 support divmod for tensors
module: numpy	267 support divmod for tensors
module: cpp	267 support divmod for tensors
module: cuda	267 support divmod for tensors
module: autograd	267 support divmod for tensors
module: nn	267 support divmod for tensors
module: onnx	267 support divmod for tensors
module: optimizer	267 support divmod for tensors
module: tests	268 support keep stride for neg with requires grad false
module: docs	268 support keep stride for neg with requires grad false
module: numpy	268 support keep stride for neg with requires grad false
module: cpp	268 support keep stride for neg with requires grad false
module: cuda	268 support keep stride for neg with requires grad false
module: autograd	268 support keep stride for neg with requires grad false
module: nn	268 support keep stride for neg with requires grad false
module: onnx	268 support keep stride for neg with requires grad false
module: optimizer	268 support keep stride for neg with requires grad false
module: tests	269 test float to int conversion finite cpu int   is failing on macos
module: docs	269 test float to int conversion finite cpu int   is failing on macos
module: numpy	269 test float to int conversion finite cpu int   is failing on macos
module: cpp	269 test float to int conversion finite cpu int   is failing on macos
module: cuda	269 test float to int conversion finite cpu int   is failing on macos
module: autograd	269 test float to int conversion finite cpu int   is failing on macos
module: nn	269 test float to int conversion finite cpu int   is failing on macos
module: onnx	269 test float to int conversion finite cpu int   is failing on macos
module: optimizer	269 test float to int conversion finite cpu int   is failing on macos
module: tests	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: docs	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: numpy	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: cpp	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: cuda	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: autograd	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: nn	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: onnx	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: optimizer	270 tolist called on torch scalar does not return a list    proposal to support new arg force   true to provide a work around
module: tests	271 torch allclose does not allow different types for comparison
module: docs	271 torch allclose does not allow different types for comparison
module: numpy	271 torch allclose does not allow different types for comparison
module: cpp	271 torch allclose does not allow different types for comparison
module: cuda	271 torch allclose does not allow different types for comparison
module: autograd	271 torch allclose does not allow different types for comparison
module: nn	271 torch allclose does not allow different types for comparison
module: onnx	271 torch allclose does not allow different types for comparison
module: optimizer	271 torch allclose does not allow different types for comparison
module: tests	272 torch ceil  torch floor should accept a dtype argument
module: docs	272 torch ceil  torch floor should accept a dtype argument
module: numpy	272 torch ceil  torch floor should accept a dtype argument
module: cpp	272 torch ceil  torch floor should accept a dtype argument
module: cuda	272 torch ceil  torch floor should accept a dtype argument
module: autograd	272 torch ceil  torch floor should accept a dtype argument
module: nn	272 torch ceil  torch floor should accept a dtype argument
module: onnx	272 torch ceil  torch floor should accept a dtype argument
module: optimizer	272 torch ceil  torch floor should accept a dtype argument
module: tests	273 torch equal can still run successfully when the parameter types are different
module: docs	273 torch equal can still run successfully when the parameter types are different
module: numpy	273 torch equal can still run successfully when the parameter types are different
module: cpp	273 torch equal can still run successfully when the parameter types are different
module: cuda	273 torch equal can still run successfully when the parameter types are different
module: autograd	273 torch equal can still run successfully when the parameter types are different
module: nn	273 torch equal can still run successfully when the parameter types are different
module: onnx	273 torch equal can still run successfully when the parameter types are different
module: optimizer	273 torch equal can still run successfully when the parameter types are different
module: tests	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: docs	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: numpy	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: cpp	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: cuda	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: autograd	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: nn	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: onnx	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: optimizer	274 torch lobpcg producing different largest eigenvalue than scipy and np linalg eig
module: tests	275 torch polygamma inconsistent with scipy special polygamma for n
module: docs	275 torch polygamma inconsistent with scipy special polygamma for n
module: numpy	275 torch polygamma inconsistent with scipy special polygamma for n
module: cpp	275 torch polygamma inconsistent with scipy special polygamma for n
module: cuda	275 torch polygamma inconsistent with scipy special polygamma for n
module: autograd	275 torch polygamma inconsistent with scipy special polygamma for n
module: nn	275 torch polygamma inconsistent with scipy special polygamma for n
module: onnx	275 torch polygamma inconsistent with scipy special polygamma for n
module: optimizer	275 torch polygamma inconsistent with scipy special polygamma for n
module: tests	276 torch searchsorted issues
module: docs	276 torch searchsorted issues
module: numpy	276 torch searchsorted issues
module: cpp	276 torch searchsorted issues
module: cuda	276 torch searchsorted issues
module: autograd	276 torch searchsorted issues
module: nn	276 torch searchsorted issues
module: onnx	276 torch searchsorted issues
module: optimizer	276 torch searchsorted issues
module: tests	277 torch tensor relies on implicit conversion being deprecated in python
module: docs	277 torch tensor relies on implicit conversion being deprecated in python
module: numpy	277 torch tensor relies on implicit conversion being deprecated in python
module: cpp	277 torch tensor relies on implicit conversion being deprecated in python
module: cuda	277 torch tensor relies on implicit conversion being deprecated in python
module: autograd	277 torch tensor relies on implicit conversion being deprecated in python
module: nn	277 torch tensor relies on implicit conversion being deprecated in python
module: onnx	277 torch tensor relies on implicit conversion being deprecated in python
module: optimizer	277 torch tensor relies on implicit conversion being deprecated in python
module: tests	278 unbuffered operation
module: docs	278 unbuffered operation
module: numpy	278 unbuffered operation
module: cpp	278 unbuffered operation
module: cuda	278 unbuffered operation
module: autograd	278 unbuffered operation
module: nn	278 unbuffered operation
module: onnx	278 unbuffered operation
module: optimizer	278 unbuffered operation
module: tests	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: docs	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: numpy	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: cpp	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: cuda	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: autograd	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: nn	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: onnx	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: optimizer	279  dynamo  revise onnxruntime backend s use of capabilitybasedpartitioner
module: tests	280  onnx  assertion in models is not supported by fx exporter
module: docs	280  onnx  assertion in models is not supported by fx exporter
module: numpy	280  onnx  assertion in models is not supported by fx exporter
module: cpp	280  onnx  assertion in models is not supported by fx exporter
module: cuda	280  onnx  assertion in models is not supported by fx exporter
module: autograd	280  onnx  assertion in models is not supported by fx exporter
module: nn	280  onnx  assertion in models is not supported by fx exporter
module: onnx	280  onnx  assertion in models is not supported by fx exporter
module: optimizer	280  onnx  assertion in models is not supported by fx exporter
module: tests	281  onnx  dynamo  failed to export cumsum with dtype float
module: docs	281  onnx  dynamo  failed to export cumsum with dtype float
module: numpy	281  onnx  dynamo  failed to export cumsum with dtype float
module: cpp	281  onnx  dynamo  failed to export cumsum with dtype float
module: cuda	281  onnx  dynamo  failed to export cumsum with dtype float
module: autograd	281  onnx  dynamo  failed to export cumsum with dtype float
module: nn	281  onnx  dynamo  failed to export cumsum with dtype float
module: onnx	281  onnx  dynamo  failed to export cumsum with dtype float
module: optimizer	281  onnx  dynamo  failed to export cumsum with dtype float
module: tests	282  onnx  export failed for module with keyword only inputs
module: docs	282  onnx  export failed for module with keyword only inputs
module: numpy	282  onnx  export failed for module with keyword only inputs
module: cpp	282  onnx  export failed for module with keyword only inputs
module: cuda	282  onnx  export failed for module with keyword only inputs
module: autograd	282  onnx  export failed for module with keyword only inputs
module: nn	282  onnx  export failed for module with keyword only inputs
module: onnx	282  onnx  export failed for module with keyword only inputs
module: optimizer	282  onnx  export failed for module with keyword only inputs
module: tests	283  onnx  fx exporter  replace aten  copy  with out place version
module: docs	283  onnx  fx exporter  replace aten  copy  with out place version
module: numpy	283  onnx  fx exporter  replace aten  copy  with out place version
module: cpp	283  onnx  fx exporter  replace aten  copy  with out place version
module: cuda	283  onnx  fx exporter  replace aten  copy  with out place version
module: autograd	283  onnx  fx exporter  replace aten  copy  with out place version
module: nn	283  onnx  fx exporter  replace aten  copy  with out place version
module: onnx	283  onnx  fx exporter  replace aten  copy  with out place version
module: optimizer	283  onnx  fx exporter  replace aten  copy  with out place version
module: tests	284  onnx  fx exporter  test models onnxruntime py  tracker
module: docs	284  onnx  fx exporter  test models onnxruntime py  tracker
module: numpy	284  onnx  fx exporter  test models onnxruntime py  tracker
module: cpp	284  onnx  fx exporter  test models onnxruntime py  tracker
module: cuda	284  onnx  fx exporter  test models onnxruntime py  tracker
module: autograd	284  onnx  fx exporter  test models onnxruntime py  tracker
module: nn	284  onnx  fx exporter  test models onnxruntime py  tracker
module: onnx	284  onnx  fx exporter  test models onnxruntime py  tracker
module: optimizer	284  onnx  fx exporter  test models onnxruntime py  tracker
module: tests	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: docs	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: numpy	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: cpp	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: cuda	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: autograd	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: nn	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: onnx	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: optimizer	285  onnx  modularize pass does not work for torch onnx dynamo export with exportedprogram
module: tests	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: docs	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: numpy	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: cpp	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: cuda	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: autograd	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: nn	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: onnx	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: optimizer	286  onnx  refactor op level debug to catch mismatches between onnx models and exportedprogram and nn module
module: tests	287  onnx  remove the deprecated function  export
module: docs	287  onnx  remove the deprecated function  export
module: numpy	287  onnx  remove the deprecated function  export
module: cpp	287  onnx  remove the deprecated function  export
module: cuda	287  onnx  remove the deprecated function  export
module: autograd	287  onnx  remove the deprecated function  export
module: nn	287  onnx  remove the deprecated function  export
module: onnx	287  onnx  remove the deprecated function  export
module: optimizer	287  onnx  remove the deprecated function  export
module: tests	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: docs	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: numpy	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: cpp	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: cuda	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: autograd	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: nn	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: onnx	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: optimizer	288  onnx  result from export onnx in pytorch returns different result from pytorch
module: tests	289  onnx  stft export fails with dynamo export
module: docs	289  onnx  stft export fails with dynamo export
module: numpy	289  onnx  stft export fails with dynamo export
module: cpp	289  onnx  stft export fails with dynamo export
module: cuda	289  onnx  stft export fails with dynamo export
module: autograd	289  onnx  stft export fails with dynamo export
module: nn	289  onnx  stft export fails with dynamo export
module: onnx	289  onnx  stft export fails with dynamo export
module: optimizer	289  onnx  stft export fails with dynamo export
module: tests	290  onnx  test op consistency py doesn t support constant inputs
module: docs	290  onnx  test op consistency py doesn t support constant inputs
module: numpy	290  onnx  test op consistency py doesn t support constant inputs
module: cpp	290  onnx  test op consistency py doesn t support constant inputs
module: cuda	290  onnx  test op consistency py doesn t support constant inputs
module: autograd	290  onnx  test op consistency py doesn t support constant inputs
module: nn	290  onnx  test op consistency py doesn t support constant inputs
module: onnx	290  onnx  test op consistency py doesn t support constant inputs
module: optimizer	290  onnx  test op consistency py doesn t support constant inputs
module: tests	291 add onnx backend to torch export api
module: docs	291 add onnx backend to torch export api
module: numpy	291 add onnx backend to torch export api
module: cpp	291 add onnx backend to torch export api
module: cuda	291 add onnx backend to torch export api
module: autograd	291 add onnx backend to torch export api
module: nn	291 add onnx backend to torch export api
module: onnx	291 add onnx backend to torch export api
module: optimizer	291 add onnx backend to torch export api
module: tests	292 aten  int repr not supported in torch onnx export
module: docs	292 aten  int repr not supported in torch onnx export
module: numpy	292 aten  int repr not supported in torch onnx export
module: cpp	292 aten  int repr not supported in torch onnx export
module: cuda	292 aten  int repr not supported in torch onnx export
module: autograd	292 aten  int repr not supported in torch onnx export
module: nn	292 aten  int repr not supported in torch onnx export
module: onnx	292 aten  int repr not supported in torch onnx export
module: optimizer	292 aten  int repr not supported in torch onnx export
module: tests	293 couldn t export yolov  quantized model to onnx
module: docs	293 couldn t export yolov  quantized model to onnx
module: numpy	293 couldn t export yolov  quantized model to onnx
module: cpp	293 couldn t export yolov  quantized model to onnx
module: cuda	293 couldn t export yolov  quantized model to onnx
module: autograd	293 couldn t export yolov  quantized model to onnx
module: nn	293 couldn t export yolov  quantized model to onnx
module: onnx	293 couldn t export yolov  quantized model to onnx
module: optimizer	293 couldn t export yolov  quantized model to onnx
module: tests	294 crash on converting circular padding  to onnx
module: docs	294 crash on converting circular padding  to onnx
module: numpy	294 crash on converting circular padding  to onnx
module: cpp	294 crash on converting circular padding  to onnx
module: cuda	294 crash on converting circular padding  to onnx
module: autograd	294 crash on converting circular padding  to onnx
module: nn	294 crash on converting circular padding  to onnx
module: onnx	294 crash on converting circular padding  to onnx
module: optimizer	294 crash on converting circular padding  to onnx
module: tests	295 documentation error of torch onnx
module: docs	295 documentation error of torch onnx
module: numpy	295 documentation error of torch onnx
module: cpp	295 documentation error of torch onnx
module: cuda	295 documentation error of torch onnx
module: autograd	295 documentation error of torch onnx
module: nn	295 documentation error of torch onnx
module: onnx	295 documentation error of torch onnx
module: optimizer	295 documentation error of torch onnx
module: tests	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: docs	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: numpy	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: cpp	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: cuda	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: autograd	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: nn	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: onnx	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: optimizer	296 error  attribute exists on the python module  but we failed to convert python type   list  to a torchscript type
module: tests	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: docs	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: numpy	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: cpp	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: cuda	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: autograd	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: nn	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: onnx	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: optimizer	297 exporting the operator  aten   convolution mode  to onnx opset version    is not supported
module: tests	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: docs	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: numpy	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: cpp	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: cuda	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: autograd	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: nn	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: onnx	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: optimizer	298 exporting the operator  aten  linalg inv  to onnx opset version    is not supported
module: tests	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: docs	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: numpy	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: cpp	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: cuda	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: autograd	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: nn	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: onnx	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: optimizer	299 exporting the operator  prim  is cuda  to onnx opset version    is not supported
module: tests	300 facing error while using onnx from scatterelements
module: docs	300 facing error while using onnx from scatterelements
module: numpy	300 facing error while using onnx from scatterelements
module: cpp	300 facing error while using onnx from scatterelements
module: cuda	300 facing error while using onnx from scatterelements
module: autograd	300 facing error while using onnx from scatterelements
module: nn	300 facing error while using onnx from scatterelements
module: onnx	300 facing error while using onnx from scatterelements
module: optimizer	300 facing error while using onnx from scatterelements
module: tests	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: docs	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: numpy	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: cpp	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: cuda	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: autograd	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: nn	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: onnx	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: optimizer	301 got expand nodes with static shape input when exporting onnx model with dynamic shape
module: tests	302 graph  export onnx   incorrect data types in the binary string representation
module: docs	302 graph  export onnx   incorrect data types in the binary string representation
module: numpy	302 graph  export onnx   incorrect data types in the binary string representation
module: cpp	302 graph  export onnx   incorrect data types in the binary string representation
module: cuda	302 graph  export onnx   incorrect data types in the binary string representation
module: autograd	302 graph  export onnx   incorrect data types in the binary string representation
module: nn	302 graph  export onnx   incorrect data types in the binary string representation
module: onnx	302 graph  export onnx   incorrect data types in the binary string representation
module: optimizer	302 graph  export onnx   incorrect data types in the binary string representation
module: tests	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: docs	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: numpy	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: cpp	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: cuda	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: autograd	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: nn	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: onnx	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: optimizer	303 huggingface gpt  has missing inputs on torch onnx dynamo export using torch nn module model
module: tests	304 onnx export of batch norm for unknown channel size issue
module: docs	304 onnx export of batch norm for unknown channel size issue
module: numpy	304 onnx export of batch norm for unknown channel size issue
module: cpp	304 onnx export of batch norm for unknown channel size issue
module: cuda	304 onnx export of batch norm for unknown channel size issue
module: autograd	304 onnx export of batch norm for unknown channel size issue
module: nn	304 onnx export of batch norm for unknown channel size issue
module: onnx	304 onnx export of batch norm for unknown channel size issue
module: optimizer	304 onnx export of batch norm for unknown channel size issue
module: tests	305 onnx exporter for circular padding mode in convolution ops
module: docs	305 onnx exporter for circular padding mode in convolution ops
module: numpy	305 onnx exporter for circular padding mode in convolution ops
module: cpp	305 onnx exporter for circular padding mode in convolution ops
module: cuda	305 onnx exporter for circular padding mode in convolution ops
module: autograd	305 onnx exporter for circular padding mode in convolution ops
module: nn	305 onnx exporter for circular padding mode in convolution ops
module: onnx	305 onnx exporter for circular padding mode in convolution ops
module: optimizer	305 onnx exporter for circular padding mode in convolution ops
module: tests	306 please verify        onnx release candidate on testpypi
module: docs	306 please verify        onnx release candidate on testpypi
module: numpy	306 please verify        onnx release candidate on testpypi
module: cpp	306 please verify        onnx release candidate on testpypi
module: cuda	306 please verify        onnx release candidate on testpypi
module: autograd	306 please verify        onnx release candidate on testpypi
module: nn	306 please verify        onnx release candidate on testpypi
module: onnx	306 please verify        onnx release candidate on testpypi
module: optimizer	306 please verify        onnx release candidate on testpypi
module: tests	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: docs	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: numpy	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: cpp	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: cuda	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: autograd	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: nn	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: onnx	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: optimizer	307 revisit checkpoint naming mismatch with torch name  and onnx initializer name as a consequence
module: tests	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: docs	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: numpy	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: cpp	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: cuda	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: autograd	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: nn	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: onnx	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: optimizer	308 runtime exception   non zero status code returned while running mul node  name   time proj mul  status message   onnxruntime src onnxruntime core providers cpu math element wise ops h     void onnxruntime  broadcastiterator  init ptrdiff t  ptrdiff t  axis         axis    largest was false  attempting to broadcast an axis by a dimension other than      by
module: tests	309 support onnx opset    to export gelu to one single op
module: docs	309 support onnx opset    to export gelu to one single op
module: numpy	309 support onnx opset    to export gelu to one single op
module: cpp	309 support onnx opset    to export gelu to one single op
module: cuda	309 support onnx opset    to export gelu to one single op
module: autograd	309 support onnx opset    to export gelu to one single op
module: nn	309 support onnx opset    to export gelu to one single op
module: onnx	309 support onnx opset    to export gelu to one single op
module: optimizer	309 support onnx opset    to export gelu to one single op
module: tests	310 test view dynamic zero dim no longer testing zero input
module: docs	310 test view dynamic zero dim no longer testing zero input
module: numpy	310 test view dynamic zero dim no longer testing zero input
module: cpp	310 test view dynamic zero dim no longer testing zero input
module: cuda	310 test view dynamic zero dim no longer testing zero input
module: autograd	310 test view dynamic zero dim no longer testing zero input
module: nn	310 test view dynamic zero dim no longer testing zero input
module: onnx	310 test view dynamic zero dim no longer testing zero input
module: optimizer	310 test view dynamic zero dim no longer testing zero input
module: tests	311 torch compile fails with  internal assert failed  when compiling gpt
module: docs	311 torch compile fails with  internal assert failed  when compiling gpt
module: numpy	311 torch compile fails with  internal assert failed  when compiling gpt
module: cpp	311 torch compile fails with  internal assert failed  when compiling gpt
module: cuda	311 torch compile fails with  internal assert failed  when compiling gpt
module: autograd	311 torch compile fails with  internal assert failed  when compiling gpt
module: nn	311 torch compile fails with  internal assert failed  when compiling gpt
module: onnx	311 torch compile fails with  internal assert failed  when compiling gpt
module: optimizer	311 torch compile fails with  internal assert failed  when compiling gpt
module: tests	312 torch export does not support torchaudio transforms spectrogram
module: docs	312 torch export does not support torchaudio transforms spectrogram
module: numpy	312 torch export does not support torchaudio transforms spectrogram
module: cpp	312 torch export does not support torchaudio transforms spectrogram
module: cuda	312 torch export does not support torchaudio transforms spectrogram
module: autograd	312 torch export does not support torchaudio transforms spectrogram
module: nn	312 torch export does not support torchaudio transforms spectrogram
module: onnx	312 torch export does not support torchaudio transforms spectrogram
module: optimizer	312 torch export does not support torchaudio transforms spectrogram
module: tests	313 torch onnx dynamo export stuck at reshape
module: docs	313 torch onnx dynamo export stuck at reshape
module: numpy	313 torch onnx dynamo export stuck at reshape
module: cpp	313 torch onnx dynamo export stuck at reshape
module: cuda	313 torch onnx dynamo export stuck at reshape
module: autograd	313 torch onnx dynamo export stuck at reshape
module: nn	313 torch onnx dynamo export stuck at reshape
module: onnx	313 torch onnx dynamo export stuck at reshape
module: optimizer	313 torch onnx dynamo export stuck at reshape
module: tests	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: docs	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: numpy	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: cpp	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: cuda	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: autograd	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: nn	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: onnx	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: optimizer	314 torch onnx errors unsupportedoperatorerror  exporting the operator  aten  binary cross entropy  to onnx opset version    is not supported
module: tests	315 unsupported  onnx export of operator group norm  unknown input rank
module: docs	315 unsupported  onnx export of operator group norm  unknown input rank
module: numpy	315 unsupported  onnx export of operator group norm  unknown input rank
module: cpp	315 unsupported  onnx export of operator group norm  unknown input rank
module: cuda	315 unsupported  onnx export of operator group norm  unknown input rank
module: autograd	315 unsupported  onnx export of operator group norm  unknown input rank
module: nn	315 unsupported  onnx export of operator group norm  unknown input rank
module: onnx	315 unsupported  onnx export of operator group norm  unknown input rank
module: optimizer	315 unsupported  onnx export of operator group norm  unknown input rank
module: tests	316 use system onnx  undefined references
module: docs	316 use system onnx  undefined references
module: numpy	316 use system onnx  undefined references
module: cpp	316 use system onnx  undefined references
module: cuda	316 use system onnx  undefined references
module: autograd	316 use system onnx  undefined references
module: nn	316 use system onnx  undefined references
module: onnx	316 use system onnx  undefined references
module: optimizer	316 use system onnx  undefined references
module: tests	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: docs	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: numpy	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: cpp	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: cuda	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: autograd	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: nn	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: onnx	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: optimizer	317 when convert to onnx with dynamix axis   the reshape op  value is always the same as static   dynamic axis is useless  it cant t inference right shape dynamically
module: tests	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: docs	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: numpy	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: cpp	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: cuda	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: autograd	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: nn	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: onnx	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: optimizer	318  c   api parity  incorrect documentation for optim initialization in serialization docs
module: tests	319  discussion  relax optimizer constructor constraints for simplicity
module: docs	319  discussion  relax optimizer constructor constraints for simplicity
module: numpy	319  discussion  relax optimizer constructor constraints for simplicity
module: cpp	319  discussion  relax optimizer constructor constraints for simplicity
module: cuda	319  discussion  relax optimizer constructor constraints for simplicity
module: autograd	319  discussion  relax optimizer constructor constraints for simplicity
module: nn	319  discussion  relax optimizer constructor constraints for simplicity
module: onnx	319  discussion  relax optimizer constructor constraints for simplicity
module: optimizer	319  discussion  relax optimizer constructor constraints for simplicity
module: tests	320  docs  torch optim lr scheduler
module: docs	320  docs  torch optim lr scheduler
module: numpy	320  docs  torch optim lr scheduler
module: cpp	320  docs  torch optim lr scheduler
module: cuda	320  docs  torch optim lr scheduler
module: autograd	320  docs  torch optim lr scheduler
module: nn	320  docs  torch optim lr scheduler
module: onnx	320  docs  torch optim lr scheduler
module: optimizer	320  docs  torch optim lr scheduler
module: tests	321  dynamo  slow compile times for optimizers due to for loops
module: docs	321  dynamo  slow compile times for optimizers due to for loops
module: numpy	321  dynamo  slow compile times for optimizers due to for loops
module: cpp	321  dynamo  slow compile times for optimizers due to for loops
module: cuda	321  dynamo  slow compile times for optimizers due to for loops
module: autograd	321  dynamo  slow compile times for optimizers due to for loops
module: nn	321  dynamo  slow compile times for optimizers due to for loops
module: onnx	321  dynamo  slow compile times for optimizers due to for loops
module: optimizer	321  dynamo  slow compile times for optimizers due to for loops
module: tests	322  feature pitch  full batch optimization toolkit
module: docs	322  feature pitch  full batch optimization toolkit
module: numpy	322  feature pitch  full batch optimization toolkit
module: cpp	322  feature pitch  full batch optimization toolkit
module: cuda	322  feature pitch  full batch optimization toolkit
module: autograd	322  feature pitch  full batch optimization toolkit
module: nn	322  feature pitch  full batch optimization toolkit
module: onnx	322  feature pitch  full batch optimization toolkit
module: optimizer	322  feature pitch  full batch optimization toolkit
module: tests	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: docs	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: numpy	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: cpp	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: cuda	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: autograd	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: nn	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: onnx	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: optimizer	323  feature request  provide functional form of scheduler formulas  and reconsider older decisions of not doing it
module: tests	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: docs	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: numpy	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: cpp	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: cuda	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: autograd	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: nn	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: onnx	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: optimizer	324  testoptimrenewed  test set default dtype works with foreach errors with dynamo
module: tests	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: docs	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: numpy	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: cpp	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: cuda	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: autograd	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: nn	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: onnx	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: optimizer	325 adam optimizer doesn t work with cycliclr scheduler but works with onecyclelr
module: tests	326 add testing regarding sparseadam state dicts
module: docs	326 add testing regarding sparseadam state dicts
module: numpy	326 add testing regarding sparseadam state dicts
module: cpp	326 add testing regarding sparseadam state dicts
module: cuda	326 add testing regarding sparseadam state dicts
module: autograd	326 add testing regarding sparseadam state dicts
module: nn	326 add testing regarding sparseadam state dicts
module: onnx	326 add testing regarding sparseadam state dicts
module: optimizer	326 add testing regarding sparseadam state dicts
module: tests	327 adding novel  adafamily  optimizer
module: docs	327 adding novel  adafamily  optimizer
module: numpy	327 adding novel  adafamily  optimizer
module: cpp	327 adding novel  adafamily  optimizer
module: cuda	327 adding novel  adafamily  optimizer
module: autograd	327 adding novel  adafamily  optimizer
module: nn	327 adding novel  adafamily  optimizer
module: onnx	327 adding novel  adafamily  optimizer
module: optimizer	327 adding novel  adafamily  optimizer
module: tests	328 all keys matched successfully missing when loading state dict on optimizers
module: docs	328 all keys matched successfully missing when loading state dict on optimizers
module: numpy	328 all keys matched successfully missing when loading state dict on optimizers
module: cpp	328 all keys matched successfully missing when loading state dict on optimizers
module: cuda	328 all keys matched successfully missing when loading state dict on optimizers
module: autograd	328 all keys matched successfully missing when loading state dict on optimizers
module: nn	328 all keys matched successfully missing when loading state dict on optimizers
module: onnx	328 all keys matched successfully missing when loading state dict on optimizers
module: optimizer	328 all keys matched successfully missing when loading state dict on optimizers
module: tests	329 allow lrscheduler to take in param groups directly without an optimizer
module: docs	329 allow lrscheduler to take in param groups directly without an optimizer
module: numpy	329 allow lrscheduler to take in param groups directly without an optimizer
module: cpp	329 allow lrscheduler to take in param groups directly without an optimizer
module: cuda	329 allow lrscheduler to take in param groups directly without an optimizer
module: autograd	329 allow lrscheduler to take in param groups directly without an optimizer
module: nn	329 allow lrscheduler to take in param groups directly without an optimizer
module: onnx	329 allow lrscheduler to take in param groups directly without an optimizer
module: optimizer	329 allow lrscheduler to take in param groups directly without an optimizer
module: tests	330 c   optimizer check for duplicate parameters
module: docs	330 c   optimizer check for duplicate parameters
module: numpy	330 c   optimizer check for duplicate parameters
module: cpp	330 c   optimizer check for duplicate parameters
module: cuda	330 c   optimizer check for duplicate parameters
module: autograd	330 c   optimizer check for duplicate parameters
module: nn	330 c   optimizer check for duplicate parameters
module: onnx	330 c   optimizer check for duplicate parameters
module: optimizer	330 c   optimizer check for duplicate parameters
module: tests	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: docs	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: numpy	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: cpp	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: cuda	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: autograd	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: nn	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: onnx	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: optimizer	331 consider adding y x    y     x optimization for  foreach div  scalarlist and other div scalar overloads
module: tests	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: docs	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: numpy	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: cpp	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: cuda	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: autograd	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: nn	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: onnx	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: optimizer	332 cosineannealingwarmrestarts lr scheduler fails when lash epoch
module: tests	333 dynamo d optimizer does not handle closure correctly
module: docs	333 dynamo d optimizer does not handle closure correctly
module: numpy	333 dynamo d optimizer does not handle closure correctly
module: cpp	333 dynamo d optimizer does not handle closure correctly
module: cuda	333 dynamo d optimizer does not handle closure correctly
module: autograd	333 dynamo d optimizer does not handle closure correctly
module: nn	333 dynamo d optimizer does not handle closure correctly
module: onnx	333 dynamo d optimizer does not handle closure correctly
module: optimizer	333 dynamo d optimizer does not handle closure correctly
module: tests	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: docs	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: numpy	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: cpp	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: cuda	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: autograd	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: nn	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: onnx	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: optimizer	334 dynamo does not correctly handle future iterations if a specific iteration is frozen
module: tests	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: docs	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: numpy	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: cpp	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: cuda	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: autograd	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: nn	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: onnx	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: optimizer	335 dynamo ing sgd w  momentum errors for cpu params with empty grads
module: tests	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: docs	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: numpy	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: cpp	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: cuda	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: autograd	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: nn	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: onnx	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: optimizer	336 ema optimizer  class form and function form  using new foreach lerp    can be used for explicit robust updates of batchnorm stats
module: tests	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: docs	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: numpy	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: cpp	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: cuda	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: autograd	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: nn	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: onnx	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: optimizer	337 exponentiallr unexpectedly calls step   when init argument last epoch is larger than
module: tests	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: docs	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: numpy	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: cpp	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: cuda	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: autograd	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: nn	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: onnx	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: optimizer	338 flattening nn parameters while maintaining gradients from neural network forward pass
module: tests	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: docs	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: numpy	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: cpp	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: cuda	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: autograd	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: nn	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: onnx	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: optimizer	339 implement l  and l  gradient as hooks with the option of changing the weight decay value
module: tests	340 implementation of cg  and bicgstab methods
module: docs	340 implementation of cg  and bicgstab methods
module: numpy	340 implementation of cg  and bicgstab methods
module: cpp	340 implementation of cg  and bicgstab methods
module: cuda	340 implementation of cg  and bicgstab methods
module: autograd	340 implementation of cg  and bicgstab methods
module: nn	340 implementation of cg  and bicgstab methods
module: onnx	340 implementation of cg  and bicgstab methods
module: optimizer	340 implementation of cg  and bicgstab methods
module: tests	341 improve  group tensors by device and dtype
module: docs	341 improve  group tensors by device and dtype
module: numpy	341 improve  group tensors by device and dtype
module: cpp	341 improve  group tensors by device and dtype
module: cuda	341 improve  group tensors by device and dtype
module: autograd	341 improve  group tensors by device and dtype
module: nn	341 improve  group tensors by device and dtype
module: onnx	341 improve  group tensors by device and dtype
module: optimizer	341 improve  group tensors by device and dtype
module: tests	342 including adabound in the list of optimizers
module: docs	342 including adabound in the list of optimizers
module: numpy	342 including adabound in the list of optimizers
module: cpp	342 including adabound in the list of optimizers
module: cuda	342 including adabound in the list of optimizers
module: autograd	342 including adabound in the list of optimizers
module: nn	342 including adabound in the list of optimizers
module: onnx	342 including adabound in the list of optimizers
module: optimizer	342 including adabound in the list of optimizers
module: tests	343 is this a typo in optimizer pyi   it says statue instead of state
module: docs	343 is this a typo in optimizer pyi   it says statue instead of state
module: numpy	343 is this a typo in optimizer pyi   it says statue instead of state
module: cpp	343 is this a typo in optimizer pyi   it says statue instead of state
module: cuda	343 is this a typo in optimizer pyi   it says statue instead of state
module: autograd	343 is this a typo in optimizer pyi   it says statue instead of state
module: nn	343 is this a typo in optimizer pyi   it says statue instead of state
module: onnx	343 is this a typo in optimizer pyi   it says statue instead of state
module: optimizer	343 is this a typo in optimizer pyi   it says statue instead of state
module: tests	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: docs	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: numpy	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: cpp	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: cuda	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: autograd	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: nn	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: onnx	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: optimizer	344 keyerror  xxxxxxxxxx when calling optimizer state dict
module: tests	345 lbfgs accuracy difference between cpu and gpu
module: docs	345 lbfgs accuracy difference between cpu and gpu
module: numpy	345 lbfgs accuracy difference between cpu and gpu
module: cpp	345 lbfgs accuracy difference between cpu and gpu
module: cuda	345 lbfgs accuracy difference between cpu and gpu
module: autograd	345 lbfgs accuracy difference between cpu and gpu
module: nn	345 lbfgs accuracy difference between cpu and gpu
module: onnx	345 lbfgs accuracy difference between cpu and gpu
module: optimizer	345 lbfgs accuracy difference between cpu and gpu
module: tests	346 learning rate scheduler list index out of range
module: docs	346 learning rate scheduler list index out of range
module: numpy	346 learning rate scheduler list index out of range
module: cpp	346 learning rate scheduler list index out of range
module: cuda	346 learning rate scheduler list index out of range
module: autograd	346 learning rate scheduler list index out of range
module: nn	346 learning rate scheduler list index out of range
module: onnx	346 learning rate scheduler list index out of range
module: optimizer	346 learning rate scheduler list index out of range
module: tests	347 optim adadelta  local variable  lr  referenced before assignment
module: docs	347 optim adadelta  local variable  lr  referenced before assignment
module: numpy	347 optim adadelta  local variable  lr  referenced before assignment
module: cpp	347 optim adadelta  local variable  lr  referenced before assignment
module: cuda	347 optim adadelta  local variable  lr  referenced before assignment
module: autograd	347 optim adadelta  local variable  lr  referenced before assignment
module: nn	347 optim adadelta  local variable  lr  referenced before assignment
module: onnx	347 optim adadelta  local variable  lr  referenced before assignment
module: optimizer	347 optim adadelta  local variable  lr  referenced before assignment
module: tests	348 potential memory leak in adam optimizer in amd chips  cpu
module: docs	348 potential memory leak in adam optimizer in amd chips  cpu
module: numpy	348 potential memory leak in adam optimizer in amd chips  cpu
module: cpp	348 potential memory leak in adam optimizer in amd chips  cpu
module: cuda	348 potential memory leak in adam optimizer in amd chips  cpu
module: autograd	348 potential memory leak in adam optimizer in amd chips  cpu
module: nn	348 potential memory leak in adam optimizer in amd chips  cpu
module: onnx	348 potential memory leak in adam optimizer in amd chips  cpu
module: optimizer	348 potential memory leak in adam optimizer in amd chips  cpu
module: tests	349 problematic asgd optimizer
module: docs	349 problematic asgd optimizer
module: numpy	349 problematic asgd optimizer
module: cpp	349 problematic asgd optimizer
module: cuda	349 problematic asgd optimizer
module: autograd	349 problematic asgd optimizer
module: nn	349 problematic asgd optimizer
module: onnx	349 problematic asgd optimizer
module: optimizer	349 problematic asgd optimizer
module: tests	350 pytorch        adam optimizer malfunction
module: docs	350 pytorch        adam optimizer malfunction
module: numpy	350 pytorch        adam optimizer malfunction
module: cpp	350 pytorch        adam optimizer malfunction
module: cuda	350 pytorch        adam optimizer malfunction
module: autograd	350 pytorch        adam optimizer malfunction
module: nn	350 pytorch        adam optimizer malfunction
module: onnx	350 pytorch        adam optimizer malfunction
module: optimizer	350 pytorch        adam optimizer malfunction
module: tests	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: docs	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: numpy	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: cpp	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: cuda	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: autograd	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: nn	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: onnx	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: optimizer	351 pytorch latest update      broke cosineannealingwarmrestarts  t cur is not define
module: tests	352 sgd optimizer with deprecation warning
module: docs	352 sgd optimizer with deprecation warning
module: numpy	352 sgd optimizer with deprecation warning
module: cpp	352 sgd optimizer with deprecation warning
module: cuda	352 sgd optimizer with deprecation warning
module: autograd	352 sgd optimizer with deprecation warning
module: nn	352 sgd optimizer with deprecation warning
module: onnx	352 sgd optimizer with deprecation warning
module: optimizer	352 sgd optimizer with deprecation warning
module: tests	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: docs	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: numpy	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: cpp	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: cuda	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: autograd	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: nn	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: onnx	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: optimizer	353 small learning rate with capturable true causes adam optimizer to blow up model parameters
module: tests	354 support for arbitrary schedulers in sequentiallr
module: docs	354 support for arbitrary schedulers in sequentiallr
module: numpy	354 support for arbitrary schedulers in sequentiallr
module: cpp	354 support for arbitrary schedulers in sequentiallr
module: cuda	354 support for arbitrary schedulers in sequentiallr
module: autograd	354 support for arbitrary schedulers in sequentiallr
module: nn	354 support for arbitrary schedulers in sequentiallr
module: onnx	354 support for arbitrary schedulers in sequentiallr
module: optimizer	354 support for arbitrary schedulers in sequentiallr
module: tests	355 suppport fused adamw on cpu
module: docs	355 suppport fused adamw on cpu
module: numpy	355 suppport fused adamw on cpu
module: cpp	355 suppport fused adamw on cpu
module: cuda	355 suppport fused adamw on cpu
module: autograd	355 suppport fused adamw on cpu
module: nn	355 suppport fused adamw on cpu
module: onnx	355 suppport fused adamw on cpu
module: optimizer	355 suppport fused adamw on cpu
module: tests	356 weight decay in torch adam
module: docs	356 weight decay in torch adam
module: numpy	356 weight decay in torch adam
module: cpp	356 weight decay in torch adam
module: cuda	356 weight decay in torch adam
module: autograd	356 weight decay in torch adam
module: nn	356 weight decay in torch adam
module: onnx	356 weight decay in torch adam
module: optimizer	356 weight decay in torch adam
module: tests	357  docker  test corrcoef cpu complex   fails on cpu build
module: docs	357  docker  test corrcoef cpu complex   fails on cpu build
module: numpy	357  docker  test corrcoef cpu complex   fails on cpu build
module: cpp	357  docker  test corrcoef cpu complex   fails on cpu build
module: cuda	357  docker  test corrcoef cpu complex   fails on cpu build
module: autograd	357  docker  test corrcoef cpu complex   fails on cpu build
module: nn	357  docker  test corrcoef cpu complex   fails on cpu build
module: onnx	357  docker  test corrcoef cpu complex   fails on cpu build
module: optimizer	357  docker  test corrcoef cpu complex   fails on cpu build
module: tests	358  feature request  quant  support fakequant qconfigs in test module init
module: docs	358  feature request  quant  support fakequant qconfigs in test module init
module: numpy	358  feature request  quant  support fakequant qconfigs in test module init
module: cpp	358  feature request  quant  support fakequant qconfigs in test module init
module: cuda	358  feature request  quant  support fakequant qconfigs in test module init
module: autograd	358  feature request  quant  support fakequant qconfigs in test module init
module: nn	358  feature request  quant  support fakequant qconfigs in test module init
module: onnx	358  feature request  quant  support fakequant qconfigs in test module init
module: optimizer	358  feature request  quant  support fakequant qconfigs in test module init
module: tests	359  multi device  tests get skipped in standard ci
module: docs	359  multi device  tests get skipped in standard ci
module: numpy	359  multi device  tests get skipped in standard ci
module: cpp	359  multi device  tests get skipped in standard ci
module: cuda	359  multi device  tests get skipped in standard ci
module: autograd	359  multi device  tests get skipped in standard ci
module: nn	359  multi device  tests get skipped in standard ci
module: onnx	359  multi device  tests get skipped in standard ci
module: optimizer	359  multi device  tests get skipped in standard ci
module: tests	360  opinfo  improvements for sparse ops tests
module: docs	360  opinfo  improvements for sparse ops tests
module: numpy	360  opinfo  improvements for sparse ops tests
module: cpp	360  opinfo  improvements for sparse ops tests
module: cuda	360  opinfo  improvements for sparse ops tests
module: autograd	360  opinfo  improvements for sparse ops tests
module: nn	360  opinfo  improvements for sparse ops tests
module: onnx	360  opinfo  improvements for sparse ops tests
module: optimizer	360  opinfo  improvements for sparse ops tests
module: tests	361  tests  cumprod opinfo tests take long time to run  around  min
module: docs	361  tests  cumprod opinfo tests take long time to run  around  min
module: numpy	361  tests  cumprod opinfo tests take long time to run  around  min
module: cpp	361  tests  cumprod opinfo tests take long time to run  around  min
module: cuda	361  tests  cumprod opinfo tests take long time to run  around  min
module: autograd	361  tests  cumprod opinfo tests take long time to run  around  min
module: nn	361  tests  cumprod opinfo tests take long time to run  around  min
module: onnx	361  tests  cumprod opinfo tests take long time to run  around  min
module: optimizer	361  tests  cumprod opinfo tests take long time to run  around  min
module: tests	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: docs	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: numpy	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: cpp	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: cuda	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: autograd	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: nn	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: onnx	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: optimizer	362 add bc test for load state dict on optimizerinfos and moduleinfos
module: tests	363 add torch nn conv d correctness test
module: docs	363 add torch nn conv d correctness test
module: numpy	363 add torch nn conv d correctness test
module: cpp	363 add torch nn conv d correctness test
module: cuda	363 add torch nn conv d correctness test
module: autograd	363 add torch nn conv d correctness test
module: nn	363 add torch nn conv d correctness test
module: onnx	363 add torch nn conv d correctness test
module: optimizer	363 add torch nn conv d correctness test
module: tests	364 backcompat tests in test nn py are slow
module: docs	364 backcompat tests in test nn py are slow
module: numpy	364 backcompat tests in test nn py are slow
module: cpp	364 backcompat tests in test nn py are slow
module: cuda	364 backcompat tests in test nn py are slow
module: autograd	364 backcompat tests in test nn py are slow
module: nn	364 backcompat tests in test nn py are slow
module: onnx	364 backcompat tests in test nn py are slow
module: optimizer	364 backcompat tests in test nn py are slow
module: tests	365 better handling of opinfo sample inputs
module: docs	365 better handling of opinfo sample inputs
module: numpy	365 better handling of opinfo sample inputs
module: cpp	365 better handling of opinfo sample inputs
module: cuda	365 better handling of opinfo sample inputs
module: autograd	365 better handling of opinfo sample inputs
module: nn	365 better handling of opinfo sample inputs
module: onnx	365 better handling of opinfo sample inputs
module: optimizer	365 better handling of opinfo sample inputs
module: tests	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: docs	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: numpy	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: cpp	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: cuda	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: autograd	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: nn	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: onnx	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: optimizer	366 cuda gradcheck tests can occasionally leak memory in hud ci
module: tests	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: docs	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: numpy	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: cpp	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: cuda	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: autograd	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: nn	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: onnx	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: optimizer	367 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: tests	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: docs	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: numpy	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: cpp	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: cuda	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: autograd	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: nn	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: onnx	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: optimizer	368 disabled test nondeterministic alert maxunpool d cuda float      main   testtorchdevicetypecuda
module: tests	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: docs	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: numpy	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: cpp	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: cuda	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: autograd	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: nn	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: onnx	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: optimizer	369 fill  opinfo code not used  also  doesn t test the case where the second argument is a tensor
module: tests	370 functorch slow tests not being run in slow ci
module: docs	370 functorch slow tests not being run in slow ci
module: numpy	370 functorch slow tests not being run in slow ci
module: cpp	370 functorch slow tests not being run in slow ci
module: cuda	370 functorch slow tests not being run in slow ci
module: autograd	370 functorch slow tests not being run in slow ci
module: nn	370 functorch slow tests not being run in slow ci
module: onnx	370 functorch slow tests not being run in slow ci
module: optimizer	370 functorch slow tests not being run in slow ci
module: tests	371 get test jit py below   k lines
module: docs	371 get test jit py below   k lines
module: numpy	371 get test jit py below   k lines
module: cpp	371 get test jit py below   k lines
module: cuda	371 get test jit py below   k lines
module: autograd	371 get test jit py below   k lines
module: nn	371 get test jit py below   k lines
module: onnx	371 get test jit py below   k lines
module: optimizer	371 get test jit py below   k lines
module: tests	372 improve memory format testing
module: docs	372 improve memory format testing
module: numpy	372 improve memory format testing
module: cpp	372 improve memory format testing
module: cuda	372 improve memory format testing
module: autograd	372 improve memory format testing
module: nn	372 improve memory format testing
module: onnx	372 improve memory format testing
module: optimizer	372 improve memory format testing
module: tests	373 improve primtorch testing for view consistency
module: docs	373 improve primtorch testing for view consistency
module: numpy	373 improve primtorch testing for view consistency
module: cpp	373 improve primtorch testing for view consistency
module: cuda	373 improve primtorch testing for view consistency
module: autograd	373 improve primtorch testing for view consistency
module: nn	373 improve primtorch testing for view consistency
module: onnx	373 improve primtorch testing for view consistency
module: optimizer	373 improve primtorch testing for view consistency
module: tests	374 improve visibility in test suite timings
module: docs	374 improve visibility in test suite timings
module: numpy	374 improve visibility in test suite timings
module: cpp	374 improve visibility in test suite timings
module: cuda	374 improve visibility in test suite timings
module: autograd	374 improve visibility in test suite timings
module: nn	374 improve visibility in test suite timings
module: onnx	374 improve visibility in test suite timings
module: optimizer	374 improve visibility in test suite timings
module: tests	375 instability in test input weight equalization activation values test for random test values
module: docs	375 instability in test input weight equalization activation values test for random test values
module: numpy	375 instability in test input weight equalization activation values test for random test values
module: cpp	375 instability in test input weight equalization activation values test for random test values
module: cuda	375 instability in test input weight equalization activation values test for random test values
module: autograd	375 instability in test input weight equalization activation values test for random test values
module: nn	375 instability in test input weight equalization activation values test for random test values
module: onnx	375 instability in test input weight equalization activation values test for random test values
module: optimizer	375 instability in test input weight equalization activation values test for random test values
module: tests	376 is the current behavior with addcmul and integer dtypes intended
module: docs	376 is the current behavior with addcmul and integer dtypes intended
module: numpy	376 is the current behavior with addcmul and integer dtypes intended
module: cpp	376 is the current behavior with addcmul and integer dtypes intended
module: cuda	376 is the current behavior with addcmul and integer dtypes intended
module: autograd	376 is the current behavior with addcmul and integer dtypes intended
module: nn	376 is the current behavior with addcmul and integer dtypes intended
module: onnx	376 is the current behavior with addcmul and integer dtypes intended
module: optimizer	376 is the current behavior with addcmul and integer dtypes intended
module: tests	377 kwonly arguments without defaults don t work with test overrides py
module: docs	377 kwonly arguments without defaults don t work with test overrides py
module: numpy	377 kwonly arguments without defaults don t work with test overrides py
module: cpp	377 kwonly arguments without defaults don t work with test overrides py
module: cuda	377 kwonly arguments without defaults don t work with test overrides py
module: autograd	377 kwonly arguments without defaults don t work with test overrides py
module: nn	377 kwonly arguments without defaults don t work with test overrides py
module: onnx	377 kwonly arguments without defaults don t work with test overrides py
module: optimizer	377 kwonly arguments without defaults don t work with test overrides py
module: tests	378 linalg pinv singular tests are slow
module: docs	378 linalg pinv singular tests are slow
module: numpy	378 linalg pinv singular tests are slow
module: cpp	378 linalg pinv singular tests are slow
module: cuda	378 linalg pinv singular tests are slow
module: autograd	378 linalg pinv singular tests are slow
module: nn	378 linalg pinv singular tests are slow
module: onnx	378 linalg pinv singular tests are slow
module: optimizer	378 linalg pinv singular tests are slow
module: tests	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: docs	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: numpy	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: cpp	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: cuda	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: autograd	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: nn	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: onnx	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: optimizer	379 many inplace operators are not being tested for variant consistency  test variant consistency eager
module: tests	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: docs	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: numpy	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: cpp	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: cuda	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: autograd	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: nn	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: onnx	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: optimizer	380 opinfo incorrectly advertises lu solve support on cuda even when compiled without magma
module: tests	381 opinfo mechanism to test for nondeterminism
module: docs	381 opinfo mechanism to test for nondeterminism
module: numpy	381 opinfo mechanism to test for nondeterminism
module: cpp	381 opinfo mechanism to test for nondeterminism
module: cuda	381 opinfo mechanism to test for nondeterminism
module: autograd	381 opinfo mechanism to test for nondeterminism
module: nn	381 opinfo mechanism to test for nondeterminism
module: onnx	381 opinfo mechanism to test for nondeterminism
module: optimizer	381 opinfo mechanism to test for nondeterminism
module: tests	382 opinfo tests should compare gpu to cpu implementations
module: docs	382 opinfo tests should compare gpu to cpu implementations
module: numpy	382 opinfo tests should compare gpu to cpu implementations
module: cpp	382 opinfo tests should compare gpu to cpu implementations
module: cuda	382 opinfo tests should compare gpu to cpu implementations
module: autograd	382 opinfo tests should compare gpu to cpu implementations
module: nn	382 opinfo tests should compare gpu to cpu implementations
module: onnx	382 opinfo tests should compare gpu to cpu implementations
module: optimizer	382 opinfo tests should compare gpu to cpu implementations
module: tests	383 opinfo tests to validate that all operators are being tested with strided tensors
module: docs	383 opinfo tests to validate that all operators are being tested with strided tensors
module: numpy	383 opinfo tests to validate that all operators are being tested with strided tensors
module: cpp	383 opinfo tests to validate that all operators are being tested with strided tensors
module: cuda	383 opinfo tests to validate that all operators are being tested with strided tensors
module: autograd	383 opinfo tests to validate that all operators are being tested with strided tensors
module: nn	383 opinfo tests to validate that all operators are being tested with strided tensors
module: onnx	383 opinfo tests to validate that all operators are being tested with strided tensors
module: optimizer	383 opinfo tests to validate that all operators are being tested with strided tensors
module: tests	384 opinfos disabled for batched forward grad computation
module: docs	384 opinfos disabled for batched forward grad computation
module: numpy	384 opinfos disabled for batched forward grad computation
module: cpp	384 opinfos disabled for batched forward grad computation
module: cuda	384 opinfos disabled for batched forward grad computation
module: autograd	384 opinfos disabled for batched forward grad computation
module: nn	384 opinfos disabled for batched forward grad computation
module: onnx	384 opinfos disabled for batched forward grad computation
module: optimizer	384 opinfos disabled for batched forward grad computation
module: tests	385 pytorch        test optim fails on nvidia a
module: docs	385 pytorch        test optim fails on nvidia a
module: numpy	385 pytorch        test optim fails on nvidia a
module: cpp	385 pytorch        test optim fails on nvidia a
module: cuda	385 pytorch        test optim fails on nvidia a
module: autograd	385 pytorch        test optim fails on nvidia a
module: nn	385 pytorch        test optim fails on nvidia a
module: onnx	385 pytorch        test optim fails on nvidia a
module: optimizer	385 pytorch        test optim fails on nvidia a
module: tests	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: docs	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: numpy	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: cpp	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: cuda	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: autograd	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: nn	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: onnx	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: optimizer	386 pytorch linux xenial py    gcc    test may timeout during test multiprocessing spawn
module: tests	387 some inplace ops don t raise on incompatible shapes and meta device
module: docs	387 some inplace ops don t raise on incompatible shapes and meta device
module: numpy	387 some inplace ops don t raise on incompatible shapes and meta device
module: cpp	387 some inplace ops don t raise on incompatible shapes and meta device
module: cuda	387 some inplace ops don t raise on incompatible shapes and meta device
module: autograd	387 some inplace ops don t raise on incompatible shapes and meta device
module: nn	387 some inplace ops don t raise on incompatible shapes and meta device
module: onnx	387 some inplace ops don t raise on incompatible shapes and meta device
module: optimizer	387 some inplace ops don t raise on incompatible shapes and meta device
module: tests	388 test lazy spuriously fails if lapack is not installed
module: docs	388 test lazy spuriously fails if lapack is not installed
module: numpy	388 test lazy spuriously fails if lapack is not installed
module: cpp	388 test lazy spuriously fails if lapack is not installed
module: cuda	388 test lazy spuriously fails if lapack is not installed
module: autograd	388 test lazy spuriously fails if lapack is not installed
module: nn	388 test lazy spuriously fails if lapack is not installed
module: onnx	388 test lazy spuriously fails if lapack is not installed
module: optimizer	388 test lazy spuriously fails if lapack is not installed
module: tests	389 test nccl barrier timeout new group non member fails intermittently
module: docs	389 test nccl barrier timeout new group non member fails intermittently
module: numpy	389 test nccl barrier timeout new group non member fails intermittently
module: cpp	389 test nccl barrier timeout new group non member fails intermittently
module: cuda	389 test nccl barrier timeout new group non member fails intermittently
module: autograd	389 test nccl barrier timeout new group non member fails intermittently
module: nn	389 test nccl barrier timeout new group non member fails intermittently
module: onnx	389 test nccl barrier timeout new group non member fails intermittently
module: optimizer	389 test nccl barrier timeout new group non member fails intermittently
module: tests	390 test public bindings in ci gives weird output on error
module: docs	390 test public bindings in ci gives weird output on error
module: numpy	390 test public bindings in ci gives weird output on error
module: cpp	390 test public bindings in ci gives weird output on error
module: cuda	390 test public bindings in ci gives weird output on error
module: autograd	390 test public bindings in ci gives weird output on error
module: nn	390 test public bindings in ci gives weird output on error
module: onnx	390 test public bindings in ci gives weird output on error
module: optimizer	390 test public bindings in ci gives weird output on error
module: tests	391 test reductions ignoring some tests
module: docs	391 test reductions ignoring some tests
module: numpy	391 test reductions ignoring some tests
module: cpp	391 test reductions ignoring some tests
module: cuda	391 test reductions ignoring some tests
module: autograd	391 test reductions ignoring some tests
module: nn	391 test reductions ignoring some tests
module: onnx	391 test reductions ignoring some tests
module: optimizer	391 test reductions ignoring some tests
module: tests	392 test replicationpad d  test nn testnn  takes too long to run
module: docs	392 test replicationpad d  test nn testnn  takes too long to run
module: numpy	392 test replicationpad d  test nn testnn  takes too long to run
module: cpp	392 test replicationpad d  test nn testnn  takes too long to run
module: cuda	392 test replicationpad d  test nn testnn  takes too long to run
module: autograd	392 test replicationpad d  test nn testnn  takes too long to run
module: nn	392 test replicationpad d  test nn testnn  takes too long to run
module: onnx	392 test replicationpad d  test nn testnn  takes too long to run
module: optimizer	392 test replicationpad d  test nn testnn  takes too long to run
module: tests	393 test variant consistency eager addbmm fails on both cpu and cuda
module: docs	393 test variant consistency eager addbmm fails on both cpu and cuda
module: numpy	393 test variant consistency eager addbmm fails on both cpu and cuda
module: cpp	393 test variant consistency eager addbmm fails on both cpu and cuda
module: cuda	393 test variant consistency eager addbmm fails on both cpu and cuda
module: autograd	393 test variant consistency eager addbmm fails on both cpu and cuda
module: nn	393 test variant consistency eager addbmm fails on both cpu and cuda
module: onnx	393 test variant consistency eager addbmm fails on both cpu and cuda
module: optimizer	393 test variant consistency eager addbmm fails on both cpu and cuda
module: tests	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: docs	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: numpy	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: cpp	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: cuda	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: autograd	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: nn	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: onnx	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: optimizer	394 test variant consistency jit fails for torch tensordot with dtype float   with error internal assert failed
module: tests	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: docs	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: numpy	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: cpp	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: cuda	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: autograd	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: nn	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: onnx	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: optimizer	395 testcase assertequal does not distinguish python builtin types and single element tensor
module: tests	396 testdataloader test proper exit takes    min to finish
module: docs	396 testdataloader test proper exit takes    min to finish
module: numpy	396 testdataloader test proper exit takes    min to finish
module: cpp	396 testdataloader test proper exit takes    min to finish
module: cuda	396 testdataloader test proper exit takes    min to finish
module: autograd	396 testdataloader test proper exit takes    min to finish
module: nn	396 testdataloader test proper exit takes    min to finish
module: onnx	396 testdataloader test proper exit takes    min to finish
module: optimizer	396 testdataloader test proper exit takes    min to finish
module: tests	397 torch  dynamo config suppress errors may not be properly reset
module: docs	397 torch  dynamo config suppress errors may not be properly reset
module: numpy	397 torch  dynamo config suppress errors may not be properly reset
module: cpp	397 torch  dynamo config suppress errors may not be properly reset
module: cuda	397 torch  dynamo config suppress errors may not be properly reset
module: autograd	397 torch  dynamo config suppress errors may not be properly reset
module: nn	397 torch  dynamo config suppress errors may not be properly reset
module: onnx	397 torch  dynamo config suppress errors may not be properly reset
module: optimizer	397 torch  dynamo config suppress errors may not be properly reset
module: tests	398 tracker  moduleinfo based testing
module: docs	398 tracker  moduleinfo based testing
module: numpy	398 tracker  moduleinfo based testing
module: cpp	398 tracker  moduleinfo based testing
module: cuda	398 tracker  moduleinfo based testing
module: autograd	398 tracker  moduleinfo based testing
module: nn	398 tracker  moduleinfo based testing
module: onnx	398 tracker  moduleinfo based testing
module: optimizer	398 tracker  moduleinfo based testing
module: tests	399 when one distributed test fails in ci  the next one can fail spuriously
module: docs	399 when one distributed test fails in ci  the next one can fail spuriously
module: numpy	399 when one distributed test fails in ci  the next one can fail spuriously
module: cpp	399 when one distributed test fails in ci  the next one can fail spuriously
module: cuda	399 when one distributed test fails in ci  the next one can fail spuriously
module: autograd	399 when one distributed test fails in ci  the next one can fail spuriously
module: nn	399 when one distributed test fails in ci  the next one can fail spuriously
module: onnx	399 when one distributed test fails in ci  the next one can fail spuriously
module: optimizer	399 when one distributed test fails in ci  the next one can fail spuriously
module: tests	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: docs	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: numpy	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: cpp	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: cuda	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: autograd	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: nn	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: onnx	400 where opinfo doesn t handle cases where one of the inputs is a scalar
module: optimizer	400 where opinfo doesn t handle cases where one of the inputs is a scalar